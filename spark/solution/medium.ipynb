{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4438428",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('basic').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bff3e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (countDistinct,collect_set,collect_list,concat_ws,col,\\\n",
    "                nanvl,coalesce,abs,min,row_number,lead,lag,date_format,max,split,initcap,count,length,datediff,expr,avg,round,sum,avg,desc,asc,rank,when,expr,lit,to_date)\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType,StringType,DateType\n",
    "from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859514ad",
   "metadata": {},
   "source": [
    "#### 031 Managers with at Least 5 Direct Reports  \n",
    "\n",
    "Table: Employee\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| id          | int     |\n",
    "| name        | varchar |\n",
    "| department  | varchar |\n",
    "| managerId   | int     |\n",
    "+-------------+---------+\n",
    "id is the primary key (column with unique values) for this table.\n",
    "Each row of this table indicates the name of an employee, their department, and the id of their manager.\n",
    "If managerId is null, then the employee does not have a manager.\n",
    "No employee will be the manager of themself.\n",
    " \n",
    "\n",
    "Write a solution to find managers with at least five direct reports.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Employee table:\n",
    "+-----+-------+------------+-----------+\n",
    "| id  | name  | department | managerId |\n",
    "+-----+-------+------------+-----------+\n",
    "| 101 | John  | A          | null      |\n",
    "| 102 | Dan   | A          | 101       |\n",
    "| 103 | James | A          | 101       |\n",
    "| 104 | Amy   | A          | 101       |\n",
    "| 105 | Anne  | A          | 101       |\n",
    "| 106 | Ron   | B          | 101       |\n",
    "+-----+-------+------------+-----------+\n",
    "Output: \n",
    "+------+\n",
    "| name |\n",
    "+------+\n",
    "| John |\n",
    "+------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16e7601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgr_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/031_ManagerswithatLeast5DirectReports.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e83a78df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- managerId: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mgr_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c029757c",
   "metadata": {},
   "outputs": [],
   "source": [
    " max_ge = mgr_df.groupby('managerId').agg(count('managerId').alias('mgr_cnt')).filter(col('mgr_cnt') >= 5)\\\n",
    "            .select(col('managerId').alias('max_mgr_id'),'mgr_cnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84de7f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|John|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mgr_df.join(max_ge,col('max_mgr_id') == col('id'),'inner').select('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89118fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgr_df.createOrReplaceTempView('mgrdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "721ec83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|John|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select name from mgrdf where id in \\\n",
    "            (select managerId from mgrdf group by managerId having count(managerId) >= 5 )\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d86dd1",
   "metadata": {},
   "source": [
    "#### 032 Confirmation Rate\n",
    "\n",
    "Table: Signups\n",
    "\n",
    "+----------------+----------+\n",
    "| Column Name    | Type     |\n",
    "+----------------+----------+\n",
    "| user_id        | int      |\n",
    "| time_stamp     | datetime |\n",
    "+----------------+----------+\n",
    "user_id is the column of unique values for this table.\n",
    "Each row contains information about the signup time for the user with ID user_id.\n",
    " \n",
    "\n",
    "Table: Confirmations\n",
    "\n",
    "+----------------+----------+\n",
    "| Column Name    | Type     |\n",
    "+----------------+----------+\n",
    "| user_id        | int      |\n",
    "| time_stamp     | datetime |\n",
    "| action         | ENUM     |\n",
    "+----------------+----------+\n",
    "(user_id, time_stamp) is the primary key (combination of columns with unique values) for this table.\n",
    "user_id is a foreign key (reference column) to the Signups table.\n",
    "action is an ENUM (category) of the type ('confirmed', 'timeout')\n",
    "Each row of this table indicates that the user with ID user_id requested a confirmation message at time_stamp and that confirmation message was either confirmed ('confirmed') or expired without confirming ('timeout').\n",
    " \n",
    "\n",
    "The confirmation rate of a user is the number of 'confirmed' messages divided by the total number of requested confirmation messages. The confirmation rate of a user that did not request any confirmation messages is 0. Round the confirmation rate to two decimal places.\n",
    "\n",
    "Write a solution to find the confirmation rate of each user.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Signups table:\n",
    "+---------+---------------------+\n",
    "| user_id | time_stamp          |\n",
    "+---------+---------------------+\n",
    "| 3       | 2020-03-21 10:16:13 |\n",
    "| 7       | 2020-01-04 13:57:59 |\n",
    "| 2       | 2020-07-29 23:09:44 |\n",
    "| 6       | 2020-12-09 10:39:37 |\n",
    "+---------+---------------------+\n",
    "Confirmations table:\n",
    "+---------+---------------------+-----------+\n",
    "| user_id | time_stamp          | action    |\n",
    "+---------+---------------------+-----------+\n",
    "| 3       | 2021-01-06 03:30:46 | timeout   |\n",
    "| 3       | 2021-07-14 14:00:00 | timeout   |\n",
    "| 7       | 2021-06-12 11:57:29 | confirmed |\n",
    "| 7       | 2021-06-13 12:58:28 | confirmed |\n",
    "| 7       | 2021-06-14 13:59:27 | confirmed |\n",
    "| 2       | 2021-01-22 00:00:00 | confirmed |\n",
    "| 2       | 2021-02-28 23:59:59 | timeout   |\n",
    "+---------+---------------------+-----------+\n",
    "Output: \n",
    "+---------+-------------------+\n",
    "| user_id | confirmation_rate |\n",
    "+---------+-------------------+\n",
    "| 6       | 0.00              |\n",
    "| 3       | 0.00              |\n",
    "| 7       | 1.00              |\n",
    "| 2       | 0.50              |\n",
    "+---------+-------------------+\n",
    "Explanation: \n",
    "User 6 did not request any confirmation messages. The confirmation rate is 0.\n",
    "User 3 made 2 requests and both timed out. The confirmation rate is 0.\n",
    "User 7 made 3 requests and all were confirmed. The confirmation rate is 1.\n",
    "User 2 made 2 requests where one was confirmed and the other timed out. The confirmation rate is 1 / 2 = 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4525d34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "signup_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/032_Confirmation_Rate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0afdb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "confirm_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/032a_Confirmation_Rate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6964d5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- time_stamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "signup_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3a762368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- time_stamp: string (nullable = true)\n",
      " |-- action: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confirm_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fce62a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df = signup_df.select(col('user_id').alias('sig_user_id'),col('time_stamp').alias('sig_timestamp'))\\\n",
    "        .join(confirm_df,col('sig_user_id') == col('user_id'),'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bc0249e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|sig_user_id|confirmation_rate|\n",
      "+-----------+-----------------+\n",
      "|          6|              0.0|\n",
      "|          3|              0.0|\n",
      "|          7|              1.0|\n",
      "|          2|              0.5|\n",
      "+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_df.groupby('sig_user_id').agg( (sum(when(col('action') == 'confirmed',1).otherwise(0))/count('*')).alias('confirmation_rate') )\\\n",
    "                .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5d1dc8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "signup_df.createOrReplaceTempView('signupdf')\n",
    "confirm_df.createOrReplaceTempView('confirmdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "83916a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|user_id|confirmation_rate|\n",
      "+-------+-----------------+\n",
      "|      6|              0.0|\n",
      "|      3|              0.0|\n",
      "|      7|              1.0|\n",
      "|      2|              0.5|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" select user_id,( sum(CASE WHEN action = 'confirmed' THEN 1 ELSE 0 END)/count(*) )as confirmation_rate  from \\\n",
    "    (select sig.user_id,con.action from signupdf sig left join confirmdf con on sig.user_id = con.user_id)as a  \\\n",
    "    GROUP BY a.user_id\"  ) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4e503c",
   "metadata": {},
   "source": [
    "#### 033 Monthly Transactions\n",
    "\n",
    "Table: Transactions\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| id            | int     |\n",
    "| country       | varchar |\n",
    "| state         | enum    |\n",
    "| amount        | int     |\n",
    "| trans_date    | date    |\n",
    "+---------------+---------+\n",
    "id is the primary key of this table.\n",
    "The table has information about incoming transactions.\n",
    "The state column is an enum of type [\"approved\", \"declined\"].\n",
    " \n",
    "\n",
    "Write an SQL query to find for each month and country, the number of transactions and their total amount, the number of approved transactions and their total amount.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The query result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Transactions table:\n",
    "+------+---------+----------+--------+------------+\n",
    "| id   | country | state    | amount | trans_date |\n",
    "+------+---------+----------+--------+------------+\n",
    "| 121  | US      | approved | 1000   | 2018-12-18 |\n",
    "| 122  | US      | declined | 2000   | 2018-12-19 |\n",
    "| 123  | US      | approved | 2000   | 2019-01-01 |\n",
    "| 124  | DE      | approved | 2000   | 2019-01-07 |\n",
    "+------+---------+----------+--------+------------+\n",
    "Output: \n",
    "+----------+---------+-------------+----------------+--------------------+-----------------------+\n",
    "| month    | country | trans_count | approved_count | trans_total_amount | approved_total_amount |\n",
    "+----------+---------+-------------+----------------+--------------------+-----------------------+\n",
    "| 2018-12  | US      | 2           | 1              | 3000               | 1000                  |\n",
    "| 2019-01  | US      | 1           | 1              | 2000               | 2000                  |\n",
    "| 2019-01  | DE      | 1           | 1              | 2000               | 2000                  |\n",
    "+----------+---------+-------------+----------------+--------------------+-----------------------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29bd72e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_schema = StructType([\n",
    "                StructField('id',IntegerType()),\n",
    "                StructField('country',StringType()),\n",
    "                StructField('state',StringType()),\n",
    "                StructField('amount',IntegerType()),\n",
    "                StructField('trans_date',DateType())\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab2f2dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_df = spark.read.option('header',True).schema(trans_schema).format('csv')\\\n",
    "        .load('../data/easymedium/033_MonthlyTransactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "0edcb14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- trans_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trans_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "be245be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_trans_df = trans_df.select(date_format(col('trans_date'),'yyyy-MM').alias('date'),'country','amount') \\\n",
    "        .groupby('date','country').agg(count('date').alias('num_trans'),sum('amount').alias('total_amount')) \\\n",
    "        .select('date','country','num_trans','total_amount',lit(0).alias('approved_count'),lit(0).alias('approved_total_amount') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "37f2f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "approved_trans_df = trans_df.filter(col('state') == 'approved')\\\n",
    "    .select(date_format(col('trans_date'),'yyyy-MM').alias('date'),'country','amount') \\\n",
    "    .groupby('date','country').agg(count('date').alias('approved_count'),sum('amount').alias('approved_total_amount')) \\\n",
    "    .select('date','country',lit(0).alias('num_trans'),lit(0).alias('total_amount'),'approved_count','approved_total_amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a5c203b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+------------+--------------+---------------------+\n",
      "|   date|country|num_trans|total_amount|approved_count|approved_total_amount|\n",
      "+-------+-------+---------+------------+--------------+---------------------+\n",
      "|2019-01|     US|        1|        2000|             1|                 2000|\n",
      "|2019-01|     DE|        1|        2000|             1|                 2000|\n",
      "|2018-12|     US|        2|        3000|             1|                 1000|\n",
      "+-------+-------+---------+------------+--------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "group_trans_df.union(approved_trans_df).groupby('date','country')\\\n",
    "    .agg(sum('num_trans').alias('num_trans'),\\\n",
    "    sum('total_amount').alias('total_amount'),sum('approved_count').alias('approved_count'), \\\n",
    "    sum('approved_total_amount').alias('approved_total_amount')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5e398b",
   "metadata": {},
   "source": [
    "### No Union another method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "1585ab74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------+------------+--------------+---------------+\n",
      "|trans_date|country|num_trans|total_amount|approved_count|approvedd_count|\n",
      "+----------+-------+---------+------------+--------------+---------------+\n",
      "|   2019-01|     US|        1|        2000|             1|           2000|\n",
      "|   2019-01|     DE|        1|        2000|             1|           2000|\n",
      "|   2018-12|     US|        2|        3000|             1|           1000|\n",
      "+----------+-------+---------+------------+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#        count(when(col(\"state\").isNotNull(), 1)).alias(\"num_trans\"),\n",
    "#       _sum(when(col(\"state\").isNotNull(), col(\"amount\"))).alias(\"total_amount\"),\n",
    "\n",
    "trans_df.withColumn('trans_date',date_format(col('trans_date'),'yyyy-MM')).groupBy('trans_date','country') \\\n",
    "    .agg( \n",
    "         count('trans_date').alias('num_trans'), \\\n",
    "         sum('amount').alias('total_amount'), \\\n",
    "         count(when(col('state') == 'approved',1)).alias('approved_count'), \\\n",
    "            sum(when(col('state') == 'approved',col('amount'))).alias('approvedd_count')\\\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "b0a19ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_df.createOrReplaceTempView('transdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "9d4d3dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------+------------+--------------+---------------------+\n",
      "|trans_date|country|num_trans|total_amount|approved_count|approved_total_amount|\n",
      "+----------+-------+---------+------------+--------------+---------------------+\n",
      "|   2018/12|     US|        2|        3000|             1|                 1000|\n",
      "|   2019/01|     DE|        1|        2000|             1|                 2000|\n",
      "|   2019/01|     US|        1|        2000|             1|                 2000|\n",
      "+----------+-------+---------+------------+--------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" select trans_date,country,sum(num_trans) as num_trans,sum(total_amount) as total_amount \\\n",
    "            ,sum(approved_count) as approved_count,sum(approved_total_amount) as approved_total_amount from  \\\n",
    "              ( \\\n",
    "            (select date_format(trans_date,'yyyy/MM') as trans_date,country,count(*) as num_trans, sum(amount) as total_amount \\\n",
    "            ,0 as approved_count,0 as approved_total_amount from transdf group by trans_date,country) \\\n",
    "            UNION \\\n",
    "            (select date_format(trans_date,'yyyy/MM') as trans_date,country, 0 as num_trans, 0 as total_amount \\\n",
    "            ,count(trans_date) as approved_count, sum(amount) as approved_total_amount \\\n",
    "            from transdf where state = 'approved' group by trans_date,country \\\n",
    "            )  \\\n",
    "             ) group by trans_date,country \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e232b2",
   "metadata": {},
   "source": [
    "### No Union another method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "03d18c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+------------+--------------+---------------------+\n",
      "|   date|country|num_trans|total_amount|approved_count|approved_total_amount|\n",
      "+-------+-------+---------+------------+--------------+---------------------+\n",
      "|2019-01|     US|        1|        2000|             1|                 2000|\n",
      "|2019-01|     DE|        1|        2000|             1|                 2000|\n",
      "|2018-12|     US|        2|        3000|             2|                 1000|\n",
      "+-------+-------+---------+------------+--------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select date_format(trans_date,'yyyy-MM') as date,country,count(trans_date) as num_trans, \\\n",
    "            sum(amount) as total_amount,count(IF(state = 'approved',1,0)) as approved_count, \\\n",
    "            sum(if(state = 'approved',amount,0)) as approved_total_amount \\\n",
    "            from transdf group by date,country\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae95f9c",
   "metadata": {},
   "source": [
    "#### 034 Immediate Food Delivery\n",
    "\n",
    "Table: Delivery\n",
    "\n",
    "+-----------------------------+---------+\n",
    "| Column Name                 | Type    |\n",
    "+-----------------------------+---------+\n",
    "| delivery_id                 | int     |\n",
    "| customer_id                 | int     |\n",
    "| order_date                  | date    |\n",
    "| customer_pref_delivery_date | date    |\n",
    "+-----------------------------+---------+\n",
    "delivery_id is the column of unique values of this table.\n",
    "The table holds information about food delivery to customers that make orders at some date and specify a preferred delivery date (on the same order date or after it).\n",
    " \n",
    "\n",
    "If the customer's preferred delivery date is the same as the order date, then the order is called immediate; otherwise, it is called scheduled.\n",
    "\n",
    "The first order of a customer is the order with the earliest order date that the customer made. It is guaranteed that a customer has precisely one first order.\n",
    "\n",
    "Write a solution to find the percentage of immediate orders in the first orders of all customers, rounded to 2 decimal places.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Delivery table:\n",
    "+-------------+-------------+------------+-----------------------------+\n",
    "| delivery_id | customer_id | order_date | customer_pref_delivery_date |\n",
    "+-------------+-------------+------------+-----------------------------+\n",
    "| 1           | 1           | 2019-08-01 | 2019-08-02                  |\n",
    "| 2           | 2           | 2019-08-02 | 2019-08-02                  |\n",
    "| 3           | 1           | 2019-08-11 | 2019-08-12                  |\n",
    "| 4           | 3           | 2019-08-24 | 2019-08-24                  |\n",
    "| 5           | 3           | 2019-08-21 | 2019-08-22                  |\n",
    "| 6           | 2           | 2019-08-11 | 2019-08-13                  |\n",
    "| 7           | 4           | 2019-08-09 | 2019-08-09                  |\n",
    "+-------------+-------------+------------+-----------------------------+\n",
    "Output: \n",
    "+----------------------+\n",
    "| immediate_percentage |\n",
    "+----------------------+\n",
    "| 50.00                |\n",
    "+----------------------+\n",
    "Explanation: \n",
    "The customer id 1 has a first order with delivery id 1 and it is scheduled.\n",
    "The customer id 2 has a first order with delivery id 2 and it is immediate.\n",
    "The customer id 3 has a first order with delivery id 5 and it is scheduled.\n",
    "The customer id 4 has a first order with delivery id 7 and it is immediate.\n",
    "Hence, half the customers have immediate first orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4df6f988",
   "metadata": {},
   "outputs": [],
   "source": [
    "delivery_schema = StructType([\n",
    "                StructField('delivery_id',IntegerType()),\n",
    "                StructField('customer_id',IntegerType()),\n",
    "                StructField('order_date',DateType()),\n",
    "                StructField('cust_prefdelivery_date',DateType()),\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8600e7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "delivery_df = spark.read.option('header',True).schema(delivery_schema).format('csv')\\\n",
    "        .load('../data/easymedium/034_ImmediateFoodDelivery.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "7313a3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- delivery_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- cust_prefdelivery_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delivery_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "8c297193",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy('customer_id').orderBy('customer_id','order_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "e005d200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|percentage|\n",
      "+----------+\n",
      "|       0.5|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delivery_df.withColumn('diff',lag('order_date').over(window_spec)).filter(col('diff').isNull()) \\\n",
    ".withColumn('immediate',datediff(col('cust_prefdelivery_date'),col('order_date')))\\\n",
    ".select( (sum(when(col('immediate') == 0,1))/count('*')).alias('percentage') ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "115a4a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "delivery_df.createOrReplaceTempView('deliverydf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "22c37053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|percentage|\n",
      "+----------+\n",
      "|       0.5|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select sum(if(flag = 0,1,0))/ count(*) as percentage from \\\n",
    "            (select delivery_id,customer_id,order_date,cust_prefdelivery_date, \\\n",
    "            lag(order_date) over(partition by customer_id order by customer_id,order_date)  as diff , \\\n",
    "            datediff(cust_prefdelivery_date,order_date) as flag \\\n",
    "            from deliverydf ) where diff is null \\\n",
    "              \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "a1abcb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|pct|\n",
      "+---+\n",
      "|0.5|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" with first_orders as (select customer_id,min(order_date) as order_date \\\n",
    "    from deliverydf group by customer_id) \\\n",
    "    select sum(case when d.order_date = d.cust_prefdelivery_date then 1 else  0 end)/count(*)  as pct \\\n",
    "    from first_orders f inner join deliverydf d on d.customer_id = f.customer_id \\\n",
    "    and d.order_date = f.order_date \\\n",
    "    \").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "7f89d79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|pct|\n",
      "+---+\n",
      "|0.5|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" with first_order as ( \\\n",
    " select customer_id,order_date,cust_prefdelivery_date, \\\n",
    " row_number() over(partition by customer_id order by customer_id,order_date) as rnum \\\n",
    " from deliverydf ) \\\n",
    "     select sum(case when d.order_date = d.cust_prefdelivery_date then 1 else  0 end)/count(*) as pct \\\n",
    "     from first_order d where rnum = 1 \\\n",
    "         \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730c913e",
   "metadata": {},
   "source": [
    "#### 035 Game Play Analysis\n",
    "\n",
    "Table: Activity\n",
    "\n",
    "+--------------+---------+\n",
    "| Column Name  | Type    |\n",
    "+--------------+---------+\n",
    "| player_id    | int     |\n",
    "| device_id    | int     |\n",
    "| event_date   | date    |\n",
    "| games_played | int     |\n",
    "+--------------+---------+\n",
    "(player_id, event_date) is the primary key (combination of columns with unique values) of this table.\n",
    "This table shows the activity of players of some games.\n",
    "Each row is a record of a player who logged in and played a number of games (possibly 0) before logging out on someday using some device.\n",
    " \n",
    "\n",
    "Write a solution to report the fraction of players that logged in again on the day after the day they first logged in, rounded to 2 decimal places. In other words, you need to count the number of players that logged in for at least two consecutive days starting from their first login date, then divide that number by the total number of players.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Activity table:\n",
    "+-----------+-----------+------------+--------------+\n",
    "| player_id | device_id | event_date | games_played |\n",
    "+-----------+-----------+------------+--------------+\n",
    "| 1         | 2         | 2016-03-01 | 5            |\n",
    "| 1         | 2         | 2016-03-02 | 6            |\n",
    "| 2         | 3         | 2017-06-25 | 1            |\n",
    "| 3         | 1         | 2016-03-02 | 0            |\n",
    "| 3         | 4         | 2018-07-03 | 5            |\n",
    "+-----------+-----------+------------+--------------+\n",
    "Output: \n",
    "+-----------+\n",
    "| fraction  |\n",
    "+-----------+\n",
    "| 0.33      |\n",
    "+-----------+\n",
    "Explanation: \n",
    "Only the player with id 1 logged back in after the first day he had logged in so the answer is 1/3 = 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77e8a49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_schema = StructType([\n",
    "                StructField('player_id',IntegerType()),\n",
    "                StructField('device_id',IntegerType()),\n",
    "                StructField('event_date',DateType()),\n",
    "                StructField('games_played',IntegerType()),\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ad6e163",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_df = spark.read.option('header',True).schema(game_schema).format('csv')\\\n",
    "        .load('../data/easymedium/035_GamePlayAnalysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "22523941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- player_id: integer (nullable = true)\n",
      " |-- device_id: integer (nullable = true)\n",
      " |-- event_date: date (nullable = true)\n",
      " |-- games_played: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "game_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3b2e4502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|franction|\n",
      "+---------+\n",
      "|     0.33|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "game_df.groupby('player_id').agg(when(datediff(max(col('event_date')),\\\n",
    "                            min(col('event_date'))) == 1,1).otherwise(0).alias('x')) \\\n",
    "                            .select(round(sum(when(col('x') == 1 ,1).otherwise(0))/count('*'),2).alias('franction')) \\\n",
    "                            .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f79fcbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_window = Window.partitionBy('player_id').orderBy('player_id','event_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c872ebe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_df.createOrReplaceTempView('gamedf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1fcead63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|fraction|\n",
      "+--------+\n",
      "|    0.33|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select round(sum(if(a.x=1,1,0))/count(*),2) as fraction from \\\n",
    "          (select player_id,if(datediff( max(event_date),min(event_date)) = 1,1,0) as x\\\n",
    "          from gamedf group by player_id ) as a \").show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9562a7f6",
   "metadata": {},
   "source": [
    "#### 036 Product Sales Analysis\n",
    "\n",
    "Table: Sales\n",
    "\n",
    "+-------------+-------+\n",
    "| Column Name | Type  |\n",
    "+-------------+-------+\n",
    "| sale_id     | int   |\n",
    "| product_id  | int   |\n",
    "| year        | int   |\n",
    "| quantity    | int   |\n",
    "| price       | int   |\n",
    "+-------------+-------+\n",
    "(sale_id, year) is the primary key (combination of columns with unique values) of this table.\n",
    "product_id is a foreign key (reference column) to Product table.\n",
    "Each row of this table shows a sale on the product product_id in a certain year.\n",
    "Note that the price is per unit.\n",
    " \n",
    "\n",
    "Table: Product\n",
    "\n",
    "+--------------+---------+\n",
    "| Column Name  | Type    |\n",
    "+--------------+---------+\n",
    "| product_id   | int     |\n",
    "| product_name | varchar |\n",
    "+--------------+---------+\n",
    "product_id is the primary key (column with unique values) of this table.\n",
    "Each row of this table indicates the product name of each product.\n",
    " \n",
    "\n",
    "Write a solution to select the product id, year, quantity, and price for the first year of every product sold.\n",
    "\n",
    "Return the resulting table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Sales table:\n",
    "+---------+------------+------+----------+-------+\n",
    "| sale_id | product_id | year | quantity | price |\n",
    "+---------+------------+------+----------+-------+ \n",
    "| 1       | 100        | 2008 | 10       | 5000  |\n",
    "| 2       | 100        | 2009 | 12       | 5000  |\n",
    "| 7       | 200        | 2011 | 15       | 9000  |\n",
    "+---------+------------+------+----------+-------+\n",
    "Product table:\n",
    "+------------+--------------+\n",
    "| product_id | product_name |\n",
    "+------------+--------------+\n",
    "| 100        | Nokia        |\n",
    "| 200        | Apple        |\n",
    "| 300        | Samsung      |\n",
    "+------------+--------------+\n",
    "Output: \n",
    "+------------+------------+----------+-------+\n",
    "| product_id | first_year | quantity | price |\n",
    "+------------+------------+----------+-------+ \n",
    "| 100        | 2008       | 10       | 5000  |\n",
    "| 200        | 2011       | 15       | 9000  |\n",
    "+------------+------------+----------+-------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ebf032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sles_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/036_ProductSalesAnalysis.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "398ee525",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/036a_ProductSalesAnalysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "715b5f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sale_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sles_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "56e3662e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4e26bed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+-----+\n",
      "|product_id|first_year|quantity|price|\n",
      "+----------+----------+--------+-----+\n",
      "|       100|      2008|      10| 5000|\n",
      "|       200|      2011|      15| 9000|\n",
      "+----------+----------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sles_df.groupby('product_id').agg(min(col('year')).alias('first_year'))\\\n",
    "    .select(col('product_id').alias('agg_product_id'),col('first_year')) \\\n",
    "    .join(sles_df,(col('first_year') == col('year')) & (col('agg_product_id') == col('product_id')),'inner') \\\n",
    "    .select('product_id','first_year','quantity','price') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f68db4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sles_df.createOrReplaceTempView('slesdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b5f68806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+-----+\n",
      "|product_id|first_year|quantity|price|\n",
      "+----------+----------+--------+-----+\n",
      "|       100|      2008|      10| 5000|\n",
      "|       200|      2011|      15| 9000|\n",
      "+----------+----------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" select product_id,a.first_year,quantity,price from slesdf join \\\n",
    "            (select product_id as agg_product_id, min(year) as first_year from slesdf group by product_id) a \\\n",
    "            on a.agg_product_id = product_id and a.first_year = year \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc10b78d",
   "metadata": {},
   "source": [
    "#### 037 Customers Who Bought All Products  \n",
    "\n",
    "Table: Customer\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| customer_id | int     |\n",
    "| product_key | int     |\n",
    "+-------------+---------+\n",
    "This table may contain duplicates rows. \n",
    "customer_id is not NULL.\n",
    "product_key is a foreign key (reference column) to Product table.\n",
    " \n",
    "\n",
    "Table: Product\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| product_key | int     |\n",
    "+-------------+---------+\n",
    "product_key is the primary key (column with unique values) for this table.\n",
    " \n",
    "\n",
    "Write a solution to report the customer ids from the Customer table that bought all the products in the Product table.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Customer table:\n",
    "+-------------+-------------+\n",
    "| customer_id | product_key |\n",
    "+-------------+-------------+\n",
    "| 1           | 5           |\n",
    "| 2           | 6           |\n",
    "| 3           | 5           |\n",
    "| 3           | 6           |\n",
    "| 1           | 6           |\n",
    "+-------------+-------------+\n",
    "Product table:\n",
    "+-------------+\n",
    "| product_key |\n",
    "+-------------+\n",
    "| 5           |\n",
    "| 6           |\n",
    "+-------------+\n",
    "Output: \n",
    "+-------------+\n",
    "| customer_id |\n",
    "+-------------+\n",
    "| 1           |\n",
    "| 3           |\n",
    "+-------------+\n",
    "Explanation: \n",
    "The customers who bought all the products (5 and 6) are customers with IDs 1 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67a35575",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/037_CustomersWhoBoughtAllProducts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96068126",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/037a_CustomersWhoBoughtAllProducts.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "7ea42d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- product_key: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "c0bd10d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_key: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "d24fea53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|customer_id|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          3|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.groupby('customer_id').agg(count('product_key').alias('cnt_key')) \\\n",
    "      .join(product_df,product_df.select(count('*')).collect()[0][0] == col('cnt_key'),'inner') \\\n",
    "      .select('customer_id').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "905b07bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df.createOrReplaceTempView('customerdf')\n",
    "product_df.createOrReplaceTempView('productdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "ecbeb3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|customer_id|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          3|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" select customer_id from \\\n",
    "    ( select c.customer_id,count(c.product_key) as c_key from customerdf c join productdf p on  c.product_key \\\n",
    "    = p.product_key group by c.customer_id \\\n",
    "    having c_key in (select count(*) from productdf ) )\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aca05c",
   "metadata": {},
   "source": [
    "#### 038 Consecutive Numbers\n",
    "\n",
    "Table: Logs\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| id          | int     |\n",
    "| num         | varchar |\n",
    "+-------------+---------+\n",
    "In SQL, id is the primary key for this table.\n",
    "id is an autoincrement column.\n",
    " \n",
    "\n",
    "Find all numbers that appear at least three times consecutively.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Logs table:\n",
    "+----+-----+\n",
    "| id | num |\n",
    "+----+-----+\n",
    "| 1  | 1   |\n",
    "| 2  | 1   |\n",
    "| 3  | 1   |\n",
    "| 4  | 2   |\n",
    "| 5  | 1   |\n",
    "| 6  | 2   |\n",
    "| 7  | 2   |\n",
    "+----+-----+\n",
    "Output: \n",
    "+-----------------+\n",
    "| ConsecutiveNums |\n",
    "+-----------------+\n",
    "| 1               |\n",
    "+-----------------+\n",
    "Explanation: 1 is the only number that appears consecutively for at least three times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0616d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "consecutive_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/038_ConsecutiveNumbers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b6eba8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- num: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "consecutive_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "773881bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|num|\n",
      "+---+---+\n",
      "|  1|  1|\n",
      "|  2|  1|\n",
      "|  3|  1|\n",
      "|  4|  2|\n",
      "|  5|  2|\n",
      "|  6|  2|\n",
      "|  7|  2|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "consecutive_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c858f898",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_window = Window.orderBy('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64bc1fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = consecutive_df.select( lag('num').over(game_window).alias('lags'),col('num')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e63bfa37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|con_num|\n",
      "+-------+\n",
      "|      1|\n",
      "|      2|\n",
      "+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/22 12:25:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "xxx.select('num','lags',when( (col('lags').isNull()),consecutive_df.select('num').collect()[0]['num'])\\\n",
    "           .when(col('num') == col('lags'),col('lags')).otherwise(0).alias('x'))\\\n",
    "           .select(when( (col('num') == col('x')),col('x')).alias('con_num')).groupby(col('con_num'))\\\n",
    "           .agg(count(col('con_num')).alias('ccnt')).filter(col('ccnt') >= 3).select('con_num').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "03e1dcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "consecutive_df.createOrReplaceTempView('consecutivedf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "75a6df73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  x|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/22 12:25:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" with lags as (select lag(num) over(order by id) as lags,num from consecutivedf) \\\n",
    "            select CASE WHEN lags is null THEN  ( select num from consecutivedf limit 1)  \\\n",
    "                        WHEN lags  = num THEN num  else 0 END  as x from lags \\\n",
    "                  GROUP BY x \\\n",
    "                  HAVING count(*) >=3  and x !=0\\\n",
    "                  \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eda3fdd",
   "metadata": {},
   "source": [
    "#### 039 Product Price at a Given Date\n",
    "\n",
    "Table: Products\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| product_id    | int     |\n",
    "| new_price     | int     |\n",
    "| change_date   | date    |\n",
    "+---------------+---------+\n",
    "(product_id, change_date) is the primary key (combination of columns with unique values) of this table.\n",
    "Each row of this table indicates that the price of some product was changed to a new price at some date.\n",
    " \n",
    "\n",
    "Write a solution to find the prices of all products on 2019-08-16. Assume the price of all products before any change is 10.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Products table:\n",
    "+------------+-----------+-------------+\n",
    "| product_id | new_price | change_date |\n",
    "+------------+-----------+-------------+\n",
    "| 1          | 20        | 2019-08-14  |\n",
    "| 2          | 50        | 2019-08-14  |\n",
    "| 1          | 30        | 2019-08-15  |\n",
    "| 1          | 35        | 2019-08-16  |\n",
    "| 2          | 65        | 2019-08-17  |\n",
    "| 3          | 20        | 2019-08-18  |\n",
    "+------------+-----------+-------------+\n",
    "Output: \n",
    "+------------+-------+\n",
    "| product_id | price |\n",
    "+------------+-------+\n",
    "| 2          | 50    |\n",
    "| 1          | 35    |\n",
    "| 3          | 10    |\n",
    "+------------+-------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68153af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_schema = StructType([\n",
    "                StructField('product_id',IntegerType()),\n",
    "                StructField('new_price',IntegerType()),\n",
    "                StructField('change_date',DateType()),\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1764884",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/039_ProductPriceataGivenDate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "5fc4991f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- new_price: integer (nullable = true)\n",
      " |-- change_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "price_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "93ae1afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+\n",
      "|product_id|new_price|change_date|\n",
      "+----------+---------+-----------+\n",
      "|         1|       20| 2019-08-14|\n",
      "|         2|       50| 2019-08-14|\n",
      "|         1|       30| 2019-08-15|\n",
      "|         1|       35| 2019-08-16|\n",
      "|         2|       65| 2019-08-17|\n",
      "|         3|       20| 2019-08-18|\n",
      "+----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "price_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "763aabee",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_key = Window.partitionBy(col('product_id')).orderBy(col('change_date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "308d42b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+---+\n",
      "|product_id|new_price|change_date|rnk|\n",
      "+----------+---------+-----------+---+\n",
      "|         1|       20| 2019-08-14|  1|\n",
      "|         1|       30| 2019-08-15|  2|\n",
      "|         1|       35| 2019-08-16|  3|\n",
      "|         2|       50| 2019-08-14|  1|\n",
      "|         2|       65| 2019-08-17|  2|\n",
      "|         3|       20| 2019-08-18|  1|\n",
      "+----------+---------+-----------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 123:==============================================>      (174 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "price_df.select('product_id','new_price','change_date',rank().over(window_key).alias('rnk'))\\\n",
    "                    .orderBy(col('product_id'),col('rnk'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "65c36a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_diff = (price_df.filter( (col('product_id') == 1 )& (col('change_date') == lit('2019-08-16')) )\\\n",
    "        .select(col('new_price')).collect()[0][0]) - (price_df.filter( (col('product_id') == 1 )& (col('change_date') == lit('2019-08-15')) ) \\\n",
    "    .select(col('new_price')).collect()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "07702ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|product_id|price|\n",
      "+----------+-----+\n",
      "|         1|   35|\n",
      "|         2|   60|\n",
      "|         3|   10|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "price_df.select('product_id','new_price','change_date',(abs(datediff(col('change_date'),lit('2019-08-16')))\\\n",
    "          * price_diff).alias('datediff')).withColumn('price',(col('new_price')-col('datediff')))\\\n",
    "            .filter(col('change_date') >= lit('2019-08-16')).select('product_id','price').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "6699e83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df.createOrReplaceTempView('pricedf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "94a0718c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "|product_id|from|\n",
      "+----------+----+\n",
      "|         3|null|\n",
      "|         2|  50|\n",
      "|         1|  35|\n",
      "+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"with fil_product_id as ( \\\n",
    "                        select distinct product_id,max(change_date) as change_date \\\n",
    "                        from pricedf where change_date <= '2019-08-16' \\\n",
    "                        group by product_id), \\\n",
    "                price_product_id as ( \\\n",
    "                        select y.product_id,x.new_price,x.change_date \\\n",
    "                        from pricedf x \\\n",
    "                        join fil_product_id y on x.product_id = y.product_id  and  x.change_date = y.change_date )\\\n",
    "                select distinct p.product_id, pp.new_price from  from pricedf p \\\n",
    "                        left join price_product_id pp on p.product_id = pp.product_id \\\n",
    "        \").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "7d065f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|product_id|price|\n",
      "+----------+-----+\n",
      "|         3|   10|\n",
      "|         2|   50|\n",
      "|         1|   35|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" (SELECT product_id, 10 as price FROM pricedf \\\n",
    "            GROUP BY product_id HAVING  min(change_date) > '2019-08-16') \\\n",
    "            UNION  \\\n",
    "            (SELECT product_id,new_price AS price FROM pricedf WHERE (product_id, change_date) IN ( \\\n",
    "            SELECT product_id, MAX(change_date) FROM pricedf WHERE change_date <= '2019-08-16' GROUP BY product_id)) \\\n",
    "    \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e57d095",
   "metadata": {},
   "source": [
    "#### 040 Last Person to Fit in the Bus\n",
    "\n",
    "Table: Queue\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| person_id   | int     |\n",
    "| person_name | varchar |\n",
    "| weight      | int     |\n",
    "| turn        | int     |\n",
    "+-------------+---------+\n",
    "person_id column contains unique values.\n",
    "This table has the information about all people waiting for a bus.\n",
    "The person_id and turn columns will contain all numbers from 1 to n, where n is the number of rows in the table.\n",
    "turn determines the order of which the people will board the bus, where turn=1 denotes the first person to board and turn=n denotes the last person to board.\n",
    "weight is the weight of the person in kilograms.\n",
    " \n",
    "\n",
    "There is a queue of people waiting to board a bus. However, the bus has a weight limit of 1000 kilograms, so there may be some people who cannot board.\n",
    "\n",
    "Write a solution to find the person_name of the last person that can fit on the bus without exceeding the weight limit. The test cases are generated such that the first person does not exceed the weight limit.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Queue table:\n",
    "+-----------+-------------+--------+------+\n",
    "| person_id | person_name | weight | turn |\n",
    "+-----------+-------------+--------+------+\n",
    "| 5         | Alice       | 250    | 1    |\n",
    "| 4         | Bob         | 175    | 5    |\n",
    "| 3         | Alex        | 350    | 2    |\n",
    "| 6         | John Cena   | 400    | 3    |\n",
    "| 1         | Winston     | 500    | 6    |\n",
    "| 2         | Marie       | 200    | 4    |\n",
    "+-----------+-------------+--------+------+\n",
    "Output: \n",
    "+-------------+\n",
    "| person_name |\n",
    "+-------------+\n",
    "| John Cena   |\n",
    "+-------------+\n",
    "Explanation: The folowing table is ordered by the turn for simplicity.\n",
    "+------+----+-----------+--------+--------------+\n",
    "| Turn | ID | Name      | Weight | Total Weight |\n",
    "+------+----+-----------+--------+--------------+\n",
    "| 1    | 5  | Alice     | 250    | 250          |\n",
    "| 2    | 3  | Alex      | 350    | 600          |\n",
    "| 3    | 6  | John Cena | 400    | 1000         | (last person to board)\n",
    "| 4    | 2  | Marie     | 200    | 1200         | (cannot board)\n",
    "| 5    | 4  | Bob       | 175    | ___          |\n",
    "| 6    | 1  | Winston   | 500    | ___          |\n",
    "+------+----+-----------+--------+--------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73c90f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness_df = price_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/040_LastPersontoFitintheBus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "1234a340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- person_id: integer (nullable = true)\n",
      " |-- person_name: string (nullable = true)\n",
      " |-- weight: integer (nullable = true)\n",
      " |-- turn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fitness_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "67e459d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----------+------+\n",
      "|turn|person_id|person_name|weight|\n",
      "+----+---------+-----------+------+\n",
      "|   1|        5|      Alice|   250|\n",
      "|   2|        3|       Alex|   350|\n",
      "|   3|        6|  John Cena|   400|\n",
      "|   4|        2|      Marie|   200|\n",
      "|   5|        4|        Bob|   175|\n",
      "|   6|        1|    Winston|   500|\n",
      "+----+---------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fitness_df.select('turn','person_id','person_name','weight').sort('turn').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "1fb3fffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec_x= Window.orderBy(\"turn\").rowsBetween(Window.unboundedPreceding,Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "d9b20a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|person_name|\n",
      "+-----------+\n",
      "|  John Cena|\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/22 23:20:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "fitness_df.select('turn','person_id','person_name','weight').sort('turn') \\\n",
    "        .withColumn('rollweight',sum('weight').over(window_spec_x)).filter(col('rollweight') ==1000).select('person_name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "680446a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness_df.createOrReplaceTempView('fitnessdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "96125166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|person_name|\n",
      "+-----------+\n",
      "|  John Cena|\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/22 23:36:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"with main as (select turn,person_id,person_name,weight from fitnessdf order by turn) \\\n",
    "          select person_name from \\\n",
    "          (select person_name,sum(weight) over(order by turn rows between unbounded preceding and current row) \\\n",
    "          as cumulative_sum \\\n",
    "          from main) as y \\\n",
    "          where cumulative_sum = 1000\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a26efa",
   "metadata": {},
   "source": [
    "#### 041 Count Salary Categories\n",
    "\n",
    "Table: Accounts\n",
    "\n",
    "+-------------+------+\n",
    "| Column Name | Type |\n",
    "+-------------+------+\n",
    "| account_id  | int  |\n",
    "| income      | int  |\n",
    "+-------------+------+\n",
    "account_id is the primary key (column with unique values) for this table.\n",
    "Each row contains information about the monthly income for one bank account.\n",
    " \n",
    "\n",
    "Write a solution to calculate the number of bank accounts for each salary category. The salary categories are:\n",
    "\n",
    "\"Low Salary\": All the salaries strictly less than $20000.\n",
    "\"Average Salary\": All the salaries in the inclusive range [$20000, $50000].\n",
    "\"High Salary\": All the salaries strictly greater than $50000.\n",
    "The result table must contain all three categories. If there are no accounts in a category, return 0.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Accounts table:\n",
    "+------------+--------+\n",
    "| account_id | income |\n",
    "+------------+--------+\n",
    "| 3          | 108939 |\n",
    "| 2          | 12747  |\n",
    "| 8          | 87709  |\n",
    "| 6          | 91796  |\n",
    "+------------+--------+\n",
    "Output: \n",
    "+----------------+----------------+\n",
    "| category       | accounts_count |\n",
    "+----------------+----------------+\n",
    "| Low Salary     | 1              |\n",
    "| Average Salary | 0              |\n",
    "| High Salary    | 3              |\n",
    "+----------------+----------------+\n",
    "Explanation: \n",
    "Low Salary: Account 2.\n",
    "Average Salary: No accounts.\n",
    "High Salary: Accounts 3, 6, and 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5dbb344",
   "metadata": {},
   "outputs": [],
   "source": [
    "cntcat_df = price_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/041_CountSalaryCategories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "8439f3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- account_id: integer (nullable = true)\n",
      " |-- income: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cntcat_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef388810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|account_id|income|\n",
      "+----------+------+\n",
      "|         3|108939|\n",
      "|         2| 12747|\n",
      "|         8| 87709|\n",
      "|         6| 91796|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cntcat_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "01d74752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "|cnt|      category|\n",
      "+---+--------------+\n",
      "|  0|Average Salary|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = (20000,50000)\n",
    "cntcat_df.filter(  (cntcat_df['income'] > 20000) & (cntcat_df['income'] <= 50000) )\\\n",
    "    .agg(count('account_id').alias('cnt')).select(col('cnt'),lit('Average Salary').alias('category')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fdbb470f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "|cnt|      category|\n",
      "+---+--------------+\n",
      "|  0|Average Salary|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cntcat_df.filter( cntcat_df['income'].between(*x) ).select(count('*').alias('cnt'),lit('Average Salary').alias('category')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f60366c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+\n",
      "|      category|cnt|\n",
      "+--------------+---+\n",
      "|    Low Salary|  1|\n",
      "|Average Salary|  0|\n",
      "|   High Salary|  3|\n",
      "+--------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cntcat_df.filter(col('income') < 20000).groupby('account_id').agg(count('account_id').alias('cnt'))\\\n",
    "        .select(col('cnt'),lit('Low Salary').alias('category'))\\\n",
    "        .union(cntcat_df.filter( cntcat_df['income'].between(*x) ).select(count('*').alias('cnt'),lit('Average Salary').alias('category'))) \\\n",
    "        .union(cntcat_df.filter( col('income') > 50000 ).select(count('*').alias('cnt'),lit('High Salary').alias('category'))) \\\n",
    "        .select('category','cnt').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "84ee066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cntcat_df.createOrReplaceTempView('cntcatdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "6caf9d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+\n",
      "|      category|cnt|\n",
      "+--------------+---+\n",
      "|   High Salary|  3|\n",
      "|Average Salary|  0|\n",
      "|    Low Salary|  1|\n",
      "+--------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" (select 'Low Salary' as category,coalesce(sum(cnt),0) as cnt from \\\n",
    "            (select  count(*) as cnt from cntcatdf where income < 20000 group by account_id) group by cnt )\\\n",
    "    union(select 'Average Salary' as category,sum(cnt) as cnt from \\\n",
    "            (select case when income > 20000  and income  <= 50000 then count(*) else 0 end as cnt from cntcatdf group by income) \\\n",
    "            group by cnt) \\\n",
    "    union(select 'High Salary',sum(cnt) as cnt from \\\n",
    "          (select count(*) as cnt from cntcatdf where income > 50000 group by account_id) group by cnt) \\\n",
    "          \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84e7a68",
   "metadata": {},
   "source": [
    "#### 042 Exchange Seats\n",
    "\n",
    "Table: Seat\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| id          | int     |\n",
    "| student     | varchar |\n",
    "+-------------+---------+\n",
    "id is the primary key (unique value) column for this table.\n",
    "Each row of this table indicates the name and the ID of a student.\n",
    "id is a continuous increment.\n",
    " \n",
    "\n",
    "Write a solution to swap the seat id of every two consecutive students. If the number of students is odd, the id of the last student is not swapped.\n",
    "\n",
    "Return the result table ordered by id in ascending order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Seat table:\n",
    "+----+---------+\n",
    "| id | student |\n",
    "+----+---------+\n",
    "| 1  | Abbot   |\n",
    "| 2  | Doris   |\n",
    "| 3  | Emerson |\n",
    "| 4  | Green   |\n",
    "| 5  | Jeames  |\n",
    "+----+---------+\n",
    "Output: \n",
    "+----+---------+\n",
    "| id | student |\n",
    "+----+---------+\n",
    "| 1  | Doris   |\n",
    "| 2  | Abbot   |\n",
    "| 3  | Green   |\n",
    "| 4  | Emerson |\n",
    "| 5  | Jeames  |\n",
    "+----+---------+\n",
    "Explanation: \n",
    "Note that if the number of students is odd, there is no need to change the last one's seat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63acebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange_df = price_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/042_ExchangeSeats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "01cb14f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- student: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exchange_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "5fb3ce29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|student|\n",
      "+---+-------+\n",
      "|  1|  Abbot|\n",
      "|  2|  Doris|\n",
      "|  3|Emerson|\n",
      "|  4|  Green|\n",
      "|  5| Jeames|\n",
      "|  6|  qqqqq|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exchange_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "936a81bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "parity = exchange_df.select(max('id')).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "3d9fd164",
   "metadata": {},
   "outputs": [],
   "source": [
    "odd = exchange_df.filter(col('id') < parity ) \\\n",
    "    .filter(col('id')%2 == 0).select((col('id')-1).alias('id'),'student').union(\\\n",
    "     exchange_df.filter(col('id') < parity ) \\\n",
    "    .filter(col('id')%2 != 0).select((col('id')+1).alias('id'),'student'))\\\n",
    "    .union(exchange_df.filter(col('id') == parity)) \\\n",
    "    .sort('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "a5f5acca",
   "metadata": {},
   "outputs": [],
   "source": [
    "even = exchange_df.filter(col('id')%2 != 0).select((col('id')+1).alias('id'),'student')\\\n",
    ".union(exchange_df.filter(col('id')%2 == 0).select((col('id')-1).alias('id'),'student')).sort('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "b7f9d9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|student|\n",
      "+---+-------+\n",
      "|  1|  Doris|\n",
      "|  2|  Abbot|\n",
      "|  3|  Green|\n",
      "|  4|Emerson|\n",
      "|  5| Jeames|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if parity %2  == 1 :\n",
    "    odd.show()\n",
    "else:\n",
    "    even.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "e7540b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange_df.createOrReplaceTempView('exchangedf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "68059f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1944:===============================>                    (121 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|student|\n",
      "+---+-------+\n",
      "|  1|  Doris|\n",
      "|  2|  Abbot|\n",
      "|  3|  Green|\n",
      "|  4|Emerson|\n",
      "|  5| Jeames|\n",
      "+---+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"with parity as (select max(id) as maxid from exchangedf), \\\n",
    "                   odd as (select  * from exchangedf where id  < (select maxid from parity)),\\\n",
    "                  oddx as (select * from ( \\\n",
    "                          (select id-1 as id ,student from odd where id%2 = 0) union \\\n",
    "                          (select id+1 as id ,student from odd where id%2 != 0) union \\\n",
    "                          (select id,student from exchangedf where id = (select maxid from parity)) order by id )),\\\n",
    "                 evenx as (select * from ( \\\n",
    "                          (select id-1 as id ,student from exchangedf where id%2 = 0) union \\\n",
    "                          (select id+1 as id ,student from exchangedf where id%2 != 0) order by id)) \\\n",
    "                 select id,student from \\\n",
    "                    (select *,'oddx' as source from oddx union select *,'evenx' as source from evenx) combined \\\n",
    "                  where source =(select case when maxid%2 = 1 then 'oddx' else 'evenx' end from parity) order by id \\\n",
    "                    \").show()            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "62e2f857",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1989:===============================================>    (181 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|student|\n",
      "+---+-------+\n",
      "|  1|  Doris|\n",
      "|  2|  Abbot|\n",
      "|  3|  Green|\n",
      "|  4|Emerson|\n",
      "|  5| Jeames|\n",
      "+---+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"with parity as (select max(id) as maxid from exchangedf), \\\n",
    "                   odd as (select  * from exchangedf where id  < (select maxid from parity)),\\\n",
    "                  oddx as ( \\\n",
    "                          select id-1 as id ,student from odd where id%2 = 0 union \\\n",
    "                          select id+1 as id ,student from odd where id%2 != 0 union \\\n",
    "                          select id,student from exchangedf where id = (select maxid from parity) order by id ),\\\n",
    "                 evenx as ( \\\n",
    "                          select id-1 as id ,student from exchangedf where id%2 = 0 union \\\n",
    "                          select id+1 as id ,student from exchangedf where id%2 != 0 order by id) \\\n",
    "                 select id,student from \\\n",
    "                    (select *,'oddx' as source from oddx union select *,'evenx' as source from evenx) combined \\\n",
    "                  where source =(select case when maxid%2 = 1 then 'oddx' else 'evenx' end from parity) order by id \\\n",
    "                    \").show()            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "ba8d38b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange_df.createOrReplaceTempView('exchangedf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a88541",
   "metadata": {},
   "source": [
    "SELECT\n",
    "    s1.id, COALESCE(s2.student, s1.student) AS student\n",
    "FROM\n",
    "    seat s1\n",
    "        LEFT JOIN\n",
    "    seat s2 ON ((s1.id + 1) ^ 1) - 1 = s2.id\n",
    "ORDER BY s1.id;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186044b0",
   "metadata": {},
   "source": [
    "SELECT\n",
    "    (CASE\n",
    "        WHEN MOD(id, 2) != 0 AND counts != id THEN id + 1\n",
    "        WHEN MOD(id, 2) != 0 AND counts = id THEN id\n",
    "        ELSE id - 1\n",
    "    END) AS id,\n",
    "    student\n",
    "FROM\n",
    "    seat,\n",
    "    (SELECT\n",
    "        COUNT(*) AS counts\n",
    "    FROM\n",
    "        seat) AS seat_counts\n",
    "ORDER BY id ASC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "2939d85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query  = \"\"\"\n",
    "SELECT\n",
    "    (CASE\n",
    "        WHEN MOD(id, 2) != 0 AND counts != id THEN id + 1\n",
    "        WHEN MOD(id, 2) != 0 AND counts = id THEN id\n",
    "        ELSE id - 1\n",
    "    END) AS id,\n",
    "    student\n",
    "FROM\n",
    "    exchangedf,\n",
    "    (SELECT\n",
    "        COUNT(*) AS counts\n",
    "    FROM\n",
    "        exchangedf) AS seat_counts\n",
    "ORDER BY id ASC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "395a5198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---+---+\n",
      "|student|counts| id|mod|\n",
      "+-------+------+---+---+\n",
      "|  Abbot|     5|  1|  1|\n",
      "|  Doris|     5|  2|  0|\n",
      "|Emerson|     5|  3|  1|\n",
      "|  Green|     5|  4|  0|\n",
      "| Jeames|     5|  5|  1|\n",
      "+-------+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select student,x.counts,id,id%2 as mod from exchangedf,   \\\n",
    "          (SELECT COUNT(*) AS counts FROM exchangedf)as x \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "972a47a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|student|\n",
      "+---+-------+\n",
      "|  1|  Doris|\n",
      "|  2|  Abbot|\n",
      "|  3|  Green|\n",
      "|  4|Emerson|\n",
      "|  5|  qqqqq|\n",
      "|  6| Jeames|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd9aab5",
   "metadata": {},
   "source": [
    "#### 043_Movie Rating\n",
    "\n",
    "Table: Movies\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| movie_id      | int     |\n",
    "| title         | varchar |\n",
    "+---------------+---------+\n",
    "movie_id is the primary key (column with unique values) for this table.\n",
    "title is the name of the movie.\n",
    " \n",
    "\n",
    "Table: Users\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| user_id       | int     |\n",
    "| name          | varchar |\n",
    "+---------------+---------+\n",
    "user_id is the primary key (column with unique values) for this table.\n",
    " \n",
    "\n",
    "Table: MovieRating\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| movie_id      | int     |\n",
    "| user_id       | int     |\n",
    "| rating        | int     |\n",
    "| created_at    | date    |\n",
    "+---------------+---------+\n",
    "(movie_id, user_id) is the primary key (column with unique values) for this table.\n",
    "This table contains the rating of a movie by a user in their review.\n",
    "created_at is the user's review date. \n",
    " \n",
    "\n",
    "Write a solution to:\n",
    "\n",
    "Find the name of the user who has rated the greatest number of movies. In case of a tie, return the lexicographically smaller user name.\n",
    "Find the movie name with the highest average rating in February 2020. In case of a tie, return the lexicographically smaller movie name.\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Movies table:\n",
    "+-------------+--------------+\n",
    "| movie_id    |  title       |\n",
    "+-------------+--------------+\n",
    "| 1           | Avengers     |\n",
    "| 2           | Frozen 2     |\n",
    "| 3           | Joker        |\n",
    "+-------------+--------------+\n",
    "Users table:\n",
    "+-------------+--------------+\n",
    "| user_id     |  name        |\n",
    "+-------------+--------------+\n",
    "| 1           | Daniel       |\n",
    "| 2           | Monica       |\n",
    "| 3           | Maria        |\n",
    "| 4           | James        |\n",
    "+-------------+--------------+\n",
    "MovieRating table:\n",
    "+-------------+--------------+--------------+-------------+\n",
    "| movie_id    | user_id      | rating       | created_at  |\n",
    "+-------------+--------------+--------------+-------------+\n",
    "| 1           | 1            | 3            | 2020-01-12  |\n",
    "| 1           | 2            | 4            | 2020-02-11  |\n",
    "| 1           | 3            | 2            | 2020-02-12  |\n",
    "| 1           | 4            | 1            | 2020-01-01  |\n",
    "| 2           | 1            | 5            | 2020-02-17  | \n",
    "| 2           | 2            | 2            | 2020-02-01  | \n",
    "| 2           | 3            | 2            | 2020-03-01  |\n",
    "| 3           | 1            | 3            | 2020-02-22  | \n",
    "| 3           | 2            | 4            | 2020-02-25  | \n",
    "+-------------+--------------+--------------+-------------+\n",
    "Output: \n",
    "+--------------+\n",
    "| results      |\n",
    "+--------------+\n",
    "| Daniel       |\n",
    "| Frozen 2     |\n",
    "+--------------+\n",
    "Explanation: \n",
    "Daniel and Monica have rated 3 movies (\"Avengers\", \"Frozen 2\" and \"Joker\") but Daniel is smaller lexicographically.\n",
    "Frozen 2 and Joker have a rating average of 3.5 in February but Frozen 2 is smaller lexicographically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06f31ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_rating_schema = delivery_schema = StructType([\n",
    "                StructField('movie_id',IntegerType()),\n",
    "                StructField('user_id',IntegerType()),\n",
    "                StructField('rating',IntegerType()),\n",
    "                StructField('created_at',DateType()),\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b2f9c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df = price_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/043_MovieRating.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b8f7f684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movie_id: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movie_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e143c162",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_user_df = price_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/043a_MovieRating.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "40416e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movie_user_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bee6d8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_rating_df = price_df = spark.read.option('header',True).schema(movie_rating_schema).format('csv')\\\n",
    "        .load('../data/easymedium/043b_MovieRating.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00eb466a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movie_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- created_at: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movie_rating_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "88640586",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_rating_df.createOrReplaceTempView('movierating')\n",
    "movie_user_df.createOrReplaceTempView('movieuserdf')\n",
    "movie_df.createOrReplaceTempView('movie')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "caeb5919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 395:===========================================>         (166 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|  result|\n",
      "+--------+\n",
      "|Frozen 2|\n",
      "|  Daniel|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('with usr_cnt as \\\n",
    "                  (select user_id,cnt from \\\n",
    "                  (select user_id, count(*) as cnt from movierating group by user_id) x \\\n",
    "                  where cnt = (select max(cnt) as mcnt from (select count(*) as cnt \\\n",
    "                  from movierating group by user_id) as y)),\\\n",
    "    usr_result as (select min(name) as result from movieuserdf join usr_cnt on movieuserdf.user_id = usr_cnt.user_id), \\\n",
    "      movie_id as ( \\\n",
    "                  select movie_id,sum(rating)/count(movie_id)  as rating from \\\n",
    "                  (select movie_id, rating, date_format(created_at,\"MM-yyyy\") mmyyyy from movierating) x \\\n",
    "                  where mmyyyy = \"02-2020\" group by  movie_id), \\\n",
    "  movie_result as (select min(title) as result from movie join \\\n",
    "                  (select movie_id as ymovie_id,rating from movie_id where rating = (select max(rating) from movie_id)) y \\\n",
    "                  on movie_id = ymovie_id) \\\n",
    "                  select * from usr_result union select * from movie_result \\\n",
    "          ').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9d5c16",
   "metadata": {},
   "source": [
    "#### 044 Restaurant Growth\n",
    "\n",
    "Table: Customer\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| customer_id   | int     |\n",
    "| name          | varchar |\n",
    "| visited_on    | date    |\n",
    "| amount        | int     |\n",
    "+---------------+---------+\n",
    "In SQL,(customer_id, visited_on) is the primary key for this table.\n",
    "This table contains data about customer transactions in a restaurant.\n",
    "visited_on is the date on which the customer with ID (customer_id) has visited the restaurant.\n",
    "amount is the total paid by a customer.\n",
    " \n",
    "\n",
    "You are the restaurant owner and you want to analyze a possible expansion (there will be at least one customer every day).\n",
    "\n",
    "Compute the moving average of how much the customer paid in a seven days window (i.e., current day + 6 days before). average_amount should be rounded to two decimal places.\n",
    "\n",
    "Return the result table ordered by visited_on in ascending order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Customer table:\n",
    "+-------------+--------------+--------------+-------------+\n",
    "| customer_id | name         | visited_on   | amount      |\n",
    "+-------------+--------------+--------------+-------------+\n",
    "| 1           | Jhon         | 2019-01-01   | 100         |\n",
    "| 2           | Daniel       | 2019-01-02   | 110         |\n",
    "| 3           | Jade         | 2019-01-03   | 120         |\n",
    "| 4           | Khaled       | 2019-01-04   | 130         |\n",
    "| 5           | Winston      | 2019-01-05   | 110         | \n",
    "| 6           | Elvis        | 2019-01-06   | 140         | \n",
    "| 7           | Anna         | 2019-01-07   | 150         |\n",
    "| 8           | Maria        | 2019-01-08   | 80          |\n",
    "| 9           | Jaze         | 2019-01-09   | 110         | \n",
    "| 1           | Jhon         | 2019-01-10   | 130         | \n",
    "| 3           | Jade         | 2019-01-10   | 150         | \n",
    "+-------------+--------------+--------------+-------------+\n",
    "Output: \n",
    "+--------------+--------------+----------------+\n",
    "| visited_on   | amount       | average_amount |\n",
    "+--------------+--------------+----------------+\n",
    "| 2019-01-07   | 860          | 122.86         |\n",
    "| 2019-01-08   | 840          | 120            |\n",
    "| 2019-01-09   | 840          | 120            |\n",
    "| 2019-01-10   | 1000         | 142.86         |\n",
    "+--------------+--------------+----------------+\n",
    "Explanation: \n",
    "1st moving average from 2019-01-01 to 2019-01-07 has an average_amount of (100 + 110 + 120 + 130 + 110 + 140 + 150)/7 = 122.86\n",
    "2nd moving average from 2019-01-02 to 2019-01-08 has an average_amount of (110 + 120 + 130 + 110 + 140 + 150 + 80)/7 = 120\n",
    "3rd moving average from 2019-01-03 to 2019-01-09 has an average_amount of (120 + 130 + 110 + 140 + 150 + 80 + 110)/7 = 120\n",
    "4th moving average from 2019-01-04 to 2019-01-10 has an average_amount of (130 + 110 + 140 + 150 + 80 + 110 + 130 + 150)/7 = 142.86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01c51129",
   "metadata": {},
   "outputs": [],
   "source": [
    "resgrowth_schema = StructType([\n",
    "                StructField('customer_id',IntegerType()),\n",
    "                StructField('name',StringType()),\n",
    "                StructField('visited_on',DateType()),\n",
    "                StructField('amount',IntegerType())\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea8e6fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "resgrowth_df = price_df = spark.read.option('header',True).schema(resgrowth_schema).format('csv')\\\n",
    "        .load('../data/easymedium/044_RestaurantGrowth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "82dd52f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- visited_on: date (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resgrowth_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "466d06f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+----------+------+\n",
      "|customer_id|   name|visited_on|amount|\n",
      "+-----------+-------+----------+------+\n",
      "|          1|   Jhon|2019-01-01|   100|\n",
      "|          2| Daniel|2019-01-02|   110|\n",
      "|          3|   Jade|2019-01-03|   120|\n",
      "|          4| Khaled|2019-01-04|   130|\n",
      "|          5|Winston|2019-01-05|   110|\n",
      "|          6|  Elvis|2019-01-06|   140|\n",
      "|          7|   Anna|2019-01-07|   150|\n",
      "|          8|  Maria|2019-01-08|    80|\n",
      "|          9|   Jaze|2019-01-09|   110|\n",
      "|          1|   Jhon|2019-01-10|   130|\n",
      "|          3|   Jade|2019-01-10|   150|\n",
      "+-----------+-------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resgrowth_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d08b8fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.orderBy('visited_on').rowsBetween(-6,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "dff328c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/24 18:30:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+\n",
      "|visited_on|moving_sub|moving_avg|\n",
      "+----------+----------+----------+\n",
      "|2019-01-07|       860|    122.86|\n",
      "|2019-01-08|       840|     120.0|\n",
      "|2019-01-09|       840|     120.0|\n",
      "|2019-01-10|      1000|    142.86|\n",
      "+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resgrowth_df.groupby('visited_on').agg(sum('amount').alias('amount'),countDistinct('visited_on').alias('rows')).orderBy('visited_on') \\\n",
    "    .select('visited_on','rows',\\\n",
    "    sum('amount').over(window_spec).alias('moving_sum'),\\\n",
    "    round(avg('amount').over(window_spec),2).alias('moving_avg'),\n",
    "    sum('rows').over(window_spec).alias('moving_rowcnt') ).filter(col('moving_rowcnt') > 6) \\\n",
    "    .select('visited_on','moving_sub','moving_avg').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "c9ff07a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "resgrowth_df.createOrReplaceTempView('resgrowthdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "c20269b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/24 19:23:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+\n",
      "|visited_on|moving_sum|moving_avg|\n",
      "+----------+----------+----------+\n",
      "|2019-01-07|       860|    122.86|\n",
      "|2019-01-08|       840|     120.0|\n",
      "|2019-01-09|       840|     120.0|\n",
      "|2019-01-10|      1000|    142.86|\n",
      "+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"with main as (select visited_on,sum(amount) as amount,count(distinct visited_on) as dis_rows \\\n",
    "                        from resgrowthdf group by visited_on), \\\n",
    "              second as (select visited_on, \\\n",
    "                        sum(amount) over(order by visited_on rows between 6 PRECEDING AND CURRENT ROW) as moving_sum, \\\n",
    "                        round(avg(amount) over(order by visited_on rows between 6 PRECEDING AND CURRENT ROW),2) as moving_avg, \\\n",
    "                        sum(dis_rows) over(order by visited_on rows between 6 PRECEDING AND CURRENT ROW) as moving_row_cnt from main) \\\n",
    "                        select visited_on,moving_sum,moving_avg from second where moving_row_cnt > 6 \\\n",
    "                        \").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b69509f",
   "metadata": {},
   "source": [
    "#### 045 Friend Requests Who Has the Most Friends\n",
    "\n",
    "Table: RequestAccepted\n",
    "\n",
    "+----------------+---------+\n",
    "| Column Name    | Type    |\n",
    "+----------------+---------+\n",
    "| requester_id   | int     |\n",
    "| accepter_id    | int     |\n",
    "| accept_date    | date    |\n",
    "+----------------+---------+\n",
    "(requester_id, accepter_id) is the primary key (combination of columns with unique values) for this table.\n",
    "This table contains the ID of the user who sent the request, the ID of the user who received the request, and the date when the request was accepted.\n",
    " \n",
    "\n",
    "Write a solution to find the people who have the most friends and the most friends number.\n",
    "\n",
    "The test cases are generated so that only one person has the most friends.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "RequestAccepted table:\n",
    "+--------------+-------------+-------------+\n",
    "| requester_id | accepter_id | accept_date |\n",
    "+--------------+-------------+-------------+\n",
    "| 1            | 2           | 2016/06/03  |\n",
    "| 1            | 3           | 2016/06/08  |\n",
    "| 2            | 3           | 2016/06/08  |\n",
    "| 3            | 4           | 2016/06/09  |\n",
    "+--------------+-------------+-------------+\n",
    "Output: \n",
    "+----+-----+\n",
    "| id | num |\n",
    "+----+-----+\n",
    "| 3  | 3   |\n",
    "+----+-----+\n",
    "Explanation: \n",
    "The person with id 3 is a friend of people 1, 2, and 4, so he has three friends in total, which is the most number than any others.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9199ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "friends_schema = StructType([\n",
    "                StructField('requester_id',IntegerType()),\n",
    "                StructField('accepter_id',IntegerType()),\n",
    "                StructField('accept_date',DateType())\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53668efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "friends_df = price_df = spark.read.option('header',True).schema(friends_schema).format('csv')\\\n",
    "        .load('../data/easymedium/045_FriendRequestsWhoHastheMostFriends.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "7e7de237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- requester_id: integer (nullable = true)\n",
      " |-- accepter_id: integer (nullable = true)\n",
      " |-- accept_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "friends_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "8cc03897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----------+\n",
      "|requester_id|accepter_id|accept_date|\n",
      "+------------+-----------+-----------+\n",
      "|           1|          2| 2016-06-03|\n",
      "|           1|          3| 2016-06-08|\n",
      "|           2|          3| 2016-06-08|\n",
      "|           3|          4| 2016-06-09|\n",
      "+------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "friends_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "80650d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = friends_df.select(col('requester_id').alias('id') ).union(friends_df.select(col('accepter_id').alias('id') ))\\\n",
    "    .groupby(col('id')).agg(count('*').alias('cnt')).select(max(col('cnt')).alias('mcnt')).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "3201c6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|cnt|\n",
      "+---+---+\n",
      "|  3|  3|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "friends_df.select(col('requester_id').alias('id') ).union(friends_df.select(col('accepter_id').alias('id') ))\\\n",
    "    .groupby(col('id')).agg(count('*').alias('cnt')).filter(col('cnt') == main_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "35284690",
   "metadata": {},
   "outputs": [],
   "source": [
    "friends_df.createOrReplaceTempView('friendsdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "3230d757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|cnt|\n",
      "+---+---+\n",
      "|  3|  3|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" with main as (select id, count(id) as cnt from  \\\n",
    "                         (select requester_id  as id from friendsdf union all select accepter_id as id from friendsdf) \\\n",
    "                         group by id ) \\\n",
    "                         select id, cnt from main where cnt = (select max(cnt) from main) \\\n",
    "            \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2185c974",
   "metadata": {},
   "source": [
    " #### 046 Investments in 2016\n",
    " \n",
    " Table: Insurance\n",
    "\n",
    "+-------------+-------+\n",
    "| Column Name | Type  |\n",
    "+-------------+-------+\n",
    "| pid         | int   |\n",
    "| tiv_2015    | float |\n",
    "| tiv_2016    | float |\n",
    "| lat         | float |\n",
    "| lon         | float |\n",
    "+-------------+-------+\n",
    "pid is the primary key (column with unique values) for this table.\n",
    "Each row of this table contains information about one policy where:\n",
    "pid is the policyholder's policy ID.\n",
    "tiv_2015 is the total investment value in 2015 and tiv_2016 is the total investment value in 2016.\n",
    "lat is the latitude of the policy holder's city. It's guaranteed that lat is not NULL.\n",
    "lon is the longitude of the policy holder's city. It's guaranteed that lon is not NULL.\n",
    " \n",
    "\n",
    "Write a solution to report the sum of all total investment values in 2016 tiv_2016, for all policyholders who:\n",
    "\n",
    "have the same tiv_2015 value as one or more other policyholders, and\n",
    "are not located in the same city as any other policyholder (i.e., the (lat, lon) attribute pairs must be unique).\n",
    "Round tiv_2016 to two decimal places.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Insurance table:\n",
    "+-----+----------+----------+-----+-----+\n",
    "| pid | tiv_2015 | tiv_2016 | lat | lon |\n",
    "+-----+----------+----------+-----+-----+\n",
    "| 1   | 10       | 5        | 10  | 10  |\n",
    "| 2   | 20       | 20       | 20  | 20  |\n",
    "| 3   | 10       | 30       | 20  | 20  |\n",
    "| 4   | 10       | 40       | 40  | 40  |\n",
    "+-----+----------+----------+-----+-----+\n",
    "Output: \n",
    "+----------+\n",
    "| tiv_2016 |\n",
    "+----------+\n",
    "| 45.00    |\n",
    "+----------+\n",
    "Explanation: \n",
    "The first record in the table, like the last record, meets both of the two criteria.\n",
    "The tiv_2015 value 10 is the same as the third and fourth records, and its location is unique.\n",
    "\n",
    "The second record does not meet any of the two criteria. Its tiv_2015 is not like any other policyholders and its location is the same as the third record, which makes the third record fail, too.\n",
    "So, the result is the sum of tiv_2016 of the first and last record, which is 45."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dcf07d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "investment_df = price_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/046_Investmentsin2016.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "085a3a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+---+---+\n",
      "|pid|tiv_2015|tiv_2016|lat|lon|\n",
      "+---+--------+--------+---+---+\n",
      "|  1|      10|       5| 10| 10|\n",
      "|  2|      20|      20| 20| 20|\n",
      "|  3|      10|      30| 20| 20|\n",
      "|  4|      10|      40| 40| 40|\n",
      "+---+--------+--------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "investment_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57d66fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|tiv_2016|\n",
      "+--------+\n",
      "|      45|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "investment_df.select('*',\\\n",
    "                count('*').over(Window.partitionBy('tiv_2015')).alias('tiv_2015_cnt'),\\\n",
    "                count('*').over(Window.partitionBy('lat','lon')).alias('loc_cnt'))\\\n",
    "                .filter( (col('tiv_2015_cnt') > 1) & (col('loc_cnt') ==1) ) \\\n",
    "                .select(round(sum(col('tiv_2016')),2).alias('tiv_2016')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7745a315",
   "metadata": {},
   "outputs": [],
   "source": [
    "investment_df.createOrReplaceTempView('investment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aca793ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "query  = ''' SELECT ROUND(SUM(tiv_2016), 2) AS tiv_2016\n",
    "                FROM (\n",
    "                   SELECT *,\n",
    "                       COUNT(*)OVER(PARTITION BY tiv_2015) AS tiv_2015_cnt,\n",
    "                       COUNT(*)OVER(PARTITION BY lat, lon) AS loc_cnt\n",
    "                   FROM investment\n",
    "                   )t0\n",
    "                WHERE tiv_2015_cnt > 1\n",
    "                AND loc_cnt = 1\n",
    "      '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7c772df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|tiv_2016|\n",
      "+--------+\n",
      "|      45|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 67:=================================================>    (184 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e027490f",
   "metadata": {},
   "source": [
    "#### 047 Second Highest Salary\n",
    "\n",
    "Table: Employee\n",
    "\n",
    "+-------------+------+\n",
    "| Column Name | Type |\n",
    "+-------------+------+\n",
    "| id          | int  |\n",
    "| salary      | int  |\n",
    "+-------------+------+\n",
    "id is the primary key (column with unique values) for this table.\n",
    "Each row of this table contains information about the salary of an employee.\n",
    " \n",
    "\n",
    "Write a solution to find the second highest salary from the Employee table. If there is no second highest salary, return null (return None in Pandas).\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Employee table:\n",
    "+----+--------+\n",
    "| id | salary |\n",
    "+----+--------+\n",
    "| 1  | 100    |\n",
    "| 2  | 200    |\n",
    "| 3  | 300    |\n",
    "+----+--------+\n",
    "Output: \n",
    "+---------------------+\n",
    "| SecondHighestSalary |\n",
    "+---------------------+\n",
    "| 200                 |\n",
    "+---------------------+\n",
    "Example 2:\n",
    "\n",
    "Input: \n",
    "Employee table:\n",
    "+----+--------+\n",
    "| id | salary |\n",
    "+----+--------+\n",
    "| 1  | 100    |\n",
    "+----+--------+\n",
    "Output: \n",
    "+---------------------+\n",
    "| SecondHighestSalary |\n",
    "+---------------------+\n",
    "| null                |\n",
    "+---------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3282c55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_df = price_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/047_SecondHighestSalary.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "93ddeb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|salary|\n",
      "+---+------+\n",
      "|  1|   100|\n",
      "|  2|   200|\n",
      "|  3|   300|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salary_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b150669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.orderBy(\"salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "fed62121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|second_value|\n",
      "+------------+\n",
      "|         200|\n",
      "+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/25 20:01:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "x.select( when( salary_df.select(count('*').alias('cnt')).first()[0] >= lit(2),salary_df.select(lead('salary').over(window_spec))\\\n",
    "              .first()[0]).otherwise(lit('null')).alias('second_value')).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "8217d5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_df.createOrReplaceTempView('salarydf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "5bf95e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|second_value|\n",
      "+------------+\n",
      "|        null|\n",
      "+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/25 20:24:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "spark.sql('with main as (select lead(salary) over(order by (salary)) as second_value from salarydf limit 1 ), \\\n",
    "                 cnt as (select count(*) as cnt from salarydf ) \\\n",
    "                        select case when cnt >= 2 then (select second_value from main) \\\n",
    "                        else null end as second_value from cnt \\\n",
    "                 ' ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50023ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    " if(select count(*) from salarydf >=2,select * from main,\"null\" '"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
