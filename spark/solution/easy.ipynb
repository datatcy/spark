{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cdd0af6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../spark-3.1.2-bin-hadoop3.2')\n",
    "from pyspark.sql.functions import sum\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.appName('basic').getOrCreate()\n",
    "    print('Hi')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19302198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('basic').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cc6b079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format,countDistinct,collect_set,collect_list,concat_ws,col,max,split,initcap,count,length,datediff,expr,avg,round,sum,avg,desc,asc,rank,when,expr,lit,to_date\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType,StringType,DateType\n",
    "from pyspark.sql.window import Window\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b342663",
   "metadata": {},
   "source": [
    "window_spec = Window.partitionBy('race_year')\\\n",
    "    .orderBy(desc('total_points'),desc('wins'))\n",
    "driverstanding = driver_standing_df.withColumn('rank',rank().over(window_spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79833f24",
   "metadata": {},
   "source": [
    "#### 001 Recyclable and Low Fat Products\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| product_id  | int     |\n",
    "| low_fats    | enum    |\n",
    "| recyclable  | enum    |\n",
    "+-------------+---------+\n",
    "product_id is the primary key (column with unique values) for this table.\n",
    "low_fats is an ENUM (category) of type ('Y', 'N') where 'Y' means this product is low fat and 'N' means it is not.\n",
    "recyclable is an ENUM (category) of types ('Y', 'N') where 'Y' means this product is recyclable and 'N' means it is not.\n",
    " \n",
    "\n",
    "Write a solution to find the ids of products that are both low fat and recyclable.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Products table:\n",
    "+-------------+----------+------------+\n",
    "| product_id  | low_fats | recyclable |\n",
    "+-------------+----------+------------+\n",
    "| 0           | Y        | N          |\n",
    "| 1           | Y        | Y          |\n",
    "| 2           | N        | Y          |\n",
    "| 3           | Y        | Y          |\n",
    "| 4           | N        | N          |\n",
    "+-------------+----------+------------+\n",
    "Output: \n",
    "+-------------+\n",
    "| product_id  |\n",
    "+-------------+\n",
    "| 1           |\n",
    "| 3           |\n",
    "+-------------+\n",
    "Explanation: Only products 1 and 3 are both low fat and recyclable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b3484e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fat_df = spark.read.option('header',True).option('inferSchema',True).format('csv').load('../data/easymedium/001_RecyclableAndLowFatProducts.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ebc2ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- low_fats: string (nullable = true)\n",
      " |-- recyclable: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fat_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9ab139f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = fat_df.filter( (col('low_fats') == 'Y') & (col('recyclable') == 'Y') ).select('product_id').orderBy('product_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82287240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|product_id|\n",
      "+----------+\n",
      "|         1|\n",
      "|         3|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a21f08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fat_df.createTempView('fatdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d44bca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|product_id|\n",
      "+----------+\n",
      "|         1|\n",
      "|         3|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select product_id from fatdf where low_fats ='Y' and recyclable = 'Y' \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdca392",
   "metadata": {},
   "source": [
    "#### 002 Find Customer Referee\n",
    "\n",
    "Table: Customer\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| id          | int     |\n",
    "| name        | varchar |\n",
    "| referee_id  | int     |\n",
    "+-------------+---------+\n",
    "In SQL, id is the primary key column for this table.\n",
    "Each row of this table indicates the id of a customer, their name, and the id of the customer who referred them.\n",
    " \n",
    "\n",
    "Find the names of the customer that are not referred by the customer with id = 2.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Customer table:\n",
    "+----+------+------------+\n",
    "| id | name | referee_id |\n",
    "+----+------+------------+\n",
    "| 1  | Will | null       |\n",
    "| 2  | Jane | null       |\n",
    "| 3  | Alex | 2          |\n",
    "| 4  | Bill | null       |\n",
    "| 5  | Zack | 1          |\n",
    "| 6  | Mark | 2          |\n",
    "+----+------+------------+\n",
    "Output: \n",
    "+------+\n",
    "| name |\n",
    "+------+\n",
    "| Will |\n",
    "| Jane |\n",
    "| Bill |\n",
    "| Zack |\n",
    "+------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfdba1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_schema = StructType([\n",
    "    (StructField('id',IntegerType(),False)),\n",
    "    (StructField('name',StringType(),False)),\n",
    "    (StructField('referee_id',IntegerType(),True))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76efed29",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_df = spark.read.option('header',True).schema(ref_schema).format('csv').load('../data/easymedium/002_FindCustomerFeferre1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "39506b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- referee_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ref_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "50087761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+\n",
      "| id|name|referee_id|\n",
      "+---+----+----------+\n",
      "|  1|Will|      null|\n",
      "|  2|Jane|      null|\n",
      "|  3|Alex|         2|\n",
      "|  4|Bill|      null|\n",
      "|  5|Zack|         1|\n",
      "|  6|Mark|         2|\n",
      "+---+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ref_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "01b9f7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|name|referee_id|\n",
      "+----+----------+\n",
      "|Will|      null|\n",
      "|Jane|      null|\n",
      "|Bill|      null|\n",
      "|Zack|         1|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ref_df.filter( (col('referee_id') != 2) | (col('referee_id').isNull()) ).select(col('name'),col('referee_id')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ff1db0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_df.createTempView('refdf1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5f9b057d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|Will|\n",
      "|Jane|\n",
      "|Bill|\n",
      "|Zack|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select name from refdf1 where  referee_id is null or referee_id !=2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3da002",
   "metadata": {},
   "source": [
    "#### 003 Big Countries\n",
    "\n",
    "Table: World\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| name        | varchar |\n",
    "| continent   | varchar |\n",
    "| area        | int     |\n",
    "| population  | int     |\n",
    "| gdp         | bigint  |\n",
    "+-------------+---------+\n",
    "name is the primary key (column with unique values) for this table.\n",
    "Each row of this table gives information about the name of a country, the continent to which it belongs, its area, the population, and its GDP value.\n",
    " \n",
    "\n",
    "A country is big if:\n",
    "\n",
    "it has an area of at least three million (i.e., 3000000 km2), or\n",
    "it has a population of at least twenty-five million (i.e., 25000000).\n",
    "Write a solution to find the name, population, and area of the big countries.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "World table:\n",
    "+-------------+-----------+---------+------------+--------------+\n",
    "| name        | continent | area    | population | gdp          |\n",
    "+-------------+-----------+---------+------------+--------------+\n",
    "| Afghanistan | Asia      | 652230  | 25500100   | 20343000000  |\n",
    "| Albania     | Europe    | 28748   | 2831741    | 12960000000  |\n",
    "| Algeria     | Africa    | 2381741 | 37100000   | 188681000000 |\n",
    "| Andorra     | Europe    | 468     | 78115      | 3712000000   |\n",
    "| Angola      | Africa    | 1246700 | 20609294   | 100990000000 |\n",
    "+-------------+-----------+---------+------------+--------------+\n",
    "Output: \n",
    "+-------------+------------+---------+\n",
    "| name        | population | area    |\n",
    "+-------------+------------+---------+\n",
    "| Afghanistan | 25500100   | 652230  |\n",
    "| Algeria     | 37100000   | 2381741 |\n",
    "+-------------+------------+---------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "debe1408",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_df = spark.read.option('header',True).option('inferSchema',True).format('csv').load('../data/easymedium/003_BigCountries.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d58e85da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- continet: string (nullable = true)\n",
      " |-- area: integer (nullable = true)\n",
      " |-- population: integer (nullable = true)\n",
      " |-- gdp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "33ef70a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+----------+------------+\n",
      "|       name|continet|   area|population|         gdp|\n",
      "+-----------+--------+-------+----------+------------+\n",
      "|Afghanistan|    Asia| 652230|  25500100| 20343000000|\n",
      "|    Albania|  Europe|  28748|   2831741| 12960000000|\n",
      "|    Algeria|  Africa|2381741|  37100000|188681000000|\n",
      "|    Andorra|  Europe|    468|     78115|  3712000000|\n",
      "|     Angola|  Africa|1246700|  20609294|100990000000|\n",
      "+-----------+--------+-------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "05374020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------+\n",
      "|       name|population|   area|\n",
      "+-----------+----------+-------+\n",
      "|Afghanistan|  25500100| 652230|\n",
      "|    Algeria|  37100000|2381741|\n",
      "+-----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries_df.filter( (col('area') >= 3000000) | (col('population') >= 25000000) )\\\n",
    "                    .select(col('name'),col('population'),col('area') ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3ce2afe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_df.createTempView('countriesdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ade78a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------+\n",
      "|       name|population|   area|\n",
      "+-----------+----------+-------+\n",
      "|Afghanistan|  25500100| 652230|\n",
      "|    Algeria|  37100000|2381741|\n",
      "+-----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select name,population,area from countriesdf where area >=3000000 OR population >= 25000000 \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2724dae7",
   "metadata": {},
   "source": [
    "#### 004 Article Views\n",
    "\n",
    "Table: Views\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| article_id    | int     |\n",
    "| author_id     | int     |\n",
    "| viewer_id     | int     |\n",
    "| view_date     | date    |\n",
    "+---------------+---------+\n",
    "There is no primary key (column with unique values) for this table, the table may have duplicate rows.\n",
    "Each row of this table indicates that some viewer viewed an article (written by some author) on some date. \n",
    "Note that equal author_id and viewer_id indicate the same person.\n",
    " \n",
    "\n",
    "Write a solution to find all the authors that viewed at least one of their own articles.\n",
    "\n",
    "Return the result table sorted by id in ascending order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Views table:\n",
    "+------------+-----------+-----------+------------+\n",
    "| article_id | author_id | viewer_id | view_date  |\n",
    "+------------+-----------+-----------+------------+\n",
    "| 1          | 3         | 5         | 2019-08-01 |\n",
    "| 1          | 3         | 6         | 2019-08-02 |\n",
    "| 2          | 7         | 7         | 2019-08-01 |\n",
    "| 2          | 7         | 6         | 2019-08-02 |\n",
    "| 4          | 7         | 1         | 2019-07-22 |\n",
    "| 3          | 4         | 4         | 2019-07-21 |\n",
    "| 3          | 4         | 4         | 2019-07-21 |\n",
    "+------------+-----------+-----------+------------+\n",
    "Output: \n",
    "+------+\n",
    "| id   |\n",
    "+------+\n",
    "| 4    |\n",
    "| 7    |\n",
    "+------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8bc0526",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_schema = StructType([\n",
    "    (StructField('article_id',IntegerType(),True)),\n",
    "    (StructField('author_id',IntegerType(),True)),\n",
    "    (StructField('viewer_id',IntegerType(),True)),\n",
    "    (StructField('view_date',DateType(),True))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25b25e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df = spark.read.option('header',True).schema(article_schema).format('csv').load('../data/easymedium/004_ArticleViews.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5e27b61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- article_id: integer (nullable = true)\n",
      " |-- author_id: integer (nullable = true)\n",
      " |-- viewer_id: integer (nullable = true)\n",
      " |-- view_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "article_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "daed7371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|author_id|\n",
      "+---------+\n",
      "|        7|\n",
      "|        4|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "article_df.filter((col('author_id') == col('viewer_id'))).groupBy('author_id').count()\\\n",
    "        .select('author_id').orderBy(col('author_id').desc()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d816271",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df.createTempView('articledf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c3f6f1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|author_id|\n",
      "+---------+\n",
      "|        7|\n",
      "|        4|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select author_id from articledf where author_id = viewer_id group by author_id \\\n",
    "            having count(*) >=1 order by author_id desc' ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e709001",
   "metadata": {},
   "source": [
    "#### 005 Invalid Tweets\n",
    "\n",
    "Table: Tweets\n",
    "\n",
    "+----------------+---------+\n",
    "| Column Name    | Type    |\n",
    "+----------------+---------+\n",
    "| tweet_id       | int     |\n",
    "| content        | varchar |\n",
    "+----------------+---------+\n",
    "tweet_id is the primary key (column with unique values) for this table.\n",
    "This table contains all the tweets in a social media app.\n",
    " \n",
    "\n",
    "Write a solution to find the IDs of the invalid tweets. The tweet is invalid if the number of characters used in the content of the tweet is strictly greater than 15.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Tweets table:\n",
    "+----------+----------------------------------+\n",
    "| tweet_id | content                          |\n",
    "+----------+----------------------------------+\n",
    "| 1        | Vote for Biden                   |\n",
    "| 2        | Let us make America great again! |\n",
    "+----------+----------------------------------+\n",
    "Output: \n",
    "+----------+\n",
    "| tweet_id |\n",
    "+----------+\n",
    "| 2        |\n",
    "+----------+\n",
    "Explanation: \n",
    "Tweet 1 has length = 14. It is a valid tweet.\n",
    "Tweet 2 has length = 32. It is an invalid tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b4775be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = spark.read.option('header',True).option('inferSchema',True).format('csv').load('../data/easymedium/005_InvalidTweets.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1d23b279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tweet_id: integer (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "329a4e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|tweet_id|\n",
      "+--------+\n",
      "|       2|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_df.filter( length(col('content')) > 15).select('tweet_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "7c4005bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.createTempView('tweetsdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "71934377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|tweet_id|\n",
      "+--------+\n",
      "|       2|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select tweet_id from tweetsdf where length(content) > 15\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb33b79e",
   "metadata": {},
   "source": [
    "006 Replace Employee ID With The Unique Identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92e7a6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/006_ReplaceEmployeeIDWithTheUniqueIdentifier.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c22c23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_uni_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/006a_ReplaceEmployeeIDWithTheUniqueIdentifier.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ea34de5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "4099a527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- unique_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_uni_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "5c2a4250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|    name|\n",
      "+---+--------+\n",
      "|  1|   Alice|\n",
      "|  7|     Bob|\n",
      "| 11|    Meir|\n",
      "| 90| Winston|\n",
      "|  3|Jonathan|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a0c0ef9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|unique_id|\n",
      "+---+---------+\n",
      "|  3|        1|\n",
      "| 11|        2|\n",
      "| 90|        3|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_uni_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "bf0b8d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|unique_id|    name|\n",
      "+---------+--------+\n",
      "|     null|   Alice|\n",
      "|     null|     Bob|\n",
      "|        2|    Meir|\n",
      "|        3| Winston|\n",
      "|        1|Jonathan|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.withColumnRenamed('id','emp_id')\\\n",
    "    .join(employees_uni_df,employees_uni_df.id == col('emp_id'),'left').select('unique_id','name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "7e022c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_df.createTempView('employeesdf')\n",
    "employees_uni_df.createTempView('employeesunidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "2440fcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|unique_id|    name|\n",
      "+---------+--------+\n",
      "|     null|   Alice|\n",
      "|     null|     Bob|\n",
      "|        2|    Meir|\n",
      "|        3| Winston|\n",
      "|        1|Jonathan|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select unique_id,name from employeesdf left join employeesunidf on employeesdf.id = employeesunidf.id\")\\\n",
    "            .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0213a2f",
   "metadata": {},
   "source": [
    "#### 007 Product Sales Analysis\n",
    "\n",
    "Table: Sales\n",
    "\n",
    "+-------------+-------+\n",
    "| Column Name | Type  |\n",
    "+-------------+-------+\n",
    "| sale_id     | int   |\n",
    "| product_id  | int   |\n",
    "| year        | int   |\n",
    "| quantity    | int   |\n",
    "| price       | int   |\n",
    "+-------------+-------+\n",
    "(sale_id, year) is the primary key (combination of columns with unique values) of this table.\n",
    "product_id is a foreign key (reference column) to Product table.\n",
    "Each row of this table shows a sale on the product product_id in a certain year.\n",
    "Note that the price is per unit.\n",
    " \n",
    "\n",
    "Table: Product\n",
    "\n",
    "+--------------+---------+\n",
    "| Column Name  | Type    |\n",
    "+--------------+---------+\n",
    "| product_id   | int     |\n",
    "| product_name | varchar |\n",
    "+--------------+---------+\n",
    "product_id is the primary key (column with unique values) of this table.\n",
    "Each row of this table indicates the product name of each product.\n",
    " \n",
    "\n",
    "Write a solution to report the product_name, year, and price for each sale_id in the Sales table.\n",
    "\n",
    "Return the resulting table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Sales table:\n",
    "+---------+------------+------+----------+-------+\n",
    "| sale_id | product_id | year | quantity | price |\n",
    "+---------+------------+------+----------+-------+ \n",
    "| 1       | 100        | 2008 | 10       | 5000  |\n",
    "| 2       | 100        | 2009 | 12       | 5000  |\n",
    "| 7       | 200        | 2011 | 15       | 9000  |\n",
    "+---------+------------+------+----------+-------+\n",
    "Product table:\n",
    "+------------+--------------+\n",
    "| product_id | product_name |\n",
    "+------------+--------------+\n",
    "| 100        | Nokia        |\n",
    "| 200        | Apple        |\n",
    "| 300        | Samsung      |\n",
    "+------------+--------------+\n",
    "Output: \n",
    "+--------------+-------+-------+\n",
    "| product_name | year  | price |\n",
    "+--------------+-------+-------+\n",
    "| Nokia        | 2008  | 5000  |\n",
    "| Nokia        | 2009  | 5000  |\n",
    "| Apple        | 2011  | 9000  |\n",
    "+--------------+-------+-------+\n",
    "Explanation: \n",
    "From sale_id = 1, we can conclude that Nokia was sold for 5000 in the year 2008.\n",
    "From sale_id = 2, we can conclude that Nokia was sold for 5000 in the year 2009.\n",
    "From sale_id = 7, we can conclude that Apple was sold for 9000 in the year 2011."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99dbad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/007_ProductSalesAnalysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "791f6797",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/007a_ProductSalesAnalysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "963fb70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sale_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "8c33e203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "c0034fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+-----+\n",
      "|product_name|year|price|\n",
      "+------------+----+-----+\n",
      "|       Nokia|2008| 5000|\n",
      "|       Nokia|2009| 5000|\n",
      "|       Apple|2011| 9000|\n",
      "+------------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.join(product_df,product_df.product_id == sales_df.product_id,'inner')\\\n",
    "    .select('product_name','year','price').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "812c1fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df.createOrReplaceTempView('salesdf')\n",
    "product_df.createTempView('productdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "bf39cab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+-----+\n",
      "|product_name|year|price|\n",
      "+------------+----+-----+\n",
      "|       Nokia|2008| 5000|\n",
      "|       Nokia|2009| 5000|\n",
      "|       Apple|2011| 9000|\n",
      "+------------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select product_name,year,price from salesdf join productdf on salesdf.product_id = productdf.product_id\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaa623f",
   "metadata": {},
   "source": [
    "#### 008  Customer Who Visited but Did Not Make Any Transactions\n",
    "\n",
    "Table: Visits\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| visit_id    | int     |\n",
    "| customer_id | int     |\n",
    "+-------------+---------+\n",
    "visit_id is the column with unique values for this table.\n",
    "This table contains information about the customers who visited the mall.\n",
    " \n",
    "\n",
    "Table: Transactions\n",
    "\n",
    "+----------------+---------+\n",
    "| Column Name    | Type    |\n",
    "+----------------+---------+\n",
    "| transaction_id | int     |\n",
    "| visit_id       | int     |\n",
    "| amount         | int     |\n",
    "+----------------+---------+\n",
    "transaction_id is column with unique values for this table.\n",
    "This table contains information about the transactions made during the visit_id.\n",
    " \n",
    "\n",
    "Write a solution to find the IDs of the users who visited without making any transactions and the number of times they made these types of visits.\n",
    "\n",
    "Return the result table sorted in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Visits\n",
    "+----------+-------------+\n",
    "| visit_id | customer_id |\n",
    "+----------+-------------+\n",
    "| 1        | 23          |\n",
    "| 2        | 9           |\n",
    "| 4        | 30          |\n",
    "| 5        | 54          |\n",
    "| 6        | 96          |\n",
    "| 7        | 54          |\n",
    "| 8        | 54          |\n",
    "+----------+-------------+\n",
    "Transactions\n",
    "+----------------+----------+--------+\n",
    "| transaction_id | visit_id | amount |\n",
    "+----------------+----------+--------+\n",
    "| 2              | 5        | 310    |\n",
    "| 3              | 5        | 300    |\n",
    "| 9              | 5        | 200    |\n",
    "| 12             | 1        | 910    |\n",
    "| 13             | 2        | 970    |\n",
    "+----------------+----------+--------+\n",
    "Output: \n",
    "+-------------+----------------+\n",
    "| customer_id | count_no_trans |\n",
    "+-------------+----------------+\n",
    "| 54          | 2              |\n",
    "| 30          | 1              |\n",
    "| 96          | 1              |\n",
    "+-------------+----------------+\n",
    "Explanation: \n",
    "Customer with id = 23 visited the mall once and made one transaction during the visit with id = 12.\n",
    "Customer with id = 9 visited the mall once and made one transaction during the visit with id = 13.\n",
    "Customer with id = 30 visited the mall once and did not make any transactions.\n",
    "Customer with id = 54 visited the mall three times. During 2 visits they did not make any transactions, and during one visit they made 3 transactions.\n",
    "Customer with id = 96 visited the mall once and did not make any transactions.\n",
    "As we can see, users with IDs 30 and 96 visited the mall one time without making any transactions. Also, user 54 visited the mall twice and did not make any transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1bf69bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/008_CustomerWhoVisitedbutDidNotMakeAnyTransactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae34bca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactiions_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/008a_CustomerWhoVisitedbutDidNotMakeAnyTransactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "2b910e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- visit_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visits_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b7c4ade7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|visit_id|customer_id|\n",
      "+--------+-----------+\n",
      "|       1|         23|\n",
      "|       2|          9|\n",
      "|       4|         30|\n",
      "|       5|         54|\n",
      "|       6|         96|\n",
      "|       7|         54|\n",
      "|       8|         54|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visits_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "65d18a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------+\n",
      "|transaction_id|visit_id|amount|\n",
      "+--------------+--------+------+\n",
      "|             2|       5|   310|\n",
      "|             3|       5|   300|\n",
      "|             9|       5|   200|\n",
      "|            12|       1|   910|\n",
      "|            13|       2|   970|\n",
      "+--------------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactiions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "299dbfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- visit_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactiions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "7d09760e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|customer_id|count|\n",
      "+-----------+-----+\n",
      "|         54|    2|\n",
      "|         96|    1|\n",
      "|         30|    1|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visits_df.join(transactiions_df,visits_df.visit_id  == transactiions_df.visit_id,'anti')\\\n",
    "    .groupBy('customer_id').agg(count('*').alias('count')).select('customer_id',col('count')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "0a479875",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits_df.createOrReplaceTempView('visitsdf')\n",
    "transactiions_df.createOrReplaceTempView('transactiionsdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "fecdc934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|customer_id|count(1)|\n",
      "+-----------+--------+\n",
      "|         54|       2|\n",
      "|         96|       1|\n",
      "|         30|       1|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select customer_id,count(*) from visitsdf \\\n",
    "    where visit_id not in (select visit_id from transactiionsdf) \\\n",
    "    group by customer_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4751f3eb",
   "metadata": {},
   "source": [
    "#### 009 Rising Temperature\n",
    "\n",
    "Table: Weather\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| id            | int     |\n",
    "| recordDate    | date    |\n",
    "| temperature   | int     |\n",
    "+---------------+---------+\n",
    "id is the column with unique values for this table.\n",
    "There are no different rows with the same recordDate.\n",
    "This table contains information about the temperature on a certain day.\n",
    " \n",
    "\n",
    "Write a solution to find all dates' Id with higher temperatures compared to its previous dates (yesterday).\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Weather table:\n",
    "+----+------------+-------------+\n",
    "| id | recordDate | temperature |\n",
    "+----+------------+-------------+\n",
    "| 1  | 2015-01-01 | 10          |\n",
    "| 2  | 2015-01-02 | 25          |\n",
    "| 3  | 2015-01-03 | 20          |\n",
    "| 4  | 2015-01-04 | 30          |\n",
    "+----+------------+-------------+\n",
    "Output: \n",
    "+----+\n",
    "| id |\n",
    "+----+\n",
    "| 2  |\n",
    "| 4  |\n",
    "+----+\n",
    "Explanation: \n",
    "In 2015-01-02, the temperature was higher than the previous day (10 -> 25).\n",
    "In 2015-01-04, the temperature was higher than the previous day (20 -> 30)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "077be40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_schema = StructType([\n",
    "    (StructField('id',IntegerType(),False)),\n",
    "    (StructField('recordDate',DateType(),False)),\n",
    "    (StructField('temperature',IntegerType(),True))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1b10e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_df = spark.read.option('header',True).schema(temperature_schema).format('csv')\\\n",
    "        .load('../data/easymedium/009_RisingTemperature.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "6bc36191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- recordDate: date (nullable = true)\n",
      " |-- temperature: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "bd921463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+\n",
      "| id|recordDate|temperature|\n",
      "+---+----------+-----------+\n",
      "|  1|2015-01-01|         10|\n",
      "|  2|2015-01-02|         25|\n",
      "|  3|2015-01-03|         20|\n",
      "|  4|2015-01-04|         30|\n",
      "+---+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "1194b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = temperature_df\n",
    "w2 = temperature_df.withColumnRenamed('recordDate','twodate')\\\n",
    "                    .withColumnRenamed('temperature','temp')\\\n",
    "                    .withColumnRenamed('id','temp_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "67322b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|temp_id|\n",
      "+-------+\n",
      "|      2|\n",
      "|      4|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w1.join(w2,datediff(w1.recordDate,w2.twodate)==-1).filter(col('temp') > col('temperature'))\\\n",
    "            .select(col('temp_id')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "7c5b8f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_df.createOrReplaceTempView('tempone')\n",
    "temperature_df.createOrReplaceTempView('recordDate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "6eb41f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+-----------+-----------+\n",
      "| id|recordDate|recordDate|temperature|temperature|\n",
      "+---+----------+----------+-----------+-----------+\n",
      "|  2|2015-01-01|2015-01-02|         25|         10|\n",
      "|  4|2015-01-03|2015-01-04|         30|         20|\n",
      "+---+----------+----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select temptwo.id,tempone.recordDate,temptwo.recordDate,temptwo.temperature,tempone.temperature \\\n",
    "        from tempone join temptwo on datediff(tempone.recordDate,temptwo.recordDate)=-1 \\\n",
    "        where temptwo.temperature > tempone.temperature\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99e18b4",
   "metadata": {},
   "source": [
    "#### 010 Average Time of Process per Machine\n",
    "\n",
    "Table: Activity\n",
    "\n",
    "+----------------+---------+\n",
    "| Column Name    | Type    |\n",
    "+----------------+---------+\n",
    "| machine_id     | int     |\n",
    "| process_id     | int     |\n",
    "| activity_type  | enum    |\n",
    "| timestamp      | float   |\n",
    "+----------------+---------+\n",
    "The table shows the user activities for a factory website.\n",
    "(machine_id, process_id, activity_type) is the primary key (combination of columns with unique values) of this table.\n",
    "machine_id is the ID of a machine.\n",
    "process_id is the ID of a process running on the machine with ID machine_id.\n",
    "activity_type is an ENUM (category) of type ('start', 'end').\n",
    "timestamp is a float representing the current time in seconds.\n",
    "'start' means the machine starts the process at the given timestamp and 'end' means the machine ends the process at the given timestamp.\n",
    "The 'start' timestamp will always be before the 'end' timestamp for every (machine_id, process_id) pair.\n",
    " \n",
    "\n",
    "There is a factory website that has several machines each running the same number of processes. Write a solution to find the average time each machine takes to complete a process.\n",
    "\n",
    "The time to complete a process is the 'end' timestamp minus the 'start' timestamp. The average time is calculated by the total time to complete every process on the machine divided by the number of processes that were run.\n",
    "\n",
    "The resulting table should have the machine_id along with the average time as processing_time, which should be rounded to 3 decimal places.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Activity table:\n",
    "+------------+------------+---------------+-----------+\n",
    "| machine_id | process_id | activity_type | timestamp |\n",
    "+------------+------------+---------------+-----------+\n",
    "| 0          | 0          | start         | 0.712     |\n",
    "| 0          | 0          | end           | 1.520     |\n",
    "| 0          | 1          | start         | 3.140     |\n",
    "| 0          | 1          | end           | 4.120     |\n",
    "| 1          | 0          | start         | 0.550     |\n",
    "| 1          | 0          | end           | 1.550     |\n",
    "| 1          | 1          | start         | 0.430     |\n",
    "| 1          | 1          | end           | 1.420     |\n",
    "| 2          | 0          | start         | 4.100     |\n",
    "| 2          | 0          | end           | 4.512     |\n",
    "| 2          | 1          | start         | 2.500     |\n",
    "| 2          | 1          | end           | 5.000     |\n",
    "+------------+------------+---------------+-----------+\n",
    "Output: \n",
    "+------------+-----------------+\n",
    "| machine_id | processing_time |\n",
    "+------------+-----------------+\n",
    "| 0          | 0.894           |\n",
    "| 1          | 0.995           |\n",
    "| 2          | 1.456           |\n",
    "+------------+-----------------+\n",
    "Explanation: \n",
    "There are 3 machines running 2 processes each.\n",
    "Machine 0's average time is ((1.520 - 0.712) + (4.120 - 3.140)) / 2 = 0.894\n",
    "Machine 1's average time is ((1.550 - 0.550) + (1.420 - 0.430)) / 2 = 0.995\n",
    "Machine 2's average time is ((4.512 - 4.100) + (5.000 - 2.500)) / 2 = 1.456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "27b59fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/010_AverageTimeofProcessperMachine.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "5bb5878d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- machine_id: integer (nullable = true)\n",
      " |-- process_id: integer (nullable = true)\n",
      " |-- activity_type: string (nullable = true)\n",
      " |-- timestamp: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "machine_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "be0a416b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------+---------+\n",
      "|machine_id|process_id|activity_type|timestamp|\n",
      "+----------+----------+-------------+---------+\n",
      "|         0|         0|        start|    0.712|\n",
      "|         0|         0|          end|     1.52|\n",
      "|         0|         1|        start|     3.14|\n",
      "|         0|         1|          end|     4.12|\n",
      "|         1|         0|        start|     0.55|\n",
      "|         1|         0|          end|     1.55|\n",
      "|         1|         1|        start|     0.43|\n",
      "|         1|         1|          end|     1.42|\n",
      "|         2|         0|        start|      4.1|\n",
      "|         2|         0|          end|    4.512|\n",
      "|         2|         1|        start|      2.5|\n",
      "|         2|         1|          end|      5.0|\n",
      "+----------+----------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "machine_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "3ffe1a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_df = machine_df.filter(col('activity_type') == 'start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "0a66ba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_df = machine_df.filter(col('activity_type') == 'end').withColumnRenamed('machine_id','end_machine_id')\\\n",
    "        .withColumnRenamed('process_id','end_process_id')\\\n",
    "        .withColumnRenamed('activity_type','end_activity_type')\\\n",
    "        .withColumnRenamed('timestamp','end_timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "28d7bbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------+---------+\n",
      "|machine_id|process_id|activity_type|timestamp|\n",
      "+----------+----------+-------------+---------+\n",
      "|         0|         0|        start|    0.712|\n",
      "|         0|         1|        start|     3.14|\n",
      "|         1|         0|        start|     0.55|\n",
      "|         1|         1|        start|     0.43|\n",
      "|         2|         0|        start|      4.1|\n",
      "|         2|         1|        start|      2.5|\n",
      "+----------+----------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "5e25da72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+-----------------+-------------+\n",
      "|end_machine_id|end_process_id|end_activity_type|end_timestamp|\n",
      "+--------------+--------------+-----------------+-------------+\n",
      "|             0|             0|              end|         1.52|\n",
      "|             0|             1|              end|         4.12|\n",
      "|             1|             0|              end|         1.55|\n",
      "|             1|             1|              end|         1.42|\n",
      "|             2|             0|              end|        4.512|\n",
      "|             2|             1|              end|          5.0|\n",
      "+--------------+--------------+-----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "end_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54800b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "00ae3876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|machine_id|avg_process_tiime|\n",
      "+----------+-----------------+\n",
      "|         0|            0.894|\n",
      "|         1|            0.995|\n",
      "|         2|            1.456|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_df.join(end_df, (start_df.machine_id == end_df.end_machine_id) \\\n",
    "                    & (start_df.process_id == end_df.end_process_id),'inner')\\\n",
    "                    .withColumn('process_time',col('end_timestamp') - col('timestamp'))\\\n",
    "                    .groupby('machine_id')\\\n",
    "                    .agg(round(avg(col('process_time')),3).alias('avg_process_tiime'))\\\n",
    "                    .orderBy(col('machine_id').asc())\\\n",
    "                    .show()\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "7308fb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_df.createOrReplaceTempView('startdf')\n",
    "end_df.createOrReplaceTempView('enddf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "2dd33e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|machine_id|average_process_time|\n",
      "+----------+--------------------+\n",
      "|         0|               0.894|\n",
      "|         1|               0.995|\n",
      "|         2|               1.456|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" select  machine_id, round(avg(end_timestamp - timestamp),3) as average_process_time \\\n",
    "            from startdf join enddf on machine_id = end_machine_id AND process_id = end_process_id \\\n",
    "            group by machine_id \\\n",
    "            order by machine_id asc\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "63c9a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_df.createOrReplaceTempView('machinedf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878cb063",
   "metadata": {},
   "outputs": [],
   "source": [
    "select machine_id, round(avg(time_lapse), 3) as processing_time\n",
    "from\n",
    "(select machine_id, process_id,\n",
    "max(timestamp) over (partition by machine_id, process_id) \n",
    "-\n",
    "min(timestamp) over (partition by machine_id, process_id) \n",
    "as time_lapse from Activity) temp\n",
    "group by machine_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "6fa2c6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|machine_id|average_processing_time|\n",
      "+----------+-----------------------+\n",
      "|         0|                  0.894|\n",
      "|         1|                  0.995|\n",
      "|         2|                  1.456|\n",
      "+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" select machine_id,round(avg(time_lapse),3) as average_processing_time from  (select machine_id,process_id,max(timestamp) OVER(partition by machine_id,process_id) \\\n",
    "                            - min(timestamp) OVER(partition by machine_id,process_id) as time_lapse \\\n",
    "                              from machinedf) temp \\\n",
    "                              group by machine_id \\\n",
    "                              order by machine_id \") .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037eb814",
   "metadata": {},
   "source": [
    "#### 011 Employee Bonus\n",
    "\n",
    "Table: Employee\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| empId       | int     |\n",
    "| name        | varchar |\n",
    "| supervisor  | int     |\n",
    "| salary      | int     |\n",
    "+-------------+---------+\n",
    "empId is the column with unique values for this table.\n",
    "Each row of this table indicates the name and the ID of an employee in addition to their salary and the id of their manager.\n",
    " \n",
    "\n",
    "Table: Bonus\n",
    "\n",
    "+-------------+------+\n",
    "| Column Name | Type |\n",
    "+-------------+------+\n",
    "| empId       | int  |\n",
    "| bonus       | int  |\n",
    "+-------------+------+\n",
    "empId is the column of unique values for this table.\n",
    "empId is a foreign key (reference column) to empId from the Employee table.\n",
    "Each row of this table contains the id of an employee and their respective bonus.\n",
    " \n",
    "\n",
    "Write a solution to report the name and bonus amount of each employee with a bonus less than 1000.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Employee table:\n",
    "+-------+--------+------------+--------+\n",
    "| empId | name   | supervisor | salary |\n",
    "+-------+--------+------------+--------+\n",
    "| 3     | Brad   | null       | 4000   |\n",
    "| 1     | John   | 3          | 1000   |\n",
    "| 2     | Dan    | 3          | 2000   |\n",
    "| 4     | Thomas | 3          | 4000   |\n",
    "+-------+--------+------------+--------+\n",
    "Bonus table:\n",
    "+-------+-------+\n",
    "| empId | bonus |\n",
    "+-------+-------+\n",
    "| 2     | 500   |\n",
    "| 4     | 2000  |\n",
    "+-------+-------+\n",
    "Output: \n",
    "+------+-------+\n",
    "| name | bonus |\n",
    "+------+-------+\n",
    "| Brad | null  |\n",
    "| John | null  |\n",
    "| Dan  | 500   |\n",
    "+------+-------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12ca1358",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/011_EmployeeBonus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8a829e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "bonus_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/011a_EmployeeBonus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4271314b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empId: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- supervisor: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f22cc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empId: integer (nullable = true)\n",
      " |-- bonus: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bonus_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "028d0760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|name|bonus|\n",
      "+----+-----+\n",
      "|Brad| null|\n",
      "|John| null|\n",
      "| Dan|  500|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.join(bonus_df,emp_df.empId == bonus_df.empId,'left')\\\n",
    "        .filter( (col('bonus').isNull()) | (col('bonus') < 1000)).select('name','bonus').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b395064",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df.createOrReplaceTempView('empdf')\n",
    "bonus_df.createOrReplaceTempView('bonusdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1274fa9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|name|bonus|\n",
      "+----+-----+\n",
      "|Brad| null|\n",
      "|John| null|\n",
      "| Dan|  500|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select name,bonus from empdf left join bonusdf on empdf.empId = bonusdf.empId \\\n",
    "        where (bonus is null) OR (bonus < 1000 ) \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4ce12a",
   "metadata": {},
   "source": [
    "#### 012 Students and Examinations \n",
    "\n",
    "Table: Students\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| student_id    | int     |\n",
    "| student_name  | varchar |\n",
    "+---------------+---------+\n",
    "student_id is the primary key (column with unique values) for this table.\n",
    "Each row of this table contains the ID and the name of one student in the school.\n",
    " \n",
    "\n",
    "Table: Subjects\n",
    "\n",
    "+--------------+---------+\n",
    "| Column Name  | Type    |\n",
    "+--------------+---------+\n",
    "| subject_name | varchar |\n",
    "+--------------+---------+\n",
    "subject_name is the primary key (column with unique values) for this table.\n",
    "Each row of this table contains the name of one subject in the school.\n",
    " \n",
    "\n",
    "Table: Examinations\n",
    "\n",
    "+--------------+---------+\n",
    "| Column Name  | Type    |\n",
    "+--------------+---------+\n",
    "| student_id   | int     |\n",
    "| subject_name | varchar |\n",
    "+--------------+---------+\n",
    "There is no primary key (column with unique values) for this table. It may contain duplicates.\n",
    "Each student from the Students table takes every course from the Subjects table.\n",
    "Each row of this table indicates that a student with ID student_id attended the exam of subject_name.\n",
    " \n",
    "\n",
    "Write a solution to find the number of times each student attended each exam.\n",
    "\n",
    "Return the result table ordered by student_id and subject_name.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Students table:\n",
    "+------------+--------------+\n",
    "| student_id | student_name |\n",
    "+------------+--------------+\n",
    "| 1          | Alice        |\n",
    "| 2          | Bob          |\n",
    "| 13         | John         |\n",
    "| 6          | Alex         |\n",
    "+------------+--------------+\n",
    "Subjects table:\n",
    "+--------------+\n",
    "| subject_name |\n",
    "+--------------+\n",
    "| Math         |\n",
    "| Physics      |\n",
    "| Programming  |\n",
    "+--------------+\n",
    "Examinations table:\n",
    "+------------+--------------+\n",
    "| student_id | subject_name |\n",
    "+------------+--------------+\n",
    "| 1          | Math         |\n",
    "| 1          | Physics      |\n",
    "| 1          | Programming  |\n",
    "| 2          | Programming  |\n",
    "| 1          | Physics      |\n",
    "| 1          | Math         |\n",
    "| 13         | Math         |\n",
    "| 13         | Programming  |\n",
    "| 13         | Physics      |\n",
    "| 2          | Math         |\n",
    "| 1          | Math         |\n",
    "+------------+--------------+\n",
    "Output: \n",
    "+------------+--------------+--------------+----------------+\n",
    "| student_id | student_name | subject_name | attended_exams |\n",
    "+------------+--------------+--------------+----------------+\n",
    "| 1          | Alice        | Math         | 3              |\n",
    "| 1          | Alice        | Physics      | 2              |\n",
    "| 1          | Alice        | Programming  | 1              |\n",
    "| 2          | Bob          | Math         | 1              |\n",
    "| 2          | Bob          | Physics      | 0              |\n",
    "| 2          | Bob          | Programming  | 1              |\n",
    "| 6          | Alex         | Math         | 0              |\n",
    "| 6          | Alex         | Physics      | 0              |\n",
    "| 6          | Alex         | Programming  | 0              |\n",
    "| 13         | John         | Math         | 1              |\n",
    "| 13         | John         | Physics      | 1              |\n",
    "| 13         | John         | Programming  | 1              |\n",
    "+------------+--------------+--------------+----------------+\n",
    "Explanation: \n",
    "The result table should contain all students and all subjects.\n",
    "Alice attended the Math exam 3 times, the Physics exam 2 times, and the Programming exam 1 time.\n",
    "Bob attended the Math exam 1 time, the Programming exam 1 time, and did not attend the Physics exam.\n",
    "Alex did not attend any exams.\n",
    "John attended the Math exam 1 time, the Physics exam 1 time, and the Programming exam 1 time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8774a220",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/012_StudentsandExaminations.csv')\\\n",
    "        .withColumnRenamed('student_id','stu_student_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd5d3284",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/012a_StudentsandExaminations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "40ad6bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "examp_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/012b_StudentsandExaminations.csv')\\\n",
    "        .withColumnRenamed('student_id','ex_student_id') \\\n",
    "        .withColumnRenamed('subject_name','ex_subject_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "620f2584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- stu_student_id: integer (nullable = true)\n",
      " |-- student_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c646650c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- subject_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subject_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1abc6937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ex_student_id: integer (nullable = true)\n",
      " |-- ex_subject_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0607e27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+---------------+---------------+\n",
      "|stu_student_id|student_name|ex_subject_name|attended_exames|\n",
      "+--------------+------------+---------------+---------------+\n",
      "|             1|       Alice|           Math|              3|\n",
      "|             1|       Alice|        Physics|              2|\n",
      "|             1|       Alice|    Programming|              1|\n",
      "|             2|         Bob|           Math|              1|\n",
      "|             2|         Bob|    Programming|              1|\n",
      "|            13|        John|           Math|              1|\n",
      "|            13|        John|        Physics|              1|\n",
      "|            13|        John|    Programming|              1|\n",
      "+--------------+------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examp_df.join(student_df,col('ex_student_id') == col('stu_student_id'),'inner')\\\n",
    "        .join(subject_df,col('ex_subject_name') == col('subject_name'),'inner')\\\n",
    "        .groupBy('stu_student_id','student_name','ex_subject_name')\\\n",
    "        .agg(count('ex_subject_name').alias('attended_exames'))\\\n",
    "        .select('stu_student_id','student_name','ex_subject_name',col('attended_exames'))\\\n",
    "        .orderBy(col('stu_student_id'),col('ex_subject_name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "59cbc2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+------------+\n",
      "|stu_student_id|student_name|subject_name|\n",
      "+--------------+------------+------------+\n",
      "|             1|       Alice|        Math|\n",
      "|             1|       Alice|     Physics|\n",
      "|             1|       Alice| Programming|\n",
      "|             2|         Bob|        Math|\n",
      "|             2|         Bob|     Physics|\n",
      "|             2|         Bob| Programming|\n",
      "|            13|        John|        Math|\n",
      "|            13|        John|     Physics|\n",
      "|            13|        John| Programming|\n",
      "|             6|        Alex|        Math|\n",
      "|             6|        Alex|     Physics|\n",
      "|             6|        Alex| Programming|\n",
      "+--------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student_df.join(subject_df)\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b3c74094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+------------+---------------+\n",
      "|stu_student_id|student_name|subject_name|attended_exames|\n",
      "+--------------+------------+------------+---------------+\n",
      "|             1|       Alice|        Math|              3|\n",
      "|             1|       Alice|     Physics|              2|\n",
      "|             1|       Alice| Programming|              1|\n",
      "|             2|         Bob|        Math|              1|\n",
      "|             2|         Bob|     Physics|              0|\n",
      "|             2|         Bob| Programming|              1|\n",
      "|             6|        Alex|        Math|              0|\n",
      "|             6|        Alex|     Physics|              0|\n",
      "|             6|        Alex| Programming|              0|\n",
      "|            13|        John|        Math|              1|\n",
      "|            13|        John|     Physics|              1|\n",
      "|            13|        John| Programming|              1|\n",
      "+--------------+------------+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student_df.join(subject_df)\\\n",
    "    .join(examp_df, (col('ex_student_id') == col('stu_student_id'))\n",
    "          & (col('subject_name') == col('ex_subject_name') ),'left')\\\n",
    "        .groupBy('stu_student_id','student_name','subject_name','ex_subject_name')\\\n",
    "        .agg(count('ex_subject_name').alias('attended_exames'))\\\n",
    "        .select('stu_student_id','student_name','subject_name',col('attended_exames'))\\\n",
    "        .orderBy(col('stu_student_id'),col('subject_name')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "9b056031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+------------+---------------+\n",
      "|stu_student_id|student_name|subject_name|attended_exames|\n",
      "+--------------+------------+------------+---------------+\n",
      "|             1|       Alice|        Math|              3|\n",
      "|             1|       Alice|     Physics|              2|\n",
      "|             1|       Alice| Programming|              1|\n",
      "|             2|         Bob|        Math|              1|\n",
      "|             2|         Bob|     Physics|              0|\n",
      "|             2|         Bob| Programming|              1|\n",
      "|             6|        Alex|        Math|              0|\n",
      "|             6|        Alex|     Physics|              0|\n",
      "|             6|        Alex| Programming|              0|\n",
      "|            13|        John|        Math|              1|\n",
      "|            13|        John|     Physics|              1|\n",
      "|            13|        John| Programming|              1|\n",
      "+--------------+------------+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student_df.createOrReplaceTempView('studentdf')\n",
    "subject_df.createOrReplaceTempView('subjectdf')\n",
    "examp_df.createOrReplaceTempView('exampdf')\n",
    "spark.sql(\"select stu_student_id,student_name,subject_name,count(ex_subject_name) as attended_exames\\\n",
    "        from  studentdf outer join subjectdf \\\n",
    "        left join exampdf on stu_student_id = ex_student_id AND subject_name = ex_subject_name\\\n",
    "        group by stu_student_id,student_name,subject_name \\\n",
    "        order by stu_student_id,subject_name\")\\\n",
    "        .show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05d274b",
   "metadata": {},
   "source": [
    "#### 013 Not Boring Movies\n",
    "\n",
    "Table: Cinema\n",
    "\n",
    "+----------------+----------+\n",
    "| Column Name    | Type     |\n",
    "+----------------+----------+\n",
    "| id             | int      |\n",
    "| movie          | varchar  |\n",
    "| description    | varchar  |\n",
    "| rating         | float    |\n",
    "+----------------+----------+\n",
    "id is the primary key (column with unique values) for this table.\n",
    "Each row contains information about the name of a movie, its genre, and its rating.\n",
    "rating is a 2 decimal places float in the range [0, 10]\n",
    " \n",
    "\n",
    "Write a solution to report the movies with an odd-numbered ID and a description that is not \"boring\".\n",
    "\n",
    "Return the result table ordered by rating in descending order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Cinema table:\n",
    "+----+------------+-------------+--------+\n",
    "| id | movie      | description | rating |\n",
    "+----+------------+-------------+--------+\n",
    "| 1  | War        | great 3D    | 8.9    |\n",
    "| 2  | Science    | fiction     | 8.5    |\n",
    "| 3  | irish      | boring      | 6.2    |\n",
    "| 4  | Ice song   | Fantacy     | 8.6    |\n",
    "| 5  | House card | Interesting | 9.1    |\n",
    "+----+------------+-------------+--------+\n",
    "Output: \n",
    "+----+------------+-------------+--------+\n",
    "| id | movie      | description | rating |\n",
    "+----+------------+-------------+--------+\n",
    "| 5  | House card | Interesting | 9.1    |\n",
    "| 1  | War        | great 3D    | 8.9    |\n",
    "+----+------------+-------------+--------+\n",
    "Explanation: \n",
    "We have three movies with odd-numbered IDs: 1, 3, and 5. The movie with ID = 3 is boring so we do not include it in the answer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e2becdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/013_NotBoringMovies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "8a93c2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- movie: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "2c255e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n",
      "| id|     movie|description|rating|\n",
      "+---+----------+-----------+------+\n",
      "|  5|House card|Interesting|   9.1|\n",
      "|  1|       War|   great 3D|   8.9|\n",
      "+---+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df.filter( (col('id')%2 == 1) & (col('description') != 'boring') )\\\n",
    "    .select('*').orderBy(col('rating').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "defcaecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.createOrReplaceTempView('moviesdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "085195ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n",
      "| id|     movie|description|rating|\n",
      "+---+----------+-----------+------+\n",
      "|  5|House card|Interesting|   9.1|\n",
      "|  1|       War|   great 3D|   8.9|\n",
      "+---+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from moviesdf \\\n",
    "        where id%2 = 1 AND description != 'boring' \\\n",
    "        order by rating desc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80af5bfd",
   "metadata": {},
   "source": [
    "#### 014 Average Selling Price  \n",
    "\n",
    "Table: Prices\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| product_id    | int     |\n",
    "| start_date    | date    |\n",
    "| end_date      | date    |\n",
    "| price         | int     |\n",
    "+---------------+---------+\n",
    "(product_id, start_date, end_date) is the primary key (combination of columns with unique values) for this table.\n",
    "Each row of this table indicates the price of the product_id in the period from start_date to end_date.\n",
    "For each product_id there will be no two overlapping periods. That means there will be no two intersecting periods for the same product_id.\n",
    " \n",
    "\n",
    "Table: UnitsSold\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| product_id    | int     |\n",
    "| purchase_date | date    |\n",
    "| units         | int     |\n",
    "+---------------+---------+\n",
    "This table may contain duplicate rows.\n",
    "Each row of this table indicates the date, units, and product_id of each product sold. \n",
    " \n",
    "\n",
    "Write a solution to find the average selling price for each product. average_price should be rounded to 2 decimal places.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Prices table:\n",
    "+------------+------------+------------+--------+\n",
    "| product_id | start_date | end_date   | price  |\n",
    "+------------+------------+------------+--------+\n",
    "| 1          | 2019-02-17 | 2019-02-28 | 5      |\n",
    "| 1          | 2019-03-01 | 2019-03-22 | 20     |\n",
    "| 2          | 2019-02-01 | 2019-02-20 | 15     |\n",
    "| 2          | 2019-02-21 | 2019-03-31 | 30     |\n",
    "+------------+------------+------------+--------+\n",
    "UnitsSold table:\n",
    "+------------+---------------+-------+\n",
    "| product_id | purchase_date | units |\n",
    "+------------+---------------+-------+\n",
    "| 1          | 2019-02-25    | 100   |\n",
    "| 1          | 2019-03-01    | 15    |\n",
    "| 2          | 2019-02-10    | 200   |\n",
    "| 2          | 2019-03-22    | 30    |\n",
    "+------------+---------------+-------+\n",
    "Output: \n",
    "+------------+---------------+\n",
    "| product_id | average_price |\n",
    "+------------+---------------+\n",
    "| 1          | 6.96          |\n",
    "| 2          | 16.96         |\n",
    "+------------+---------------+\n",
    "Explanation: \n",
    "Average selling price = Total Price of Product / Number of products sold.\n",
    "Average selling price for product 1 = ((100 * 5) + (15 * 20)) / 115 = 6.96\n",
    "Average selling price for product 2 = ((200 * 15) + (30 * 30)) / 230 = 16.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "004ac8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgprice_schema = StructType([\n",
    "    (StructField('product_id',IntegerType(),False)),\n",
    "    (StructField('start_date',DateType(),False)),\n",
    "    (StructField('end_date',DateType(),False)),\n",
    "    (StructField('price',IntegerType(),True))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3c6d8685",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgprice_df = spark.read.option('header',True).schema(avgprice_schema).format('csv')\\\n",
    "        .load('../data/easymedium/014_AverageSellingPrice.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "34b75ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sold_schema = StructType([\n",
    "    (StructField('product_id',IntegerType(),False)),\n",
    "    (StructField('purchase_date',DateType(),False)),\n",
    "    (StructField('units',IntegerType(),True))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9d922429",
   "metadata": {},
   "outputs": [],
   "source": [
    "sold_df = spark.read.option('header',True).schema(sold_schema).format('csv')\\\n",
    "        .load('../data/easymedium/014a_AverageSellingPrice.csv')\\\n",
    "        .withColumnRenamed('product_id','sol_product_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "29f6d304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- start_date: date (nullable = true)\n",
      " |-- end_date: date (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avgprice_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "937e94a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sol_product_id: integer (nullable = true)\n",
      " |-- purchase_date: date (nullable = true)\n",
      " |-- units: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sold_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "cd2cb233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+-----+\n",
      "|product_id|start_date|  end_date|price|\n",
      "+----------+----------+----------+-----+\n",
      "|         1|2019-02-17|2019-02-28|    5|\n",
      "|         1|2019-03-01|2019-03-22|   20|\n",
      "|         2|2019-02-01|2019-02-20|   15|\n",
      "|         2|2019-02-21|2019-03-31|   30|\n",
      "+----------+----------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avgprice_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "8e8d37aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+-----+--------------+-------------+-----+----+\n",
      "|product_id|start_date|  end_date|price|sol_product_id|purchase_date|units| mul|\n",
      "+----------+----------+----------+-----+--------------+-------------+-----+----+\n",
      "|         1|2019-02-17|2019-02-28|    5|             1|   2019-02-25|  100| 500|\n",
      "|         1|2019-03-01|2019-03-22|   20|             1|   2019-03-01|   15| 300|\n",
      "|         2|2019-02-01|2019-02-20|   15|             2|   2019-02-10|  200|3000|\n",
      "|         2|2019-02-21|2019-03-31|   30|             2|   2019-03-22|   30| 900|\n",
      "+----------+----------+----------+-----+--------------+-------------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avgprice_df.join(sold_df,col('product_id') == col('sol_product_id'),'inner')\\\n",
    "    .filter(col('purchase_date').between(col('start_date'),col('end_date')))\\\n",
    "    .withColumn('mul', col('price') * col('units'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "fdf45cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|product_id|avg_price|\n",
      "+----------+---------+\n",
      "|         1|    6.957|\n",
      "|         2|   16.957|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avgprice_df.join(sold_df,col('product_id') == col('sol_product_id'),'inner')\\\n",
    "    .filter(col('purchase_date').between(col('start_date'),col('end_date')))\\\n",
    "    .withColumn('mul', col('price') * col('units'))\\\n",
    "    .groupby('product_id')\\\n",
    "    .agg( round(sum(col('mul'))/sum(col('units')),3).alias('avg_price') )\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "ab3aa471",
   "metadata": {},
   "outputs": [],
   "source": [
    "sold_df.createOrReplaceTempView('solddf')\n",
    "avgprice_df.createOrReplaceTempView('avgpricedf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "b9a0bea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|product_id|avg_price|\n",
      "+----------+---------+\n",
      "|         1|    6.957|\n",
      "|         2|   16.957|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" select avg.product_id, \\\n",
    "              round(sum( (avg.price * sol.units) ) / sum(sol.units),3) as avg_price \\\n",
    "              from avgpricedf avg join solddf sol on avg.product_id = sol.sol_product_id \\\n",
    "              where sol.purchase_date between avg.start_date and avg.end_date \\\n",
    "              group by   avg.product_id \\\n",
    "                  \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1144731",
   "metadata": {},
   "source": [
    "#### 015 Project Employees\n",
    "\n",
    "Table: Project\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| project_id  | int     |\n",
    "| employee_id | int     |\n",
    "+-------------+---------+\n",
    "(project_id, employee_id) is the primary key of this table.\n",
    "employee_id is a foreign key to Employee table.\n",
    "Each row of this table indicates that the employee with employee_id is working on the project with project_id.\n",
    " \n",
    "\n",
    "Table: Employee\n",
    "\n",
    "+------------------+---------+\n",
    "| Column Name      | Type    |\n",
    "+------------------+---------+\n",
    "| employee_id      | int     |\n",
    "| name             | varchar |\n",
    "| experience_years | int     |\n",
    "+------------------+---------+\n",
    "employee_id is the primary key of this table. It's guaranteed that experience_years is not NULL.\n",
    "Each row of this table contains information about one employee.\n",
    " \n",
    "\n",
    "Write an SQL query that reports the average experience years of all the employees for each project, rounded to 2 digits.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The query result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Project table:\n",
    "+-------------+-------------+\n",
    "| project_id  | employee_id |\n",
    "+-------------+-------------+\n",
    "| 1           | 1           |\n",
    "| 1           | 2           |\n",
    "| 1           | 3           |\n",
    "| 2           | 1           |\n",
    "| 2           | 4           |\n",
    "+-------------+-------------+\n",
    "Employee table:\n",
    "+-------------+--------+------------------+\n",
    "| employee_id | name   | experience_years |\n",
    "+-------------+--------+------------------+\n",
    "| 1           | Khaled | 3                |\n",
    "| 2           | Ali    | 2                |\n",
    "| 3           | John   | 1                |\n",
    "| 4           | Doe    | 2                |\n",
    "+-------------+--------+------------------+\n",
    "Output: \n",
    "+-------------+---------------+\n",
    "| project_id  | average_years |\n",
    "+-------------+---------------+\n",
    "| 1           | 2.00          |\n",
    "| 2           | 2.50          |\n",
    "+-------------+---------------+\n",
    "Explanation: The average experience years for the first project is (3 + 2 + 1) / 3 = 2.00 and for the second project is (3 + 2) / 2 = 2.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d3b94b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/015_ProjectEmployees.csv')\\\n",
    "        .withColumnRenamed('employee_id','pro_employee_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "5fee6f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- project_id: integer (nullable = true)\n",
      " |-- pro_employee_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "project_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5567f32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/015a_ProjectEmployees.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "d75f7906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- experience_years: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "b6ea0d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|project_id|avg_years|\n",
      "+----------+---------+\n",
      "|         1|      2.0|\n",
      "|         2|      2.5|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "project_df.join(emp_df,col('pro_employee_id') == col('employee_id'), 'inner' ).groupby('project_id') \\\n",
    "        .agg(avg(col('experience_years')).alias('avg_years')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "b4e09eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_df.createOrReplaceTempView('projectdf')\n",
    "emp_df.createOrReplaceTempView('empdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "989ec7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|project_id|avg_years|\n",
      "+----------+---------+\n",
      "|         1|      2.0|\n",
      "|         2|      2.5|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select project_id,avg(experience_years) as avg_years \\\n",
    "              from projectdf join empdf on pro_employee_id = employee_id \\\n",
    "              group by project_id \").show()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e535cf8f",
   "metadata": {},
   "source": [
    "#### 016 Percentage of Users Attended a Contest   \n",
    "\n",
    "Table: Users\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| user_id     | int     |\n",
    "| user_name   | varchar |\n",
    "+-------------+---------+\n",
    "user_id is the primary key (column with unique values) for this table.\n",
    "Each row of this table contains the name and the id of a user.\n",
    " \n",
    "\n",
    "Table: Register\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| contest_id  | int     |\n",
    "| user_id     | int     |\n",
    "+-------------+---------+\n",
    "(contest_id, user_id) is the primary key (combination of columns with unique values) for this table.\n",
    "Each row of this table contains the id of a user and the contest they registered into.\n",
    " \n",
    "\n",
    "Write a solution to find the percentage of the users registered in each contest rounded to two decimals.\n",
    "\n",
    "Return the result table ordered by percentage in descending order. In case of a tie, order it by contest_id in ascending order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Users table:\n",
    "+---------+-----------+\n",
    "| user_id | user_name |\n",
    "+---------+-----------+\n",
    "| 6       | Alice     |\n",
    "| 2       | Bob       |\n",
    "| 7       | Alex      |\n",
    "+---------+-----------+\n",
    "Register table:\n",
    "+------------+---------+\n",
    "| contest_id | user_id |\n",
    "+------------+---------+\n",
    "| 215        | 6       |\n",
    "| 209        | 2       |\n",
    "| 208        | 2       |\n",
    "| 210        | 6       |\n",
    "| 208        | 6       |\n",
    "| 209        | 7       |\n",
    "| 209        | 6       |\n",
    "| 215        | 7       |\n",
    "| 208        | 7       |\n",
    "| 210        | 2       |\n",
    "| 207        | 2       |\n",
    "| 210        | 7       |\n",
    "+------------+---------+\n",
    "Output: \n",
    "+------------+------------+\n",
    "| contest_id | percentage |\n",
    "+------------+------------+\n",
    "| 208        | 100.0      |\n",
    "| 209        | 100.0      |\n",
    "| 210        | 100.0      |\n",
    "| 215        | 66.67      |\n",
    "| 207        | 33.33      |\n",
    "+------------+------------+\n",
    "Explanation: \n",
    "All the users registered in contests 208, 209, and 210. The percentage is 100% and we sort them in the answer table by contest_id in ascending order.\n",
    "Alice and Alex registered in contest 215 and the percentage is ((2/3) * 100) = 66.67%\n",
    "Bob registered in contest 207 and the percentage is ((1/3) * 100) = 33.33%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a0eba1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/016_PercentageofUsersAttendedaContest.csv')\\\n",
    "        .withColumnRenamed('user_id','use_user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "969d6f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "contest_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/016a_PercentageofUsersAttendedaContest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "7b1e4325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- use_user_id: integer (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "8c7ba977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contest_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contest_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "8da150bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|contest_id|percentage|\n",
      "+----------+----------+\n",
      "|       208|     100.0|\n",
      "|       209|     100.0|\n",
      "|       210|     100.0|\n",
      "|       215|     66.67|\n",
      "|       207|     33.33|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = user_df.join(contest_df,col('use_user_id') == col('user_id'),'inner' ) \\\n",
    "        .groupby('contest_id') \\\n",
    "        .agg( round(((count('use_user_id')/3) * 100 ),2).alias('percentage') ) \\\n",
    "        .orderBy(desc('percentage'),asc('contest_id')) \\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "990ec4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.createOrReplaceTempView('userdf')\n",
    "contest_df.createOrReplaceTempView('contestdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "c9f348f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|contest_id|percentage|\n",
      "+----------+----------+\n",
      "|       208|     100.0|\n",
      "|       209|     100.0|\n",
      "|       210|     100.0|\n",
      "|       215|     66.67|\n",
      "|       207|     33.33|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" select contest_id,percentage from \\\n",
    "          (select contest_id, round(((count('use_user_id')/3) * 100 ),2) as percentage \\\n",
    "          from userdf join  contestdf on use_user_id = user_id \\\n",
    "          group by contest_id) a  \\\n",
    "          order by percentage desc, contest_id asc \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b813d100",
   "metadata": {},
   "source": [
    "#### 017 Queries Quality and Percentage \n",
    "\n",
    "Table: Queries\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| query_name  | varchar |\n",
    "| result      | varchar |\n",
    "| position    | int     |\n",
    "| rating      | int     |\n",
    "+-------------+---------+\n",
    "This table may have duplicate rows.\n",
    "This table contains information collected from some queries on a database.\n",
    "The position column has a value from 1 to 500.\n",
    "The rating column has a value from 1 to 5. Query with rating less than 3 is a poor query.\n",
    " \n",
    "\n",
    "We define query quality as:\n",
    "\n",
    "The average of the ratio between query rating and its position.\n",
    "\n",
    "We also define poor query percentage as:\n",
    "\n",
    "The percentage of all queries with rating less than 3.\n",
    "\n",
    "Write a solution to find each query_name, the quality and poor_query_percentage.\n",
    "\n",
    "Both quality and poor_query_percentage should be rounded to 2 decimal places.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Queries table:\n",
    "+------------+-------------------+----------+--------+\n",
    "| query_name | result            | position | rating |\n",
    "+------------+-------------------+----------+--------+\n",
    "| Dog        | Golden Retriever  | 1        | 5      |\n",
    "| Dog        | German Shepherd   | 2        | 5      |\n",
    "| Dog        | Mule              | 200      | 1      |\n",
    "| Cat        | Shirazi           | 5        | 2      |\n",
    "| Cat        | Siamese           | 3        | 3      |\n",
    "| Cat        | Sphynx            | 7        | 4      |\n",
    "+------------+-------------------+----------+--------+\n",
    "Output: \n",
    "+------------+---------+-----------------------+\n",
    "| query_name | quality | poor_query_percentage |\n",
    "+------------+---------+-----------------------+\n",
    "| Dog        | 2.50    | 33.33                 |\n",
    "| Cat        | 0.66    | 33.33                 |\n",
    "+------------+---------+-----------------------+\n",
    "Explanation: \n",
    "Dog queries quality is ((5 / 1) + (5 / 2) + (1 / 200)) / 3 = 2.50\n",
    "Dog queries poor_ query_percentage is (1 / 3) * 100 = 33.33\n",
    "\n",
    "Cat queries quality equals ((2 / 5) + (3 / 3) + (4 / 7)) / 3 = 0.66\n",
    "Cat queries poor_ query_percentage is (1 / 3) * 100 = 33.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "58a9ea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/017_QueriesQualityandPercentage.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "c5857d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- query_name: string (nullable = true)\n",
      " |-- result: string (nullable = true)\n",
      " |-- position: integer (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "id": "8b5b7211",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_df = query_df.groupby('query_name')\\\n",
    "        .agg(round(avg(col('rating')/col('position') ),3).alias('quality')) \\\n",
    "        .withColumn('q_percentage',lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ec33be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/017_QueriesQualityandPercentage.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "28d7da44",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = quality_df.groupby('query_name').agg(count(col('query_name')).alias('cnt')).withColumn('q_percentage',lit(0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "a73ad781",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = quality_df.filter(col('rating') < 3).groupby('query_name').agg(count(col('query_name')).alias('cnt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "101152bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = y.withColumn('q_percentage',lit(0)).select('query_name','q_percentage','cnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "id": "f3accdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_union_df  = x.union(yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "id": "2c8191a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_df = xy_union_df.groupby('query_name')\\\n",
    "    .agg(sum(col('cnt')).alias('cnt'), sum(col('q_percentage')).alias('q_percentage'))\\\n",
    "    .withColumn('q_percentage',(col('q_percentage')/col('cnt'))*100)\\\n",
    "    .select('query_name',lit(0).alias('quality'),'q_percentage')\\\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "id": "149d8d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------------+\n",
      "|query_name|quality|q_percentage|\n",
      "+----------+-------+------------+\n",
      "|       Cat|  0.657|       33.33|\n",
      "|       Dog|  2.502|       33.33|\n",
      "+----------+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_df.union(percentage_df).groupby('query_name')\\\n",
    "    .agg(sum(col('quality')).alias('quality'),round(sum(col('q_percentage')),2).alias('q_percentage')) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "bf3d5819",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df.createOrReplaceTempView('querydf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "886aa189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------------+\n",
      "|query_name|query|q_percentage|\n",
      "+----------+-----+------------+\n",
      "|       Cat|0.657|       33.33|\n",
      "|       Dog|2.502|       33.33|\n",
      "+----------+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" select query_name,sum(query) as query, round(sum(q_percentage),2) as q_percentage from \\\n",
    "             (select query_name, round(avg(rating/position),3) as query,0 as q_percentage \\\n",
    "              from querydf group by query_name \\\n",
    "            UNION \\\n",
    "            ( \\\n",
    "              select query_name,0 as quality,(sum(q_pct)/sum(cnt))*100 as q_percentage from (\\\n",
    "            ( select query_name,count(query_name) as cnt,0 as q_pct from querydf \\\n",
    "              group by query_name )\\\n",
    "            union \\\n",
    "            ( select query_name, 0 as cnt,count(rating) as rat from querydf \\\n",
    "              where rating < 3  group by query_name )) \\\n",
    "              group by query_name \\\n",
    "            ) \\\n",
    "            ) group by query_name \\\n",
    "            \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346c28b4",
   "metadata": {},
   "source": [
    "#### 018 Number of Unique Subjects Taught by Each Teacher\n",
    "\n",
    "Table: Teacher\n",
    "\n",
    "+-------------+------+\n",
    "| Column Name | Type |\n",
    "+-------------+------+\n",
    "| teacher_id  | int  |\n",
    "| subject_id  | int  |\n",
    "| dept_id     | int  |\n",
    "+-------------+------+\n",
    "(subject_id, dept_id) is the primary key (combinations of columns with unique values) of this table.\n",
    "Each row in this table indicates that the teacher with teacher_id teaches the subject subject_id in the department dept_id.\n",
    " \n",
    "\n",
    "Write a solution to calculate the number of unique subjects each teacher teaches in the university.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is shown in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Teacher table:\n",
    "+------------+------------+---------+\n",
    "| teacher_id | subject_id | dept_id |\n",
    "+------------+------------+---------+\n",
    "| 1          | 2          | 3       |\n",
    "| 1          | 2          | 4       |\n",
    "| 1          | 3          | 3       |\n",
    "| 2          | 1          | 1       |\n",
    "| 2          | 2          | 1       |\n",
    "| 2          | 3          | 1       |\n",
    "| 2          | 4          | 1       |\n",
    "+------------+------------+---------+\n",
    "Output:  \n",
    "+------------+-----+\n",
    "| teacher_id | cnt |\n",
    "+------------+-----+\n",
    "| 1          | 2   |\n",
    "| 2          | 4   |\n",
    "+------------+-----+\n",
    "Explanation: \n",
    "Teacher 1:\n",
    "  - They teach subject 2 in departments 3 and 4.\n",
    "  - They teach subject 3 in department 3.\n",
    "Teacher 2:\n",
    "  - They teach subject 1 in department 1.\n",
    "  - They teach subject 2 in department 1.\n",
    "  - They teach subject 3 in department 1.\n",
    "  - They teach subject 4 in department 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ecaebc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/018_NumberofUniqueSubjectsTaughtbyEachTeacher.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "id": "601d5488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- teacher_id: integer (nullable = true)\n",
      " |-- subject_id: integer (nullable = true)\n",
      " |-- dept_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subject_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "id": "8d8b09dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|teacher_id|count|\n",
      "+----------+-----+\n",
      "|         1|    2|\n",
      "|         2|    4|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subject_df.select('teacher_id','subject_id').distinct().groupby('teacher_id').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "id": "35311501",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_df.createOrReplaceTempView('subjectdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "id": "8bc1fe01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|teacher_id|count|\n",
      "+----------+-----+\n",
      "|         1|    2|\n",
      "|         2|    4|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select teacher_id, count(distinct subject_id) as count from subjectdf\\\n",
    "            group by teacher_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e9d45e",
   "metadata": {},
   "source": [
    "#### 019 User Activity for the Past 30 Days  \n",
    "\n",
    "Table: Activity\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| user_id       | int     |\n",
    "| session_id    | int     |\n",
    "| activity_date | date    |\n",
    "| activity_type | enum    |\n",
    "+---------------+---------+\n",
    "This table may have duplicate rows.\n",
    "The activity_type column is an ENUM (category) of type ('open_session', 'end_session', 'scroll_down', 'send_message').\n",
    "The table shows the user activities for a social media website. \n",
    "Note that each session belongs to exactly one user.\n",
    " \n",
    "\n",
    "Write a solution to find the daily active user count for a period of 30 days ending 2019-07-27 inclusively. A user was active on someday if they made at least one activity on that day.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Activity table:\n",
    "+---------+------------+---------------+---------------+\n",
    "| user_id | session_id | activity_date | activity_type |\n",
    "+---------+------------+---------------+---------------+\n",
    "| 1       | 1          | 2019-07-20    | open_session  |\n",
    "| 1       | 1          | 2019-07-20    | scroll_down   |\n",
    "| 1       | 1          | 2019-07-20    | end_session   |\n",
    "| 2       | 4          | 2019-07-20    | open_session  |\n",
    "| 2       | 4          | 2019-07-21    | send_message  |\n",
    "| 2       | 4          | 2019-07-21    | end_session   |\n",
    "| 3       | 2          | 2019-07-21    | open_session  |\n",
    "| 3       | 2          | 2019-07-21    | send_message  |\n",
    "| 3       | 2          | 2019-07-21    | end_session   |\n",
    "| 4       | 3          | 2019-06-25    | open_session  |\n",
    "| 4       | 3          | 2019-06-25    | end_session   |\n",
    "+---------+------------+---------------+---------------+\n",
    "Output: \n",
    "+------------+--------------+ \n",
    "| day        | active_users |\n",
    "+------------+--------------+ \n",
    "| 2019-07-20 | 2            |\n",
    "| 2019-07-21 | 2            |\n",
    "+------------+--------------+ \n",
    "Explanation: Note that we do not care about days with zero active users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "88f910f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "useract_Schama = sold_schema =  StructType([\n",
    "                                (StructField('user_id',IntegerType(),False)),\n",
    "                                (StructField('session_id',IntegerType(),False)),\n",
    "                                (StructField('activity_date',DateType(),False)),\n",
    "                                (StructField('activity_type',StringType(),True))\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "32fce1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a solution to find the daily active user count for a period of 30 days ending 2019-07-27 inclusively.\n",
    "useract_df = spark.read.option('header',True).schema(useract_Schama).format('csv')\\\n",
    "        .load('../data/easymedium/019_UserActivityforthePast30Days.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c3763627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- session_id: integer (nullable = true)\n",
      " |-- activity_date: date (nullable = true)\n",
      " |-- activity_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "useract_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4407636c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n",
      "|activity_date|active_users|\n",
      "+-------------+------------+\n",
      "|   2019-07-21|           2|\n",
      "|   2019-07-20|           2|\n",
      "+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "useract_df.filter(datediff(to_date(lit('2019-07-27') ),col('activity_date')) < 30)\\\n",
    "        .groupby('activity_date').agg(countDistinct('user_id').alias('active_users')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99c3ed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "useract_df.createOrReplaceTempView('useractdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a66919ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 44:====================================================> (195 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------------+\n",
      "|activity_date|count(DISTINCT user_id)|\n",
      "+-------------+-----------------------+\n",
      "|   2019-07-21|                      2|\n",
      "|   2019-07-20|                      2|\n",
      "+-------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select activity_date, count(distinct user_id) from useractdf \\\n",
    "                where DATEDIFF(to_date('2019-07-27' ),activity_date) < 30 \\\n",
    "                group by activity_date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90543954",
   "metadata": {},
   "source": [
    "#### 020 Classes More Than 5 Students\n",
    "\n",
    "Table: Courses\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| student     | varchar |\n",
    "| class       | varchar |\n",
    "+-------------+---------+\n",
    "(student, class) is the primary key (combination of columns with unique values) for this table.\n",
    "Each row of this table indicates the name of a student and the class in which they are enrolled.\n",
    " \n",
    "\n",
    "Write a solution to find all the classes that have at least five students.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Courses table:\n",
    "+---------+----------+\n",
    "| student | class    |\n",
    "+---------+----------+\n",
    "| A       | Math     |\n",
    "| B       | English  |\n",
    "| C       | Math     |\n",
    "| D       | Biology  |\n",
    "| E       | Math     |\n",
    "| F       | Computer |\n",
    "| G       | Math     |\n",
    "| H       | Math     |\n",
    "| I       | Math     |\n",
    "+---------+----------+\n",
    "Output: \n",
    "+---------+\n",
    "| class   |\n",
    "+---------+\n",
    "| Math    |\n",
    "+---------+\n",
    "Explanation: \n",
    "- Math has 6 students, so we include it.\n",
    "- English has 1 student, so we do not include it.\n",
    "- Biology has 1 student, so we do not include it.\n",
    "- Computer has 1 student, so we do not include it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f43c298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "studentt_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/020_ClassesMoreThan5Students.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b5bb1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- student: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "studentt_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a48724c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|class|\n",
      "+-----+\n",
      "| Math|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "studentt_df.groupby('class').agg(count(col('class')).alias('cnt')).filter(col('cnt')>=5).select(col('class')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f1d5c224",
   "metadata": {},
   "outputs": [],
   "source": [
    "studentt_df.createOrReplaceTempView('studenttdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "29641a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|class|\n",
      "+-----+\n",
      "| Math|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select class from studenttdf group by class having count(class) >=5 \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55782b28",
   "metadata": {},
   "source": [
    "#### 021 Find Followers Count|\n",
    "\n",
    "Table: Followers\n",
    "\n",
    "+-------------+------+\n",
    "| Column Name | Type |\n",
    "+-------------+------+\n",
    "| user_id     | int  |\n",
    "| follower_id | int  |\n",
    "+-------------+------+\n",
    "(user_id, follower_id) is the primary key (combination of columns with unique values) for this table.\n",
    "This table contains the IDs of a user and a follower in a social media app where the follower follows the user.\n",
    " \n",
    "\n",
    "Write a solution that will, for each user, return the number of followers.\n",
    "\n",
    "Return the result table ordered by user_id in ascending order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Followers table:\n",
    "+---------+-------------+\n",
    "| user_id | follower_id |\n",
    "+---------+-------------+\n",
    "| 0       | 1           |\n",
    "| 1       | 0           |\n",
    "| 2       | 0           |\n",
    "| 2       | 1           |\n",
    "+---------+-------------+\n",
    "Output: \n",
    "+---------+----------------+\n",
    "| user_id | followers_count|\n",
    "+---------+----------------+\n",
    "| 0       | 1              |\n",
    "| 1       | 1              |\n",
    "| 2       | 2              |\n",
    "+---------+----------------+\n",
    "Explanation: \n",
    "The followers of 0 are {1}\n",
    "The followers of 1 are {0}\n",
    "The followers of 2 are {0,1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "be6ad356",
   "metadata": {},
   "outputs": [],
   "source": [
    "followers_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/021_Find_Followers_Count.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "89ad27d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- follower_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "followers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "60219668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|user_id|follower_id|\n",
      "+-------+-----------+\n",
      "|      0|          1|\n",
      "|      1|          1|\n",
      "|      2|          2|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "followers_df.groupby('user_id').agg(count('follower_id').alias('follower_id')).orderBy('user_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bd0d0fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "followers_df.createOrReplaceTempView('followersdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0c9e75ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|user_id|follower_id|\n",
      "+-------+-----------+\n",
      "|      0|          1|\n",
      "|      1|          1|\n",
      "|      2|          2|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select user_id,count(follower_id) as follower_id from  followersdf group by user_id order by user_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd09388",
   "metadata": {},
   "source": [
    "#### 022 Biggest Single Number\n",
    "\n",
    "Table: MyNumbers\n",
    "\n",
    "+-------------+------+\n",
    "| Column Name | Type |\n",
    "+-------------+------+\n",
    "| num         | int  |\n",
    "+-------------+------+\n",
    "This table may contain duplicates (In other words, there is no primary key for this table in SQL).\n",
    "Each row of this table contains an integer.\n",
    " \n",
    "\n",
    "A single number is a number that appeared only once in the MyNumbers table.\n",
    "\n",
    "Find the largest single number. If there is no single number, report null.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "MyNumbers table:\n",
    "+-----+\n",
    "| num |\n",
    "+-----+\n",
    "| 8   |\n",
    "| 8   |\n",
    "| 3   |\n",
    "| 3   |\n",
    "| 1   |\n",
    "| 4   |\n",
    "| 5   |\n",
    "| 6   |\n",
    "+-----+\n",
    "Output: \n",
    "+-----+\n",
    "| num |\n",
    "+-----+\n",
    "| 6   |\n",
    "+-----+\n",
    "Explanation: The single numbers are 1, 4, 5, and 6.\n",
    "Since 6 is the largest single number, we return it.\n",
    "Example 2:\n",
    "\n",
    "Input: \n",
    "MyNumbers table:\n",
    "+-----+\n",
    "| num |\n",
    "+-----+\n",
    "| 8   |\n",
    "| 8   |\n",
    "| 7   |\n",
    "| 7   |\n",
    "| 3   |\n",
    "| 3   |\n",
    "| 3   |\n",
    "+-----+\n",
    "Output: \n",
    "+------+\n",
    "| num  |\n",
    "+------+\n",
    "| null |\n",
    "+------+\n",
    "Explanation: There are no single numbers in the input table so we return null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ad59d9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/022_BiggestSingleNumber.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7394715b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- num: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "single_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "35fd3330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|max(num)|\n",
      "+--------+\n",
      "|       6|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "single_df.groupby('num').agg(count('num').alias('cnt'))\\\n",
    "        .filter(col('cnt') == 1).select(max('num')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4b64fe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_df.createOrReplaceTempView('singledf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "486a52d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|max(m_num)|\n",
      "+----------+\n",
      "|         6|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" select max(m_num) from (select max(num) as m_num from singledf group by num having count(num) = 1 )\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef83a47",
   "metadata": {},
   "source": [
    "#### 023 The Number of Employees Which Report to Each Employee  \n",
    "\n",
    "Table: Employees\n",
    "\n",
    "+-------------+----------+\n",
    "| Column Name | Type     |\n",
    "+-------------+----------+\n",
    "| employee_id | int      |\n",
    "| name        | varchar  |\n",
    "| reports_to  | int      |\n",
    "| age         | int      |\n",
    "+-------------+----------+\n",
    "employee_id is the column with unique values for this table.\n",
    "This table contains information about the employees and the id of the manager they report to. Some employees do not report to anyone (reports_to is null). \n",
    " \n",
    "\n",
    "For this problem, we will consider a manager an employee who has at least 1 other employee reporting to them.\n",
    "\n",
    "Write a solution to report the ids and the names of all managers, the number of employees who report directly to them, and the average age of the reports rounded to the nearest integer.\n",
    "\n",
    "Return the result table ordered by employee_id.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Employees table:\n",
    "+-------------+---------+------------+-----+\n",
    "| employee_id | name    | reports_to | age |\n",
    "+-------------+---------+------------+-----+\n",
    "| 9           | Hercy   | null       | 43  |\n",
    "| 6           | Alice   | 9          | 41  |\n",
    "| 4           | Bob     | 9          | 36  |\n",
    "| 2           | Winston | null       | 37  |\n",
    "+-------------+---------+------------+-----+\n",
    "Output: \n",
    "+-------------+-------+---------------+-------------+\n",
    "| employee_id | name  | reports_count | average_age |\n",
    "+-------------+-------+---------------+-------------+\n",
    "| 9           | Hercy | 2             | 39          |\n",
    "+-------------+-------+---------------+-------------+\n",
    "Explanation: Hercy has 2 people report directly to him, Alice and Bob. Their average age is (41+36)/2 = 38.5, which is 39 after rounding it to the nearest integer.\n",
    "Example 2:\n",
    "\n",
    "Input: \n",
    "Employees table:\n",
    "+-------------+---------+------------+-----+ \n",
    "| employee_id | name    | reports_to | age |\n",
    "|-------------|---------|------------|-----|\n",
    "| 1           | Michael | null       | 45  |\n",
    "| 2           | Alice   | 1          | 38  |\n",
    "| 3           | Bob     | 1          | 42  |\n",
    "| 4           | Charlie | 2          | 34  |\n",
    "| 5           | David   | 2          | 40  |\n",
    "| 6           | Eve     | 3          | 37  |\n",
    "| 7           | Frank   | null       | 50  |\n",
    "| 8           | Grace   | null       | 48  |\n",
    "+-------------+---------+------------+-----+ \n",
    "Output: \n",
    "+-------------+---------+---------------+-------------+\n",
    "| employee_id | name    | reports_count | average_age |\n",
    "| ----------- | ------- | ------------- | ----------- |\n",
    "| 1           | Michael | 2             | 40          |\n",
    "| 2           | Alice   | 2             | 37          |\n",
    "| 3           | Bob     | 1             | 37          |\n",
    "+-------------+---------+---------------+-------------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3d23d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "mng_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/023a_The NumberofEmployeesWhichReportoEach Employee.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c303b48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- reports_to: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mng_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f7e67dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_df = mng_df.select('employee_id','name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "24adef73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------------+-------+\n",
      "|employee_id|   name|report_count|avg_age|\n",
      "+-----------+-------+------------+-------+\n",
      "|          1|Michael|           2|   40.0|\n",
      "|          2|  Alice|           2|   37.0|\n",
      "|          3|    Bob|           1|   37.0|\n",
      "+-----------+-------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mng_df.filter(col('reports_to').isNotNull()).groupby('reports_to')\\\n",
    "    .agg(count('reports_to').alias('report_count'),avg('age').alias('avg_age'))\\\n",
    "    .join(name_df,col('employee_id') == col('reports_to'),'inner')\\\n",
    "    .select('employee_id','name','report_count',round('avg_age',0).alias('avg_age')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6b86c793",
   "metadata": {},
   "outputs": [],
   "source": [
    "mng_df.createOrReplaceTempView('mngdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "8b5d36b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------------+-------+\n",
      "|employee_id|   name|report_count|avg_age|\n",
      "+-----------+-------+------------+-------+\n",
      "|          1|Michael|           2|   40.0|\n",
      "|          2|  Alice|           2|   37.0|\n",
      "|          3|    Bob|           1|   37.0|\n",
      "+-----------+-------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" select employee_id,name,report_count,round(avg_age,0) as avg_age from mngdf \\\n",
    "            join ( \\\n",
    "            select reports_to,count(reports_to) as report_count,avg(age) as avg_age \\\n",
    "            from mngdf where reports_to is not null group by reports_to ) a on employee_id = a.reports_to \\\n",
    "          \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded6e1cf",
   "metadata": {},
   "source": [
    "#### 024 Primary Department for Each Employee\n",
    "\n",
    "Table: Employee\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   |  Type   |\n",
    "+---------------+---------+\n",
    "| employee_id   | int     |\n",
    "| department_id | int     |\n",
    "| primary_flag  | varchar |\n",
    "+---------------+---------+\n",
    "(employee_id, department_id) is the primary key (combination of columns with unique values) for this table.\n",
    "employee_id is the id of the employee.\n",
    "department_id is the id of the department to which the employee belongs.\n",
    "primary_flag is an ENUM (category) of type ('Y', 'N'). If the flag is 'Y', the department is the primary department for the employee. If the flag is 'N', the department is not the primary.\n",
    " \n",
    "\n",
    "Employees can belong to multiple departments. When the employee joins other departments, they need to decide which department is their primary department. Note that when an employee belongs to only one department, their primary column is 'N'.\n",
    "\n",
    "Write a solution to report all the employees with their primary department. For employees who belong to one department, report their only department.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Employee table:\n",
    "+-------------+---------------+--------------+\n",
    "| employee_id | department_id | primary_flag |\n",
    "+-------------+---------------+--------------+\n",
    "| 1           | 1             | N            |\n",
    "| 2           | 1             | Y            |\n",
    "| 2           | 2             | N            |\n",
    "| 3           | 3             | N            |\n",
    "| 4           | 2             | N            |\n",
    "| 4           | 3             | Y            |\n",
    "| 4           | 4             | N            |\n",
    "+-------------+---------------+--------------+\n",
    "Output: \n",
    "+-------------+---------------+\n",
    "| employee_id | department_id |\n",
    "+-------------+---------------+\n",
    "| 1           | 1             |\n",
    "| 2           | 1             |\n",
    "| 3           | 3             |\n",
    "| 4           | 3             |\n",
    "+-------------+---------------+\n",
    "Explanation: \n",
    "- The Primary department for employee 1 is 1.\n",
    "- The Primary department for employee 2 is 1.\n",
    "- The Primary department for employee 3 is 3.\n",
    "- The Primary department for employee 4 is 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5f2c11c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "primemp_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/024_PrimaryDepartmentforEachEmployee.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2f0710d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- department_id: integer (nullable = true)\n",
      " |-- primary_flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "primemp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d9f9364a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|employee_id|department_id|\n",
      "+-----------+-------------+\n",
      "|          1|            1|\n",
      "|          2|            1|\n",
      "|          3|            1|\n",
      "|          4|            3|\n",
      "+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "primemp_df.filter( (col('primary_flag') == 'Y')).select('employee_id','department_id')\\\n",
    "    .union( primemp_df.groupby('employee_id').agg(count(col('department_id')).alias('cnt_dep')) \\\n",
    "    .filter(col('cnt_dep') == 1)).orderBy('employee_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "6e245afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "primemp_df.createOrReplaceTempView('primemp_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "7bccdb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|employee_id|department_id|\n",
      "+-----------+-------------+\n",
      "|          1|            1|\n",
      "|          2|            1|\n",
      "|          3|            1|\n",
      "|          4|            3|\n",
      "+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select employee_id,department_id from primemp_df where primary_flag = 'Y' \\\n",
    "          UNION ( select employee_id, count(department_id) as department_id from primemp_df \\\n",
    "          group by employee_id having department_id = 1 ) ORDER BY employee_id \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b429e2de",
   "metadata": {},
   "source": [
    "#### 025 Triangle Judgement\n",
    "\n",
    "Table: Triangle\n",
    "\n",
    "+-------------+------+\n",
    "| Column Name | Type |\n",
    "+-------------+------+\n",
    "| x           | int  |\n",
    "| y           | int  |\n",
    "| z           | int  |\n",
    "+-------------+------+\n",
    "In SQL, (x, y, z) is the primary key column for this table.\n",
    "Each row of this table contains the lengths of three line segments.\n",
    " \n",
    "\n",
    "Report for every three line segments whether they can form a triangle.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Triangle table:\n",
    "+----+----+----+\n",
    "| x  | y  | z  |\n",
    "+----+----+----+\n",
    "| 13 | 15 | 30 |\n",
    "| 10 | 20 | 15 |\n",
    "+----+----+----+\n",
    "Output: \n",
    "+----+----+----+----------+\n",
    "| x  | y  | z  | triangle |\n",
    "+----+----+----+----------+\n",
    "| 13 | 15 | 30 | No       |\n",
    "| 10 | 20 | 15 | Yes      |\n",
    "+----+----+----+----------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "05711f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "triangle_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/025_TriangleJudgement.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "c2ac19c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: integer (nullable = true)\n",
      " |-- y: integer (nullable = true)\n",
      " |-- z: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "triangle_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "37e9062b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+\n",
      "|  x|  y|  z|triangle|\n",
      "+---+---+---+--------+\n",
      "| 13| 15| 30|      NO|\n",
      "| 10| 20| 15|     YES|\n",
      "+---+---+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "triangle_df.select('x','y','z', when( ((col('x')+ col('y' ) ) > col('z') ) \\\n",
    "                                   &  ((col('x')+ col('y' ) ) > col('z') ) \\\n",
    "                                   &  ((col('x')+ col('y' ) ) > col('z') ),'YES').otherwise('NO').alias('triangle')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "8f439826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+\n",
      "|  x|  y|  z|triangle|\n",
      "+---+---+---+--------+\n",
      "| 13| 15| 30|     YES|\n",
      "| 10| 20| 15|     YES|\n",
      "+---+---+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### WRONG\n",
    "triangle_df.select('x','y','z', when( (col('x')+ col('y' ) ) > col('z'),'YES' ) \\\n",
    "                               .when( (col('z')+ col('y' ) ) > col('x'),'YES' ) \\\n",
    "                               .when( (col('x')+ col('z' ) ) > col('y'),'YES' ).otherwise('NO')\\\n",
    "                                .alias('triangle')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "6bad8556",
   "metadata": {},
   "outputs": [],
   "source": [
    "triangle_df.createOrReplaceTempView('triangle_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "28077a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+\n",
      "|  x|  y|  z|triangle|\n",
      "+---+---+---+--------+\n",
      "| 13| 15| 30|      NO|\n",
      "| 10| 20| 15|     YES|\n",
      "+---+---+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT x, y, z, \\\n",
    "           CASE WHEN (x + y) > z AND (x + z) > y AND (y + z) > x THEN 'YES' ELSE 'NO' END AS triangle \\\n",
    "    FROM triangle_df\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022675c4",
   "metadata": {},
   "source": [
    "#### 026  Employees Whose Manager Left the Company   \n",
    "\n",
    "Table: Employees\n",
    "\n",
    "+-------------+----------+\n",
    "| Column Name | Type     |\n",
    "+-------------+----------+\n",
    "| employee_id | int      |\n",
    "| name        | varchar  |\n",
    "| manager_id  | int      |\n",
    "| salary      | int      |\n",
    "+-------------+----------+\n",
    "In SQL, employee_id is the primary key for this table.\n",
    "This table contains information about the employees, their salary, and the ID of their manager. Some employees do not have a manager (manager_id is null). \n",
    " \n",
    "\n",
    "Find the IDs of the employees whose salary is strictly less than $30000 and whose manager left the company. When a manager leaves the company, their information is deleted from the Employees table, but the reports still have their manager_id set to the manager that left.\n",
    "\n",
    "Return the result table ordered by employee_id.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input:  \n",
    "Employees table:\n",
    "+-------------+-----------+------------+--------+\n",
    "| employee_id | name      | manager_id | salary |\n",
    "+-------------+-----------+------------+--------+\n",
    "| 3           | Mila      | 9          | 60301  |\n",
    "| 12          | Antonella | null       | 31000  |\n",
    "| 13          | Emery     | null       | 67084  |\n",
    "| 1           | Kalel     | 11         | 21241  |\n",
    "| 9           | Mikaela   | null       | 50937  |\n",
    "| 11          | Joziah    | 6          | 28485  |\n",
    "+-------------+-----------+------------+--------+\n",
    "Output: \n",
    "+-------------+\n",
    "| employee_id |\n",
    "+-------------+\n",
    "| 11          |\n",
    "+-------------+\n",
    "\n",
    "Explanation: \n",
    "The employees with a salary less than $30000 are 1 (Kalel) and 11 (Joziah).\n",
    "Kalel's manager is employee 11, who is still in the company (Joziah).\n",
    "Joziah's manager is employee 6, who left the company because there is no row for employee 6 as it was deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "845db33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lftcomp = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/026_EmployeesWhoseManagerLefttheCompany.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "e3b41413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- manager_id: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lftcomp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "6aec9f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df = lftcomp.withColumnRenamed('employee_id','emp_id').select(col('emp_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "6c340176",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr3000_df = lftcomp.filter( col('salary') < 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "b15dc711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+----------+------+\n",
      "|employee_id|  name|manager_id|salary|\n",
      "+-----------+------+----------+------+\n",
      "|          1| Kalel|        11| 21241|\n",
      "|         11|Joziah|         6| 28485|\n",
      "+-----------+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gr3000_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "7ed4f338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+\n",
      "|employee_id|  name|salary|\n",
      "+-----------+------+------+\n",
      "|         11|Joziah| 28485|\n",
      "+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gr3000_df.join(emp_df,col('manager_id') == col('emp_id'),'left')\\\n",
    "    .filter(col('emp_id').isNull() ).select('employee_id','name','salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "b032c9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lftcomp.createOrReplaceTempView('lftcompdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "e1e0a7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|employee_id|\n",
      "+-----------+\n",
      "|         11|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" select a.employee_id from ( \\\n",
    "          (select employee_id,name,manager_id,salary from lftcompdf where salary < 30000) as a \\\n",
    "          left join (select employee_id from  lftcompdf ) b on a.manager_id = b.employee_id ) \\\n",
    "          where b.employee_id is null\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3459bab3",
   "metadata": {},
   "source": [
    "#### 027 Fix Names in a Table\n",
    "\n",
    "Table: Users\n",
    "\n",
    "+----------------+---------+\n",
    "| Column Name    | Type    |\n",
    "+----------------+---------+\n",
    "| user_id        | int     |\n",
    "| name           | varchar |\n",
    "+----------------+---------+\n",
    "user_id is the primary key (column with unique values) for this table.\n",
    "This table contains the ID and the name of the user. The name consists of only lowercase and uppercase characters.\n",
    " \n",
    "\n",
    "Write a solution to fix the names so that only the first character is uppercase and the rest are lowercase.\n",
    "\n",
    "Return the result table ordered by user_id.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Users table:\n",
    "+---------+-------+\n",
    "| user_id | name  |\n",
    "+---------+-------+\n",
    "| 1       | aLice |\n",
    "| 2       | bOB   |\n",
    "+---------+-------+\n",
    "Output: \n",
    "+---------+-------+\n",
    "| user_id | name  |\n",
    "+---------+-------+\n",
    "| 1       | Alice |\n",
    "| 2       | Bob   |\n",
    "+---------+-------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "95e4e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "caps_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/027_FixNamesinaTable.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "b8e7ea5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "caps_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "e1ad6811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|user_id| name|\n",
      "+-------+-----+\n",
      "|      1|Alice|\n",
      "|      2|  Bob|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "caps_df.select('user_id',initcap('name').alias('name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "77dc22cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "caps_df.createOrReplaceTempView('capsdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "fe468eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|user_id| name|\n",
      "+-------+-----+\n",
      "|      1|Alice|\n",
      "|      2|  Bob|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select user_id,initcap(name) as name from capsdf\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f94c55c",
   "metadata": {},
   "source": [
    "#### 028 Patients With a Condition\n",
    "\n",
    "Table: Patients\n",
    "\n",
    "+--------------+---------+\n",
    "| Column Name  | Type    |\n",
    "+--------------+---------+\n",
    "| patient_id   | int     |\n",
    "| patient_name | varchar |\n",
    "| conditions   | varchar |\n",
    "+--------------+---------+\n",
    "patient_id is the primary key (column with unique values) for this table.\n",
    "'conditions' contains 0 or more code separated by spaces. \n",
    "This table contains information of the patients in the hospital.\n",
    " \n",
    "\n",
    "Write a solution to find the patient_id, patient_name, and conditions of the patients who have Type I Diabetes. Type I Diabetes always starts with DIAB1 prefix.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Patients table:\n",
    "+------------+--------------+--------------+\n",
    "| patient_id | patient_name | conditions   |\n",
    "+------------+--------------+--------------+\n",
    "| 1          | Daniel       | YFEV COUGH   |\n",
    "| 2          | Alice        |              |\n",
    "| 3          | Bob          | DIAB100 MYOP |\n",
    "| 4          | George       | ACNE DIAB100 |\n",
    "| 5          | Alain        | DIAB201      |\n",
    "+------------+--------------+--------------+\n",
    "Output: \n",
    "+------------+--------------+--------------+\n",
    "| patient_id | patient_name | conditions   |\n",
    "+------------+--------------+--------------+\n",
    "| 3          | Bob          | DIAB100 MYOP |\n",
    "| 4          | George       | ACNE DIAB100 | \n",
    "+------------+--------------+--------------+\n",
    "Explanation: Bob and George both have a condition that starts with DIAB1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b6f0162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/028_PatientsWithaCondition.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "0f991408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- patient_id: integer (nullable = true)\n",
      " |-- patient_name: string (nullable = true)\n",
      " |-- conditions: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pat_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "22ec0d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+\n",
      "|patient_id|patient_name|  conditions|\n",
      "+----------+------------+------------+\n",
      "|         3|         Bob|DIAB100 MYOP|\n",
      "|         4|      George|ACNE DIAB100|\n",
      "+----------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pat_df.withColumn('col1',split(col('conditions'),' ')[0]) \\\n",
    ".withColumn('col2',split(col('conditions'),' ')[1]) \\\n",
    ".filter( (col('col1').like('DIAB1%')) | (col('col2').like('DIAB1%')) ) \\\n",
    ".select('patient_id','patient_name','conditions').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "06caa5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_df.createOrReplaceTempView('patdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "0bb8c804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+\n",
      "|patient_id|patient_name|  conditions|\n",
      "+----------+------------+------------+\n",
      "|         3|         Bob|DIAB100 MYOP|\n",
      "|         4|      George|ACNE DIAB100|\n",
      "+----------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select patient_id,patient_name,conditions from \\\n",
    "          (select patient_id,patient_name,conditions,split(conditions,' ')[0] as col1, split(conditions,' ')[1] as col2 from patdf ) \\\n",
    "          where col1 like 'DIAB1%' OR col2 like 'DIAB1%' \").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58ccb23",
   "metadata": {},
   "source": [
    "#### 029  Delete Duplicate Emails\n",
    "\n",
    "Table: Person\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| id          | int     |\n",
    "| email       | varchar |\n",
    "+-------------+---------+\n",
    "id is the primary key (column with unique values) for this table.\n",
    "Each row of this table contains an email. The emails will not contain uppercase letters.\n",
    " \n",
    "\n",
    "Write a solution to delete all duplicate emails, keeping only one unique email with the smallest id.\n",
    "\n",
    "For SQL users, please note that you are supposed to write a DELETE statement and not a SELECT one.\n",
    "\n",
    "For Pandas users, please note that you are supposed to modify Person in place.\n",
    "\n",
    "After running your script, the answer shown is the Person table. The driver will first compile and run your piece of code and then show the Person table. The final order of the Person table does not matter.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Person table:\n",
    "+----+------------------+\n",
    "| id | email            |\n",
    "+----+------------------+\n",
    "| 1  | john@example.com |\n",
    "| 2  | bob@example.com  |\n",
    "| 3  | john@example.com |\n",
    "+----+------------------+\n",
    "Output: \n",
    "+----+------------------+\n",
    "| id | email            |\n",
    "+----+------------------+\n",
    "| 1  | john@example.com |\n",
    "| 2  | bob@example.com  |\n",
    "+----+------------------+\n",
    "Explanation: john@example.com is repeated two times. We keep the row with the smallest Id = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "53e428b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/029_DeleteDuplicateEmails.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "33099ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "email_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "230d3146",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_spec = Window.partitionBy('email').orderBy(asc('id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "ce0a3690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+\n",
      "| id|           email|\n",
      "+---+----------------+\n",
      "|  1|john@example.com|\n",
      "|  2| bob@example.com|\n",
      "+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "email_df.withColumn('rnk',rank().over(win_spec)).filter(col('rnk') ==1).drop('rnk').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "1a579b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_df.createOrReplaceTempView('emaildf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "eead8d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+\n",
      "| id|           email|\n",
      "+---+----------------+\n",
      "|  1|john@example.com|\n",
      "|  2| bob@example.com|\n",
      "+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select id,email from \\\n",
    "          (select id,email,rank() OVER (partition by email order by id asc) as rnk from emaildf) \\\n",
    "          where rnk = 1 \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "724f25a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+\n",
      "|min(id)|           email|\n",
      "+-------+----------------+\n",
      "|      1|john@example.com|\n",
      "|      2| bob@example.com|\n",
      "+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" SELECT min(id),email FROM emaildf GROUP BY Email \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552aedde",
   "metadata": {},
   "source": [
    "#### 030 Group Sold Products By The Date\n",
    "\n",
    "Table Activities:\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| sell_date   | date    |\n",
    "| product     | varchar |\n",
    "+-------------+---------+\n",
    "There is no primary key (column with unique values) for this table. It may contain duplicates.\n",
    "Each row of this table contains the product name and the date it was sold in a market.\n",
    " \n",
    "\n",
    "Write a solution to find for each date the number of different products sold and their names.\n",
    "\n",
    "The sold products names for each date should be sorted lexicographically.\n",
    "\n",
    "Return the result table ordered by sell_date.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Activities table:\n",
    "+------------+------------+\n",
    "| sell_date  | product     |\n",
    "+------------+------------+\n",
    "| 2020-05-30 | Headphone  |\n",
    "| 2020-06-01 | Pencil     |\n",
    "| 2020-06-02 | Mask       |\n",
    "| 2020-05-30 | Basketball |\n",
    "| 2020-06-01 | Bible      |\n",
    "| 2020-06-02 | Mask       |\n",
    "| 2020-05-30 | T-Shirt    |\n",
    "+------------+------------+\n",
    "Output: \n",
    "+------------+----------+------------------------------+\n",
    "| sell_date  | num_sold | products                     |\n",
    "+------------+----------+------------------------------+\n",
    "| 2020-05-30 | 3        | Basketball,Headphone,T-shirt |\n",
    "| 2020-06-01 | 2        | Bible,Pencil                 |\n",
    "| 2020-06-02 | 1        | Mask                         |\n",
    "+------------+----------+------------------------------+\n",
    "Explanation: \n",
    "For 2020-05-30, Sold items were (Headphone, Basketball, T-shirt), we sort them lexicographically and separate them by a comma.\n",
    "For 2020-06-01, Sold items were (Pencil, Bible), we sort them lexicographically and separate them by a comma.\n",
    "For 2020-06-02, the Sold item is (Mask), we just return it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8926131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "soldproduct_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/030_GroupSoldProductsByTheDate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "34d5f1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sell_date: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "soldproduct_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1d7b24e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 71:=============================================>        (169 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------------------+\n",
      "|sell_date |num_sold|product                         |\n",
      "+----------+--------+--------------------------------+\n",
      "|2020-05-30|3       |[Headphone, Basketball, T-Shirt]|\n",
      "|2020-06-01|2       |[Bible, Pencil]                 |\n",
      "|2020-06-02|1       |[Mask, Mask]                    |\n",
      "+----------+--------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#collect_set,collect_list,concat_ws\n",
    "soldproduct_df.groupBy('sell_date')\\\n",
    "        .agg(countDistinct('product').alias('num_sold'),collect_list('product').alias('product') ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "ac0f566e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------------------------+\n",
      "|sell_date |num_sold|product                     |\n",
      "+----------+--------+----------------------------+\n",
      "|2020-05-30|3       |T-Shirt,Basketball,Headphone|\n",
      "|2020-06-01|2       |Pencil,Bible                |\n",
      "|2020-06-02|1       |Mask                        |\n",
      "+----------+--------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "soldproduct_df.groupBy('sell_date')\\\n",
    "        .agg(countDistinct('product').alias('num_sold'),concat_ws(',',collect_set('product')).alias('product') ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "b77c9996",
   "metadata": {},
   "outputs": [],
   "source": [
    "soldproduct_df.createOrReplaceTempView('soldproductdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "6c8cfca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------------------+\n",
      "|sell_date |num_sold|products                        |\n",
      "+----------+--------+--------------------------------+\n",
      "|2020-05-30|3       |[Headphone, Basketball, T-Shirt]|\n",
      "|2020-06-01|2       |[Bible, Pencil]                 |\n",
      "|2020-06-02|1       |[Mask, Mask]                    |\n",
      "+----------+--------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select sell_date,count(distinct product) as num_sold,collect_list(product) as products \\\n",
    "        from soldproductdf group by sell_date \").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8556dc",
   "metadata": {},
   "source": [
    "####  031 List the Products Ordered in a Period\n",
    "\n",
    "Table: Products\n",
    "\n",
    "+------------------+---------+\n",
    "| Column Name      | Type    |\n",
    "+------------------+---------+\n",
    "| product_id       | int     |\n",
    "| product_name     | varchar |\n",
    "| product_category | varchar |\n",
    "+------------------+---------+\n",
    "product_id is the primary key (column with unique values) for this table.\n",
    "This table contains data about the company's products.\n",
    " \n",
    "\n",
    "Table: Orders\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| product_id    | int     |\n",
    "| order_date    | date    |\n",
    "| unit          | int     |\n",
    "+---------------+---------+\n",
    "This table may have duplicate rows.\n",
    "product_id is a foreign key (reference column) to the Products table.\n",
    "unit is the number of products ordered in order_date.\n",
    " \n",
    "\n",
    "Write a solution to get the names of products that have at least 100 units ordered in February 2020 and their amount.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Products table:\n",
    "+-------------+-----------------------+------------------+\n",
    "| product_id  | product_name          | product_category |\n",
    "+-------------+-----------------------+------------------+\n",
    "| 1           | Leetcode Solutions    | Book             |\n",
    "| 2           | Jewels of Stringology | Book             |\n",
    "| 3           | HP                    | Laptop           |\n",
    "| 4           | Lenovo                | Laptop           |\n",
    "| 5           | Leetcode Kit          | T-shirt          |\n",
    "+-------------+-----------------------+------------------+\n",
    "Orders table:\n",
    "+--------------+--------------+----------+\n",
    "| product_id   | order_date   | unit     |\n",
    "+--------------+--------------+----------+\n",
    "| 1            | 2020-02-05   | 60       |\n",
    "| 1            | 2020-02-10   | 70       |\n",
    "| 2            | 2020-01-18   | 30       |\n",
    "| 2            | 2020-02-11   | 80       |\n",
    "| 3            | 2020-02-17   | 2        |\n",
    "| 3            | 2020-02-24   | 3        |\n",
    "| 4            | 2020-03-01   | 20       |\n",
    "| 4            | 2020-03-04   | 30       |\n",
    "| 4            | 2020-03-04   | 60       |\n",
    "| 5            | 2020-02-25   | 50       |\n",
    "| 5            | 2020-02-27   | 50       |\n",
    "| 5            | 2020-03-01   | 50       |\n",
    "+--------------+--------------+----------+\n",
    "Output: \n",
    "+--------------------+---------+\n",
    "| product_name       | unit    |\n",
    "+--------------------+---------+\n",
    "| Leetcode Solutions | 130     |\n",
    "| Leetcode Kit       | 100     |\n",
    "+--------------------+---------+\n",
    "Explanation: \n",
    "Products with product_id = 1 is ordered in February a total of (60 + 70) = 130.\n",
    "Products with product_id = 2 is ordered in February a total of 80.\n",
    "Products with product_id = 3 is ordered in February a total of (2 + 3) = 5.\n",
    "Products with product_id = 4 was not ordered in February 2020.\n",
    "Products with product_id = 5 is ordered in February a total of (50 + 50) = 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "24c68f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_Schama = sold_schema =  StructType([\n",
    "                                (StructField('product_id',IntegerType(),False)),\n",
    "                                (StructField('order_date',DateType(),False)),\n",
    "                                (StructField('unit',IntegerType(),False))\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a10b585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/031_ListtheProductsOrderedinaPeriod.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8d0ac934",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df = spark.read.option('header',True).schema(order_Schama).format('csv')\\\n",
    "        .load('../data/easymedium/031a_ListtheProductsOrderedinaPeriod.csv')\\\n",
    "        .withColumn('oproduct_id',col('product_id')).withColumn('order_date',date_format(col('order_date'),'yyyy-MM'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c5cc7915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "95f166cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- unit: integer (nullable = true)\n",
      " |-- oproduct_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "87b7469a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+\n",
      "|      product_name|sum(unit)|\n",
      "+------------------+---------+\n",
      "|      Leetcode Kit|      100|\n",
      "|Leetcode Solutions|      130|\n",
      "+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_df.join(product_df,product_df.product_id == order_df.oproduct_id)\\\n",
    "    .select('product_name','unit').filter(col('order_date') == lit('2020-02'))\\\n",
    "    .groupby(col('product_name')).agg(sum(col('unit'))).filter(col('sum(unit)') >= 100).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dc581608",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df.createOrReplaceTempView('productdf')\n",
    "order_df.createOrReplaceTempView('orderdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e308911d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+\n",
      "|      product_name|total_units|\n",
      "+------------------+-----------+\n",
      "|      Leetcode Kit|        100|\n",
      "|Leetcode Solutions|        130|\n",
      "+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select product_name,sum(unit) as total_units from orderdf join productdf on productdf.product_id = oproduct_id \\\n",
    "        where order_date = '2020-02' group by product_name having total_units >= 100\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4819d108",
   "metadata": {},
   "source": [
    "#### 032 Find Users With Valid E-Mails\n",
    "\n",
    "Table: Users\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| user_id       | int     |\n",
    "| name          | varchar |\n",
    "| mail          | varchar |\n",
    "+---------------+---------+\n",
    "user_id is the primary key (column with unique values) for this table.\n",
    "This table contains information of the users signed up in a website. Some e-mails are invalid.\n",
    " \n",
    "\n",
    "Write a solution to find the users who have valid emails.\n",
    "\n",
    "A valid e-mail has a prefix name and a domain where:\n",
    "\n",
    "The prefix name is a string that may contain letters (upper or lower case), digits, underscore '_', period '.', and/or dash '-'. The prefix name must start with a letter.\n",
    "The domain is '@leetcode.com'.\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Users table:\n",
    "+---------+-----------+-------------------------+\n",
    "| user_id | name      | mail                    |\n",
    "+---------+-----------+-------------------------+\n",
    "| 1       | Winston   | winston@leetcode.com    |\n",
    "| 2       | Jonathan  | jonathanisgreat         |\n",
    "| 3       | Annabelle | bella-@leetcode.com     |\n",
    "| 4       | Sally     | sally.come@leetcode.com |\n",
    "| 5       | Marwan    | quarz#2020@leetcode.com |\n",
    "| 6       | David     | david69@gmail.com       |\n",
    "| 7       | Shapiro   | .shapo@leetcode.com     |\n",
    "+---------+-----------+-------------------------+\n",
    "Output: \n",
    "+---------+-----------+-------------------------+\n",
    "| user_id | name      | mail                    |\n",
    "+---------+-----------+-------------------------+\n",
    "| 1       | Winston   | winston@leetcode.com    |\n",
    "| 3       | Annabelle | bella-@leetcode.com     |\n",
    "| 4       | Sally     | sally.come@leetcode.com |\n",
    "+---------+-----------+-------------------------+\n",
    "Explanation: \n",
    "The mail of user 2 does not have a domain.\n",
    "The mail of user 5 has the # sign which is not allowed.\n",
    "The mail of user 6 does not have the leetcode domain.\n",
    "The mail of user 7 starts with a period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dc01df34",
   "metadata": {},
   "outputs": [],
   "source": [
    "validuser_df = spark.read.option('header',True).option('inferSchema',True).format('csv')\\\n",
    "        .load('../data/easymedium/032_FindUsersWithValidEMails.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "86459fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- mail: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validuser_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ae5a7ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------------+\n",
      "|user_id|     name|                mail|\n",
      "+-------+---------+--------------------+\n",
      "|      1|  Winston|winston@leetcode.com|\n",
      "|      3|Annabelle| bella-@leetcode.com|\n",
      "|      4|    Sally|sally.come@leetco...|\n",
      "+-------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validuser_df.select('user_id','name','mail').filter( (col('mail').like('%@leetcode.com')) \\\n",
    "                            & (~col('mail').like('%#%')) & (~col('mail').like('.%'))  ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "50f6bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "validuser_df.createOrReplaceTempView('validuserdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2d43da99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------------+\n",
      "|user_id|     name|                mail|\n",
      "+-------+---------+--------------------+\n",
      "|      1|  Winston|winston@leetcode.com|\n",
      "|      3|Annabelle| bella-@leetcode.com|\n",
      "|      4|    Sally|sally.come@leetco...|\n",
      "+-------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select user_id,name,mail from validuserdf where mail like '%@leetcode.com' \\\n",
    "        and mail not like '%#%' and mail not like '.%' \").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
