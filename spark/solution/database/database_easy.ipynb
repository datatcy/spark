{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b496830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('database').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd793a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (col,expr,count,countDistinct,datediff,to_date,date_add,year,month,lag,lead,rank,max,min,round,\n",
    "        sum,when,lit,desc,coalesce,abs,greatest,least,array,array_sort,substring, explode,collect_list,array_intersect,\n",
    "        unix_timestamp,rank,dense_rank,least,greatest,row_number,array_join,expr,trim,lower,array,sort_array,\n",
    "        array_distinct,size,initcap,length,date_format,to_timestamp,concat,regexp_extract,length,regexp_replace,\n",
    "\n",
    "                                  )\n",
    "from pyspark.sql.types import (StructField,StructType,\n",
    "                    IntegerType,StringType,DateType,TimestampType )\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import Row\n",
    "#rlike,contains,array_join,collect_list,substring,array_size,cast,stack(to unpivot),LATERAL VIEW "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedb2ad4",
   "metadata": {},
   "source": [
    " #### 1 Combine Two Tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fba230f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/1_Person.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b862adab",
   "metadata": {},
   "outputs": [],
   "source": [
    "address_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/1a_Address.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f0db19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- personId: integer (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1dee621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- addressId: integer (nullable = true)\n",
      " |-- personId: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "address_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d65cbed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------------+--------+\n",
      "|firstName|lastName|         city|   state|\n",
      "+---------+--------+-------------+--------+\n",
      "|    Allen|    Wang|         null|    null|\n",
      "|      Bob|   Alice|New York City|New York|\n",
      "+---------+--------+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(person_df.join(address_df,person_df.personId == address_df.personId,'left')\n",
    ".select(col('firstName'),col('lastName'),col('city'),col('state')).show() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5780e82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_df.createOrReplaceTempView('persondf')\n",
    "address_df.createOrReplaceTempView('addressdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "97ac48ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------------+--------+\n",
      "|firstName|lastName|         city|   state|\n",
      "+---------+--------+-------------+--------+\n",
      "|    Allen|    Wang|         null|    null|\n",
      "|      Bob|   Alice|New York City|New York|\n",
      "+---------+--------+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select firstName,lastName,city,state  \\\n",
    "          from persondf left join addressdf on persondf.personId  == addressdf.personId').show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dedbda2",
   "metadata": {},
   "source": [
    "#### 2 Employees Earning More Than Their Managers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3e35a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df = (spark.read\n",
    "              .option('header',True)\n",
    "              .option('inferSchema',True)\n",
    "              .format('csv')\n",
    "              .load('../../data/database/2_Employee.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3cb7e907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- managerId: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4a107f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df = employee_df.filter(col('managerId').isNotNull() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c89161a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgr_df = ( employee_df.filter(col('managerId').isNull() )\n",
    "          .select(col('id').alias('mgrid'),col('salary').alias('mgrsalary')) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5c8b901c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "| Joe|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "emp_df.join(mgr_df,mgr_df.mgrid == emp_df.managerId,'inner')\n",
    "    .filter(col('salary') > col('mgrsalary')).select('name').show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5cec9ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.createOrReplaceTempView('employeedf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "50b79442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "| Joe|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('with mgr as (select id,salary from employeedf where managerId is null),\\\n",
    "                emp as (select id,name,salary,managerId from employeedf where managerId is not null)\\\n",
    "                select emp.name from emp join mgr on emp.managerId == mgr.id where emp.salary > mgr.salary').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14fb98f",
   "metadata": {},
   "source": [
    "#### 3 Duplicate Emails E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33759508",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/3_person.csv') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f6f7ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "689ecdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|  email|\n",
      "+-------+\n",
      "|a@b.com|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person_df.groupby(col('email')).count().filter(col('count') >=2).select(col('email')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d7ce84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|  email|\n",
      "+-------+\n",
      "|a@b.com|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person_df.createOrReplaceTempView('person_df')\n",
    "\n",
    "spark.sql('select email from person_df group by email having count(email) >= 2').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b1e340",
   "metadata": {},
   "source": [
    "#### 4 Customers Who Never Order E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2090f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/database/4_customers.csv') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b83873b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = (spark.read\n",
    "            .option('header',True)\n",
    "            .option('inferSchema',True)\n",
    "            .format('csv')\n",
    "            .load('../../data/database/4a_orders.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1582316e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a7ad2ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- customerId: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ca34007c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "|Henry|\n",
      "|  Max|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.join(orders_df,customer_df.id == orders_df.customerId,'leftanti').select('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "55997981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "|Henry|\n",
      "|  Max|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.createOrReplaceTempView('customerdf')\n",
    "orders_df.createOrReplaceTempView('ordersdf')\n",
    "\n",
    "spark.sql('select name from customerdf left anti join ordersdf on customerdf.id == ordersdf.customerId ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70400ce7",
   "metadata": {},
   "source": [
    "#### 5 Delete Duplicate Emails E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63a43e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_df = ( spark.read\n",
    "            .option('header',True)\n",
    "            .option('inferSchema',True)\n",
    "            .format('csv')\n",
    "            .load('../../data/database/5_person.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e0939248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "116f13f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+\n",
      "| id|           email|\n",
      "+---+----------------+\n",
      "|  1|john@example.com|\n",
      "|  2| bob@example.com|\n",
      "+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person_df.dropDuplicates(['email']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a833433",
   "metadata": {},
   "outputs": [],
   "source": [
    "SET SQL_SAFE_UPDATES = 0;\n",
    "with one as (select id,email, row_number() over (partition by email order by id) as rnumber from db.person)\n",
    "DELETE FROM db.person where id in (select id from one where rnumber >= 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15ce7f7",
   "metadata": {},
   "source": [
    "#### 6 Rising Temperature E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bf08fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_schema = StructType([\n",
    "        StructField('id',IntegerType()),\n",
    "        StructField('recordDate',DateType()),\n",
    "        StructField('temperature',IntegerType())\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c694f8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = (spark.read\n",
    "              .option('header',True)\n",
    "              .schema(weather_schema)\n",
    "              .format('csv')\n",
    "              .load('../../data/database/6_weather.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "641a5645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- recordDate: date (nullable = true)\n",
      " |-- temperature: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "3b2be2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, recordDate: date, temperature: int]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "weather_df.withColumn('recordDate',to_date(col('recordDate'),'MM-dd-yyyy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "dabf2170",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.orderBy(weather_df.recordDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6136da74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  2|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/12 15:57:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "weather_df.withColumn('nextday',lag(col('temperature'),1).over(window_spec).alias('lags'))\\\n",
    "    .filter(col('temperature') > col('nextday')).select('id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9c500258",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.createOrReplaceTempView('weatherdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "ebba06e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  2|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/12 16:31:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"with nextday as (select recordDate as date,lag(temperature) over(order by recordDate) as temp from weatherdf) \\\n",
    "                    select id from weatherdf join nextday on recordDate = date \\\n",
    "                      where temperature > temp \\\n",
    "          \" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "5534a8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  2|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/12 16:36:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select id from weatherdf,\\\n",
    "    (select recordDate as date, lag(temperature) over(order by recordDate) as temp from weatherdf) \\\n",
    "          where date = recordDate and temperature > temp ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf802ce",
   "metadata": {},
   "source": [
    "#### 7 Game Play Analysis E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d30ef559",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_schema = StructType([\n",
    "                  StructField('player_id',IntegerType()),\n",
    "                  StructField('device_id',IntegerType()),\n",
    "                  StructField('event_date',DateType()),\n",
    "                  StructField('games_played',IntegerType()),\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8946245",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_df = (spark.read\n",
    "              .option('header',True)\n",
    "              .schema(activity_schema)\n",
    "              .format('csv')\n",
    "              .load('../../data/database/7_activity.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "a4579c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- player_id: integer (nullable = true)\n",
      " |-- device_id: integer (nullable = true)\n",
      " |-- event_date: date (nullable = true)\n",
      " |-- games_played: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "e028e52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy('player_id').orderBy('event_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "33f6ffc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|player_id|first_login|\n",
      "+---------+-----------+\n",
      "|        1| 2016-03-01|\n",
      "|        2| 2017-06-25|\n",
      "|        3| 2016-03-02|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity_df.withColumn('cnt',rank().over(window_spec)).filter(col('cnt') == 1)\\\n",
    "        .select('player_id',col('event_date').alias('first_login')).orderBy('player_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "d293d00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|player_id|first_login|\n",
      "+---------+-----------+\n",
      "|        1| 2016-03-01|\n",
      "|        2| 2017-06-25|\n",
      "|        3| 2016-03-02|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity_df.createOrReplaceTempView('activitydf')\n",
    "spark.sql('with rnk as (select player_id,event_date as first_login,\\\n",
    "                        rank() over(PARTITION BY player_id ORDER BY event_date) as rnk from activitydf) \\\n",
    "                select player_id,first_login from rnk where rnk == 1 order by player_id ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28346d5e",
   "metadata": {},
   "source": [
    "#### 8 Game Play Analysis II E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02c6983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_schema = StructType([\n",
    "                  StructField('player_id',IntegerType()),\n",
    "                  StructField('device_id',IntegerType()),\n",
    "                  StructField('event_date',DateType()),\n",
    "                  StructField('games_played',IntegerType()),\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fa6e56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_df = (spark.read\n",
    "              .option('header',True)\n",
    "              .schema(activity_schema)\n",
    "              .format('csv')\n",
    "              .load('../../data/database/8_activity.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "42ac4f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- player_id: integer (nullable = true)\n",
      " |-- device_id: integer (nullable = true)\n",
      " |-- event_date: date (nullable = true)\n",
      " |-- games_played: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "fb502509",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy('player_id').orderBy('event_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "ffc581fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|player_id|device_id|\n",
      "+---------+---------+\n",
      "|        1|        2|\n",
      "|        2|        3|\n",
      "|        3|        1|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity_df.withColumn('rnk',rank().over(window_spec)).filter(col('rnk') ==1 )\\\n",
    "        .select('player_id','device_id').orderBy('player_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "037d1525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|player_id|device_id|\n",
      "+---------+---------+\n",
      "|        1|        2|\n",
      "|        2|        3|\n",
      "|        3|        1|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity_df.createOrReplaceTempView('activitydf')\n",
    "spark.sql('with rnk as (select player_id,device_id,\\\n",
    "                        rank() over(PARTITION BY player_id ORDER BY event_date) as rnk from activitydf) \\\n",
    "                select player_id,device_id from rnk where rnk == 1 order by player_id ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16587060",
   "metadata": {},
   "source": [
    "#### 9 Employee Bonus E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "151ead4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/database/9_employee.csv') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4f03821",
   "metadata": {},
   "outputs": [],
   "source": [
    "bonus_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/database/9a_bonus.csv') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "e24db3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empid: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- supervisor: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "68e8b2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empid: integer (nullable = true)\n",
      " |-- bonus: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bonus_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "14f40cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|name|bonus|\n",
      "+----+-----+\n",
      "|Brad| null|\n",
      "|John| null|\n",
      "| Dan|  500|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.join(bonus_df,employee_df.empid == bonus_df.empid, 'left')\\\n",
    "        .filter( (col('bonus') <= 1000) | col('bonus').isNull() ).select('name','bonus').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "481e52f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|name|bonus|\n",
      "+----+-----+\n",
      "|Brad| null|\n",
      "|John| null|\n",
      "| Dan|  500|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.createOrReplaceTempView('employeedf')\n",
    "bonus_df.createOrReplaceTempView('bonusdf')\n",
    "\n",
    "spark.sql('select name,bonus from employeedf left join bonusdf on employeedf.empid == bonusdf.empid \\\n",
    "            where bonus <=1000 or bonus is null').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897a13a6",
   "metadata": {},
   "source": [
    "#### 10 Find Customer Referee E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee5f3c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = (spark.read\n",
    "              .option('header',True)\n",
    "              .option('inferSchema',True)\n",
    "              .format('csv')\n",
    "              .load('../../data/database/10_customer.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "bb652b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- referee_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "3babcbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|Will|\n",
      "|Jane|\n",
      "|Bill|\n",
      "|Zack|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.filter( (col('referee_id') !=2) | (col('referee_id').isNull()) ).select('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "3dbaf555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|Will|\n",
      "|Jane|\n",
      "|Bill|\n",
      "|Zack|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.createOrReplaceTempView('customerdf')\n",
    "spark.sql('select name from customerdf where referee_id !=2 or referee_id is null').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8966f78",
   "metadata": {},
   "source": [
    "#### 11 Customer Placing the Largest Number of Orders E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "10e21fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df = (spark.read\n",
    "           .option('header',True)\n",
    "           .option('inferSchema',True)\n",
    "           .format('csv')\n",
    "           .load('../../data/database/11_orders.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "cd49bbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_number: integer (nullable = true)\n",
      " |-- customer_number: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "105837ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_df.orderBy('order_number',ascending = False).select('customer_number').first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "da80e575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+\n",
      "|customer_number|mxorder|\n",
      "+---------------+-------+\n",
      "|              3|      4|\n",
      "+---------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_df.createOrReplaceTempView('orderdf')\n",
    "spark.sql('select customer_number,order_number as mxorder from orderdf order by order_number desc limit 1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8168e9",
   "metadata": {},
   "source": [
    "#### 12 Big Countries E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd019a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_df = (spark.read\n",
    "           .option('header',True)\n",
    "           .option('inferSchema',True)\n",
    "           .format('csv')\n",
    "           .load('../../data/database/12_world.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "a21bb1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- area: integer (nullable = true)\n",
      " |-- population: integer (nullable = true)\n",
      " |-- dgp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "world_df.printSchema()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "09314344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------+\n",
      "|       name|population|   area|\n",
      "+-----------+----------+-------+\n",
      "|Afghanistan|  25500100| 652230|\n",
      "|    Algeria|  37100000|2381741|\n",
      "+-----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "world_df.filter( (col('area') >= 3000000) | (col('population') >= 25000000 ) )\\\n",
    "            .select('name','population','area').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "e3608aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------+\n",
      "|       name|population|   area|\n",
      "+-----------+----------+-------+\n",
      "|Afghanistan|  25500100| 652230|\n",
      "|    Algeria|  37100000|2381741|\n",
      "+-----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "world_df.createOrReplaceTempView('worlddf')\n",
    "spark.sql('select name,population,area from worlddf where area >= 3000000 or population >= 25000000 ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f7eec0",
   "metadata": {},
   "source": [
    "#### 13 Classes More Than 5 Students E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "300462eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/13_courses.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "31b81e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- student: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "courses_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "08b06715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|class|\n",
      "+-----+\n",
      "| Math|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "courses_df.groupBy('class').agg(count('class').alias('cnt')).filter(col('cnt') >=5).select('class').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "b303a55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|class|\n",
      "+-----+\n",
      "| Math|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "courses_df.createOrReplaceTempView('coursesdf')\n",
    "spark.sql('select class from coursesdf group by class having count(\"class\") >= 5').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcb02d5",
   "metadata": {},
   "source": [
    "#### 14 Friend Requests I: Overall Acceptance Rate E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc3d4c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "friendsrequest_schema = StructType([\n",
    "                        StructField('sender_id',IntegerType()),\n",
    "                        StructField('send_to_id',IntegerType()),\n",
    "                        StructField('request_date',DateType()),\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7cb99f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "requestaccepted_schema = StructType([\n",
    "                         StructField('requester_id',IntegerType()),\n",
    "                         StructField('accepter_id',IntegerType()),\n",
    "                         StructField('accept_date',DateType()),\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0f6e0327",
   "metadata": {},
   "outputs": [],
   "source": [
    "friendsrequest_df = (spark.read\n",
    "                    .option('header',True)\n",
    "                    .schema(friendsrequest_schema)\n",
    "                    .format('csv')\n",
    "                    .load('../../data/database/14_friendrequest.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10c59edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "requestaccepted_df = (spark.read\n",
    "                    .option('header',True)\n",
    "                    .schema(requestaccepted_schema)\n",
    "                    .format('csv')\n",
    "                    .load('../../data/database/14a_requestaccepted.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "582d28e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sender_id: integer (nullable = true)\n",
      " |-- send_to_id: integer (nullable = true)\n",
      " |-- request_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "friendsrequest_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "6d681516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- requester_id: integer (nullable = true)\n",
      " |-- accepter_id: integer (nullable = true)\n",
      " |-- accept_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "requestaccepted_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "2fe47811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|acceptance_ratio|\n",
      "+----------------+\n",
      "|             0.8|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "requestaccepted_df.select(round(countDistinct('requester_id','accepter_id')/count('requester_id'),2)\\\n",
    "                    .alias('acceptance_ratio')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "a00ddcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|acceptance_rate|\n",
      "+---------------+\n",
      "|            0.8|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "requestaccepted_df.createOrReplaceTempView('requestaccepteddf')\n",
    "spark.sql('select round(count(distinct requester_id,accepter_id)/count(requester_id),2) \\\n",
    "            as acceptance_rate from requestaccepteddf').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d1cdba",
   "metadata": {},
   "source": [
    "#### 15 Consecutive Available Seats E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19c17e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cinema_df = (spark.read\n",
    "            .option('header',True)\n",
    "            .option('inferSchema',True)\n",
    "            .format('csv')\n",
    "            .load('../../data/database/15_cinema.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "46f571d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- seat_id: integer (nullable = true)\n",
      " |-- free: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cinema_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "abdba067",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.orderBy('seat_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "55140a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|seat_id|\n",
      "+-------+\n",
      "|      3|\n",
      "|      4|\n",
      "|      5|\n",
      "+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/13 11:07:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/13 11:07:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "cinema_df.withColumn('rowsdiff',sum(when(lag('free').over(window_spec) != col('free'),1) \\\n",
    "                    .otherwise(0)).over(window_spec)).filter( (col('free') != 0) & (col('rowsdiff') != 0) )\\\n",
    "                   .select('seat_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06e1903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4e38be07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|seat_id|\n",
      "+-------+\n",
      "|      3|\n",
      "|      4|\n",
      "|      5|\n",
      "+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/13 11:37:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/13 11:37:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "cinema_df.createOrReplaceTempView('cinemadf')\n",
    "spark.sql('with diff as  (select seat_id,free,sum( case when (lag(free) over( order by seat_id)) != free then 1 \\\n",
    "                                                   else 0 end) over(order by seat_id) as diff from cinemadf order by seat_id) \\\n",
    "                  select seat_id from diff where free !=0 and diff !=0 \\\n",
    "                            ').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdbcbc4",
   "metadata": {},
   "source": [
    "#### 16 Sales Person E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ec4e8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "salesperson_schema = StructType([\n",
    "                     StructField('sales_id',IntegerType()),\n",
    "                     StructField('name',StringType()),\n",
    "                     StructField('salary',IntegerType()),\n",
    "                     StructField('commission_rate',IntegerType()),\n",
    "                     StructField('hire_date',DateType())\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "44bc62d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_schema = StructType([\n",
    "                     StructField('order_id',IntegerType()),\n",
    "                     StructField('order_date',DateType()),\n",
    "                     StructField('com_id',IntegerType()),\n",
    "                     StructField('sales_id',IntegerType()),\n",
    "                     StructField('amount',IntegerType())\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "50f2b02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "salesperson_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .schema(salesperson_schema) \n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/16_salesperson.csv')\n",
    "                 .withColumn('hire_date',to_date('hire_date','MM-dd-yyyy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "23c86c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True) \n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/16a_company.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ddd39670",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .schema(order_schema) \n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/16b_order.csv')\n",
    "                 .withColumn('order_date',to_date('order_date','MM-dd-yyyy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "79205668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sales_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- commission_rate: integer (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salesperson_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "541b4750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- com_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "company_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d9f0bb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- com_id: integer (nullable = true)\n",
      " |-- sales_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4a5bc59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "| Amy|\n",
      "|Mark|\n",
      "|Alex|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordf = order_df.join(company_df,company_df.com_id == order_df.com_id,'inner').filter(col('name') == 'RED')\\\n",
    "    .select('sales_id')\n",
    "salesperson_df.join(ordf,salesperson_df.sales_id == ordf.sales_id,'leftanti').select('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "88777e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "| Amy|\n",
      "|Mark|\n",
      "|Alex|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salesperson_df.createOrReplaceTempView('salespersondf')\n",
    "company_df.createOrReplaceTempView('companydf')\n",
    "order_df.createOrReplaceTempView('orderdf')\n",
    "\n",
    "spark.sql('with one as (select sales_id from orderdf  \\\n",
    "                          join companydf on companydf.com_id == orderdf.com_id where name == \"RED\") \\\n",
    "                 select name from salespersondf left anti join one on salespersondf.sales_id == one.sales_id \\\n",
    "         ').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9ee8845e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "| Amy|\n",
      "|Mark|\n",
      "|Alex|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql('select name from salespersondf where sales_id not in (select sales_id from orderdf  \\\n",
    "                          join companydf on companydf.com_id == orderdf.com_id where name == \"RED\")\\\n",
    "         ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b61f0f3",
   "metadata": {},
   "source": [
    "#### 17 Triangle Judgement E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7d99e7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "triangle_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True) \n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/17_triangle.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "87c47a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: integer (nullable = true)\n",
      " |-- y: integer (nullable = true)\n",
      " |-- z: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "triangle_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b854c7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+\n",
      "|  x|  y|  z|triangle|\n",
      "+---+---+---+--------+\n",
      "| 13| 15| 30|      No|\n",
      "| 10| 20| 15|     Yes|\n",
      "+---+---+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "triangle_df.withColumn('triangle',when( (col('x')+col('y') > col('z')) & (col('y')+col('z') > col('x')) &\\\n",
    "                                    (col('x')+col('z') > col('y')),'Yes').otherwise('No')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a848fde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---------+\n",
      "|  x|  y|  z|traiangle|\n",
      "+---+---+---+---------+\n",
      "| 13| 15| 30|       No|\n",
      "| 10| 20| 15|      Yes|\n",
      "+---+---+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "triangle_df.createOrReplaceTempView('triangledf')\n",
    "spark.sql('select x,y,z,case when x+y>z and y+z>x and x+z>y then \"Yes\" \\\n",
    "                             else \"No\" end as traiangle from triangledf').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd2e25f",
   "metadata": {},
   "source": [
    "#### 18 Shortest Distance in a Line E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4d819306",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True) \n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/18_point.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "b8b61028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "point_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "380dbd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  x|\n",
      "+---+\n",
      "| -1|\n",
      "|  0|\n",
      "|  2|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "point_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a03b9097",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec =  Window.orderBy('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "50222dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|min_distancee|\n",
      "+-------------+\n",
      "|            1|\n",
      "+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/13 15:37:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "point_df.withColumn('xx',lag('x',1).over(window_spec)).select(expr('x-xx')\\\n",
    "                        .alias('xxx')).agg(min('xxx').alias('min_distancee')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "3cc886e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|min_distance|\n",
      "+------------+\n",
      "|           1|\n",
      "+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/13 15:42:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "point_df.createOrReplaceTempView('pointdf')\n",
    "spark.sql('with one as (select x,lag(x) over(order by x) as xx from pointdf) \\\n",
    "                   select min(x - xx) as min_distance from one').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4eaef1",
   "metadata": {},
   "source": [
    "#### 19  Biggest Single Number E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "99bfa0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mynumber_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True) \n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/19_mynumber.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "20fbabb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- num: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mynumber_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "e6ae2374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|single_largest_number|\n",
      "+---------------------+\n",
      "|                    6|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mynumber_df.groupBy('num').count().filter(col('count') == 1).agg(max(col('num'))\\\n",
    "                                        .alias('single_largest_number')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "3353cd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|single_largest_number|\n",
      "+---------------------+\n",
      "|                    6|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mynumber_df.createOrReplaceTempView('mynumberdf')\n",
    "spark.sql('select max(num) as single_largest_number from (select num from mynumberdf group by num having count(num)==1)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caecdb3",
   "metadata": {},
   "source": [
    "#### 20 Not Boring Movies E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c1c80f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "cinima_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True) \n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/20_cinima.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "283f5c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- movie: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cinima_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "3cea130c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n",
      "| id|     movie|description|rating|\n",
      "+---+----------+-----------+------+\n",
      "|  5|House card|Interesting|   9.1|\n",
      "|  1|       War|   great 3D|   8.9|\n",
      "+---+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cinima_df.filter( (col('description') != lit('boring')) & (col('id')%2 == 1))\\\n",
    "                    .orderBy(desc('rating')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "38911a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n",
      "| id|     movie|description|rating|\n",
      "+---+----------+-----------+------+\n",
      "|  5|House card|Interesting|   9.1|\n",
      "|  1|       War|   great 3D|   8.9|\n",
      "+---+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cinima_df.createOrReplaceTempView('cinimadf')\n",
    "spark.sql('select id,movie,description,rating from cinimadf \\\n",
    "              where description != \"boring\" and id%2 == 1 order by rating desc').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4bc2d8",
   "metadata": {},
   "source": [
    "#### 21 Swap Salary E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3d835025",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True) \n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/21_salary.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "14eb5658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salary_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3134ec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD DATA  LOCAL INFILE '../../data/database/21_salary.csv'\n",
    "INTO TABLE db.salary\n",
    "FIELDS TERMINATED BY ','\n",
    "LINES TERMINATED BY '\\n'\n",
    "IGNORE 1 ROWS\n",
    "(id,name,sex,salary);\n",
    "\n",
    "\n",
    "SET SQL_SAFE_UPDATES = 0;\n",
    "UPDATE db.salary\n",
    "SET sex = \n",
    "    CASE \n",
    "        WHEN sex = 'm' THEN 'f'\n",
    "        WHEN sex = 'f' THEN 'm'\n",
    "    END;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfcd483",
   "metadata": {},
   "source": [
    "#### 22 Actors and Directors Who Cooperated At Least Three Times E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "98f97812",
   "metadata": {},
   "outputs": [],
   "source": [
    "actordirector_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True) \n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/22_actordirector.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "549c322c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- actor_id: integer (nullable = true)\n",
      " |-- director_id: integer (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actordirector_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "2769c05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|director_id|actor_id|\n",
      "+-----------+--------+\n",
      "|          1|       1|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actordirector_df.groupby('director_id','actor_id').count()\\\n",
    "                .filter(col('count') >=3 ).select('director_id','actor_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "f180bb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|director_id|actor_id|\n",
      "+-----------+--------+\n",
      "|          1|       1|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actordirector_df.createOrReplaceTempView('actordirectordf')\n",
    "spark.sql('select director_id,actor_id from  actordirectordf \\\n",
    "                group by director_id,actor_id having count(actor_id) >= 3').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0c92f6",
   "metadata": {},
   "source": [
    "#### 23 Product Sales Analysis I E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e45b895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True) \n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/23_sales.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e0de243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True) \n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/23_product.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "5b02b8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sale_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "627883e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "e4ad011b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+-----+\n",
      "|product_name|year|price|\n",
      "+------------+----+-----+\n",
      "|       Nokia|2008| 5000|\n",
      "|       Nokia|2009| 5000|\n",
      "|       Apple|2011| 9000|\n",
      "+------------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.join(product_df,sales_df.product_id == product_df.product_id,'inner')\\\n",
    "                .select('product_name','year','price').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "12ec18e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+-----+\n",
      "|product_name|year|price|\n",
      "+------------+----+-----+\n",
      "|       Nokia|2008| 5000|\n",
      "|       Nokia|2009| 5000|\n",
      "|       Apple|2011| 9000|\n",
      "+------------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_df.createOrReplaceTempView('productdf')\n",
    "sales_df.createOrReplaceTempView('salesdf')\n",
    "\n",
    "spark.sql('select product_name,year,price from salesdf \\\n",
    "               join productdf on productdf.product_id == salesdf.product_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abb5f1a",
   "metadata": {},
   "source": [
    "#### 24 54 Product Sales Analysis II E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "16bcf04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True) \n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/24_sales.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8b75d055",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True) \n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/24_product.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "b9c890ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sale_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "0450db1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "cac13fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|product_id|total_quandity|\n",
      "+----------+--------------+\n",
      "|       100|            22|\n",
      "|       200|            15|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.groupby('product_id').agg(sum(col('quantity')).alias('total_quandity')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "61b1e977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|product_id|total_quandity|\n",
      "+----------+--------------+\n",
      "|       100|            22|\n",
      "|       200|            15|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.createOrReplaceTempView('salesdf')\n",
    "\n",
    "spark.sql('select product_id,sum(quantity) as total_quandity from salesdf group by product_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31240b9d",
   "metadata": {},
   "source": [
    "#### 25 56 Project Employees I E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "14d212aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True) \n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/25_employee.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3d0f8d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True) \n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/25_project.csv')\n",
    "                 .withColumnRenamed('employee_id','emp_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "905d3ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- experience_years: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "0ff8ddec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- project_id: integer (nullable = true)\n",
      " |-- emp_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "project_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "5ce82eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|project_id|average|\n",
      "+----------+-------+\n",
      "|         1|    2.0|\n",
      "|         2|    2.5|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "project_df.join(employee_df,project_df.emp_id == employee_df.employee_id, 'inner')\\\n",
    "        .groupby('project_id').agg((sum(col('experience_years'))/count(col('project_id'))).alias('average') ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "5d50e7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|project_id|average|\n",
      "+----------+-------+\n",
      "|         1|    2.0|\n",
      "|         2|    2.5|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.createOrReplaceTempView('employeedf')\n",
    "project_df.createOrReplaceTempView('projectdf')\n",
    "\n",
    "spark.sql('select project_id,sum(experience_years)/count(project_id) as average from employeedf \\\n",
    "           join projectdf on projectdf.emp_id == employeedf.employee_id group by project_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fafb90",
   "metadata": {},
   "source": [
    "#### 26 57 Project Employees II E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "20d1f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True) \n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/26_employee.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e2a03ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True) \n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/26_project.csv')\n",
    "                 .withColumnRenamed('employee_id','emp_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "cd14f17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- experience_years: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "1b00327c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- project_id: integer (nullable = true)\n",
      " |-- emp_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "project_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "2d276088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|project_id|\n",
      "+----------+\n",
      "|         1|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mx_df = project_df.groupby('project_id').agg(count(col('emp_id')).alias('emp')).collect()[0][1]\n",
    "project_df.groupby('project_id').agg(count(col('emp_id')).alias('emp'))\\\n",
    "                .filter(col('emp') == mx_df).select('project_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "27e59a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|project_id|\n",
      "+----------+\n",
      "|         1|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "project_df.createOrReplaceTempView('projectdf')\n",
    "spark.sql('select project_id from (select project_id,count(emp_id) as cnt \\\n",
    "          from projectdf group by project_id order by cnt desc limit 1)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef01e47",
   "metadata": {},
   "source": [
    "#### 27 59 Sales Analysis I E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "98ccb4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_schema = StructType([\n",
    "                     StructField('seller_id',IntegerType()),\n",
    "                     StructField('product_id',IntegerType()),\n",
    "                     StructField('buyer_id',IntegerType()),\n",
    "                     StructField('sale_date',DateType()),\n",
    "                     StructField('quantity',IntegerType()),\n",
    "                     StructField('price',IntegerType()),\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "faef87f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .schema(sales_schema)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/27_sales.csv')\n",
    "                 .withColumn('sale_date',to_date('sale_date','MM-dd-yyyy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "21822c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True) \n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/27_product.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "5b1a7972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- seller_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- buyer_id: integer (nullable = true)\n",
      " |-- sale_date: date (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "44f816d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- unit_price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "8af442ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|seller_id|\n",
      "+---------+\n",
      "|        1|\n",
      "|        3|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ms_df = sales_df.groupby('seller_id').agg(sum(col('price')).alias('mx')).select(max('mx').alias('mx')).collect()[0][0]\n",
    "sales_df.groupby('seller_id').agg(sum(col('price')).alias('mx')).filter(col('mx') == ms_df).select('seller_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "bbbb1e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|seller_id|\n",
      "+---------+\n",
      "|        1|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = sales_df.groupBy(\"seller_id\") \\\n",
    "                   .agg(sum(\"price\").alias(\"total_price\")) \\\n",
    "                   .orderBy(desc(\"total_price\")) \\\n",
    "                   .limit(1)\n",
    "result_df.select('seller_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "37d98c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|seller_id|\n",
      "+---------+\n",
      "|        1|\n",
      "|        3|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('with one as (select sum(price) as price from salesdf group by seller_id order by price desc limit 1),\\\n",
    "                two as (select seller_id,sum(price) as price from salesdf group by seller_id)\\\n",
    "                    select seller_id from two where price in (select * from one) \\\n",
    "                 ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a234f3",
   "metadata": {},
   "source": [
    "#### 28 60 Sales Analysis II E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "87c53b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_schema = StructType([\n",
    "                     StructField('seller_id',IntegerType()),\n",
    "                     StructField('product_id',IntegerType()),\n",
    "                     StructField('buyer_id',IntegerType()),\n",
    "                     StructField('sale_date',DateType()),\n",
    "                     StructField('quantity',IntegerType()),\n",
    "                     StructField('price',IntegerType()),\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "08fa88fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .schema(sales_schema)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/28_sales.csv')\n",
    "                 .withColumn('sale_date',to_date('sale_date','MM-dd-yyyy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dcd3863c",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True) \n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/28_product.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "994886ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- seller_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- buyer_id: integer (nullable = true)\n",
      " |-- sale_date: date (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "72673aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- unit_price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "506efa39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|buyer_id|\n",
      "+--------+\n",
      "|       1|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales = sales_df.join(product_df,sales_df.product_id == product_df.product_id,'inner')\\\n",
    "            .select('buyer_id','product_name')\n",
    "\n",
    "s8_df = sales.filter(col('product_name') == lit('S8'))\n",
    "gp_buyer = sales.groupBy('buyer_id').agg(countDistinct(col('product_name')).alias('gp_buyer_product'))\n",
    "s8_df.join(gp_buyer,'buyer_id').filter(gp_buyer.gp_buyer_product == 1).select('buyer_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "b7444691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|buyer_id|\n",
      "+--------+\n",
      "|       1|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.createOrReplaceTempView('salesdf')\n",
    "product_df.createOrReplaceTempView('productdf')\n",
    "\n",
    "spark.sql('with sales as (select buyer_id,product_name from salesdf \\\n",
    "                                 join productdf on salesdf.product_id  == productdf.product_id ), \\\n",
    "                   S  as (select buyer_id,product_name from sales where product_name == \"S8\"), \\\n",
    "        buyer_product as (select buyer_id,count(distinct product_name) as buyer_product from sales group by buyer_id), \\\n",
    "           final_join as (select S.buyer_id from S join buyer_product on S.buyer_id  == buyer_product.buyer_id \\\n",
    "                                 where buyer_product.buyer_product == 1) \\\n",
    "                                 select * from final_join \\\n",
    "                                 ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc71805a",
   "metadata": {},
   "source": [
    "#### 29 61 Sales Analysis III E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ec88b5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_schema = StructType([\n",
    "                     StructField('seller_id',IntegerType()),\n",
    "                     StructField('product_id',IntegerType()),\n",
    "                     StructField('buyer_id',IntegerType()),\n",
    "                     StructField('sale_date',DateType()),\n",
    "                     StructField('quantity',IntegerType()),\n",
    "                     StructField('price',IntegerType()),\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "20a89442",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .schema(sales_schema)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/29_sales.csv')\n",
    "                 .withColumn('sale_date',to_date('sale_date','MM-dd-yyyy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fa9e5384",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True) \n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/29_product.csv')\n",
    "                 .withColumnRenamed('product_id','productid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "7b748724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- seller_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- buyer_id: integer (nullable = true)\n",
      " |-- sale_date: date (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "9b17610f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- productid: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- unit_price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "ba949ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = sales_df.join(product_df,sales_df.product_id == product_df.productid,'inner')\\\n",
    "            .select('product_id','product_name','sale_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "28cd3677",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = sales.filter( (col('sale_date') >= lit('2019-04-01')) | (col('sale_date') <= lit('2018-12-31') ) )\\\n",
    "    .select('product_id')\n",
    "out = [ date_range.collect()[i][0] for i in range(0,len(date_range.collect()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "cdf953ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|product_id|product_name|\n",
      "+----------+------------+\n",
      "|         1|          S8|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.filter(~col('product_id').isin(out)).select('product_id','product_name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "eaafd728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|product_id|product_name|\n",
      "+----------+------------+\n",
      "|         1|          S8|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.createOrReplaceTempView('salesdf')\n",
    "product_df.createOrReplaceTempView('productdf')\n",
    "\n",
    "spark.sql('with sales as (select product_id,product_name,sale_date from salesdf \\\n",
    "                                join productdf on salesdf.product_id == productdf.productid),\\\n",
    "           date_range as (select product_id from sales where sale_date >= \"2019-04-01\" or sale_date <= \"2018-12-31\")\\\n",
    "                          select product_id,product_name from sales where product_id not in (select * from date_range)\\\n",
    "                              ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f5e732",
   "metadata": {},
   "source": [
    "#### 30 66 Reported Posts E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c3e6e32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_schema = StructType([\n",
    "                     StructField('user_id',IntegerType()),\n",
    "                     StructField('post_id',IntegerType()),\n",
    "                     StructField('action_date',DateType()),\n",
    "                     StructField('action',StringType()),\n",
    "                     StructField('extra',StringType()),\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1925a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .schema(action_schema)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/30_action.csv')\n",
    "                 .withColumn('action_date',to_date('action_date','MM-dd-yyyy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b69fd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- post_id: integer (nullable = true)\n",
      " |-- action_date: date (nullable = true)\n",
      " |-- action: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "action_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8353cbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "| extra|report_count|\n",
      "+------+------------+\n",
      "|  spam|           2|\n",
      "|racism|           2|\n",
      "+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "action_df.filter( (col('action_date') == lit('2019-07-04')) & (col('action') == lit('report')))\\\n",
    "        .groupBy('extra').agg(count(col('extra')).alias('report_count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "788a28de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "| extra|report_couunt|\n",
      "+------+-------------+\n",
      "|  spam|            2|\n",
      "|racism|            2|\n",
      "+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "action_df.createOrReplaceTempView('actiondf')\n",
    "\n",
    "spark.sql('select extra,count(extra) as report_couunt from actiondf \\\n",
    "                where action_date == \"2019-07-04\" and action == \"report\" \\\n",
    "                group by extra').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9d54fe",
   "metadata": {},
   "source": [
    "#### 31 70 User Activity for the Past 30 Days I E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "78b6ddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_schema = StructType([\n",
    "                     StructField('user_id',IntegerType()),\n",
    "                     StructField('session_id',IntegerType()),\n",
    "                     StructField('activity_date',DateType()),\n",
    "                     StructField('activity_type',StringType()),\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "98d2c9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .schema(action_schema)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/31_activity.csv')\n",
    "                 .withColumn('action_date',to_date('action_date','MM-dd-yyyy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "31e923f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- post_id: integer (nullable = true)\n",
      " |-- action_date: date (nullable = true)\n",
      " |-- action: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07f979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_df.filter( (col('action_date') <= lit('2019-07-27')) & \\\n",
    "                       (col('action_date') >= date_add(lit('2019-07-27'), -30) ))\\\n",
    "                        .groupby('action_date').agg(countDistinct(col('user_id')).alias('active_users')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d438552",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_df.createOrReplaceTempView('activitydf')\n",
    "\n",
    "spark.sql('select action_date, count(distinct user_id) active_users from activitydf \\\n",
    "                where action_date between date_add(\"2019-07-27\",-30) and \"2019-07-27\" \\\n",
    "                 group by action_date').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ee03e0",
   "metadata": {},
   "source": [
    "#### 32 71 User Activity for the Past 30 Days II E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "003ad438",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity32_schema = StructType([\n",
    "                     StructField('user_id',IntegerType()),\n",
    "                     StructField('session_id',IntegerType()),\n",
    "                     StructField('activity_date',DateType()),\n",
    "                     StructField('activity_type',StringType()),\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1c91f4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity32_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .schema(activity32_schema)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/32_activity.csv')\n",
    "                 .withColumn('activity_date',to_date('activity_date','MM-dd-yyyy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "139bfdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- session_id: integer (nullable = true)\n",
      " |-- activity_date: date (nullable = true)\n",
      " |-- activity_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity32_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8d9426cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|average_sessions_per_user|\n",
      "+-------------------------+\n",
      "|                     1.33|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity32_df.filter( (col('activity_date') <= lit('2019-07-27')) & \\\n",
    "                       (col('activity_date') >= date_add(lit('2019-07-27'), -30) )) \\\n",
    "                      .filter(col('activity_type') == lit('open_session'))\\\n",
    "                      .groupBy('user_id').agg(count(col('user_id')).alias('cnt'))\\\n",
    "                      .select(round(sum(col('cnt'))/count('user_id'),2).alias('average_sessions_per_user')).show()\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "624ee78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|average_sessions_per_user|\n",
      "+-------------------------+\n",
      "|                     1.33|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity32_df.createOrReplaceTempView('activity32df')\n",
    "\n",
    "spark.sql('with one as (select user_id,count(user_id) as cnt from activity32df where activity_date between \\\n",
    "                        date_add(\"2019-07-27\",-30) and \"2019-07-27\" and activity_type= \"open_session\" group by user_id) \\\n",
    "                    select round(sum(cnt)/count(user_id),2) as average_sessions_per_user from one \\\n",
    "            ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0c0cac",
   "metadata": {},
   "source": [
    "#### 33 72  Article Views I E "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "231d8029",
   "metadata": {},
   "outputs": [],
   "source": [
    "views_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/33_views.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd4a62d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- article_id: integer (nullable = true)\n",
      " |-- author_id: integer (nullable = true)\n",
      " |-- viewer_id: integer (nullable = true)\n",
      " |-- view_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "views_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2471096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  4|\n",
      "|  7|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "views_df.filter(col('author_id') == col('viewer_id') ).select(col('author_id').alias('id')).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0f7480a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|author_id|\n",
      "+---------+\n",
      "|        4|\n",
      "|        7|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "views_df.createOrReplaceTempView('viewsdf')\n",
    "\n",
    "spark.sql('''\n",
    "           select distinct author_id from viewsdf where author_id == viewer_id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8eeb01dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'4', '7'}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Python\n",
    "x = open('../../python/15_Maximumarray.txt','r')\n",
    "y=x.readlines()\n",
    "x.close()\n",
    "z =[]\n",
    "for i in y[0].split(' ')[0:]:\n",
    "'''\n",
    "\n",
    "x = open('../../data/database/33_views.csv','r')\n",
    "y = x.readlines()\n",
    "x.close()\n",
    "\n",
    "lst = []\n",
    "for i in y[1:]:\n",
    "    if i.split(',')[1] == i.split(',')[2]:\n",
    "        lst.append(i.split(',')[1])\n",
    "print(set(lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5229df",
   "metadata": {},
   "source": [
    "#### 34 73 Article Views II M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d72c3000",
   "metadata": {},
   "outputs": [],
   "source": [
    "views_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/34_views.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "308bceac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- article_id: integer (nullable = true)\n",
      " |-- author_id: integer (nullable = true)\n",
      " |-- viewer_id: integer (nullable = true)\n",
      " |-- view_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "views_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "50ccb876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  5|\n",
      "|  6|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('view_date','viewer_id').orderBy('article_id')\n",
    "views_df.withColumn('rnk',rank().over(window_spec)).filter(col('rnk') == 2).select(col('viewer_id').alias('id')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0003c1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  5|\n",
      "|  6|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "views_df.createOrReplaceTempView('viewsdf')\n",
    "\n",
    "spark.sql('''\n",
    "           select id from \n",
    "           (select viewer_id as id, rank() OVER(PARTITION BY view_date,viewer_id ORDER BY article_id) as rnk \n",
    "                from viewsdf) where rnk = 2\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617ec0e8",
   "metadata": {},
   "source": [
    "#### 35 74 Market Analysis I M "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9f45e774",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/35_users.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e10ae098",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/35_orders.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cb3fc2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/35_items.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "00c7c2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- join_date: string (nullable = true)\n",
      " |-- favorite_brand: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "93ed9300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- buyer_id: integer (nullable = true)\n",
      " |-- seller_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b8164040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- item_brand: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "71f323c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------+\n",
      "|user_id| join_date|orders_in_2019|\n",
      "+-------+----------+--------------+\n",
      "|      1|2018-01-01|             1|\n",
      "|      2|2018-02-09|             2|\n",
      "|      3|2018-01-19|             0|\n",
      "|      4|2018-05-21|             0|\n",
      "+-------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.filter( year(to_date('order_date','yyyy-MM-dd')) == 2019)\\\n",
    ".groupBy('buyer_id').agg(count(col('buyer_id')).alias('orders_in_2019'))\\\n",
    ".join(users_df,orders_df.buyer_id == users_df.user_id,'right')\\\n",
    ".select('user_id','join_date',coalesce(col('orders_in_2019'),lit('0')).alias('orders_in_2019') ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e2fd34e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------+\n",
      "|user_id| join_date|orders_in_2019|\n",
      "+-------+----------+--------------+\n",
      "|      1|2018-01-01|             1|\n",
      "|      2|2018-02-09|             2|\n",
      "|      3|2018-01-19|             0|\n",
      "|      4|2018-05-21|             0|\n",
      "+-------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.createOrReplaceTempView('usersdf')\n",
    "orders_df.createOrReplaceTempView('ordersdf')\n",
    "\n",
    "spark.sql('''\n",
    "            select user_id,join_date,coalesce(orders_in_2019,0) as orders_in_2019 from usersdf left join\n",
    "            (select buyer_id,count(buyer_id) as orders_in_2019 from ordersdf \n",
    "            where year(to_date(order_date,\"yyyy-MM-dd\")) == 2019 group by buyer_id) on user_id == buyer_id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9524673",
   "metadata": {},
   "source": [
    "#### 36 77 mmediate Food Delivery I E "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ad22e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "delivery_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/36_delivery.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ef1a4421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- delivery_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_pref_delivery_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delivery_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "76588d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|immediate_percentage|\n",
      "+--------------------+\n",
      "|               33.33|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delivery_df.filter(col('order_date') == col('customer_pref_delivery_date'))\\\n",
    ".agg( (round(count(col('delivery_id'))/delivery_df.agg(count(col('delivery_id')))\\\n",
    ".collect()[0][0],4) *100).alias('immediate_percentage') ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "50c1e3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|immediate_percentage|\n",
      "+--------------------+\n",
      "|               33.33|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delivery_df.createOrReplaceTempView('deliverydf')\n",
    "\n",
    "spark.sql('''\n",
    "  with main as (select count(delivery_id) as equ_cnt from deliverydf \n",
    "                     where order_date == customer_pref_delivery_date),\n",
    "        one as (select  count(delivery_id) as cnt from deliverydf)\n",
    "                 select round(main.equ_cnt / one.cnt * 100,2) as immediate_percentage from main,one\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d68c532",
   "metadata": {},
   "source": [
    "#### 37 79 Reformat Department Table E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3f143659",
   "metadata": {},
   "outputs": [],
   "source": [
    "department_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/37_department.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f815c830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- revenue: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "department_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a0a859d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+----+----+\n",
      "| id|  Feb| Jan| Mar| Apr|\n",
      "+---+-----+----+----+----+\n",
      "|  1| 7000|8000|6000|null|\n",
      "|  3|10000|null|null|null|\n",
      "|  2| null|9000|null|null|\n",
      "+---+-----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivot_df = department_df.groupBy(\"id\").pivot(\"month\").agg(max(\"revenue\"))\n",
    "pivot_df.withColumn('Apr',lit('null')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb22005",
   "metadata": {},
   "outputs": [],
   "source": [
    "department_df.groupBy(\"id\").pivot(\"month\").agg(max(\"revenue\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "9c1db3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+-----+\n",
      "| id|month|revenue|  jan|\n",
      "+---+-----+-------+-----+\n",
      "|  1|  Jan|   8000| 8000|\n",
      "|  2|  Jan|   9000| 9000|\n",
      "|  3|  Feb|  10000|10000|\n",
      "|  1|  Feb|   7000| 7000|\n",
      "|  1|  Mar|   6000| 6000|\n",
      "+---+-----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "department_df.withColumn('jan',when( (col('month') == lit('Jan')),col('revenue') ) \\\n",
    ".when( (col('month') == lit('Feb')),col('revenue') ) \\\n",
    ".when( (col('month') == lit('Mar')),col('revenue') ) \\\n",
    ".when( (col('month') == lit('Apr')),col('revenue') ) \n",
    ".when( (col('month') == lit('Apr')),col('revenue') )                         )\\\n",
    ".select('id','month','revenue','jan').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8ad5fde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_df = department_df.withColumn('Jan_rev',when( (col('month') == lit('Jan')),col('revenue') )) \\\n",
    ".groupBy(col('id')).agg(max(col('Jan_rev')).alias('Jan_rev'))\\\n",
    ".select('id','Jan_rev',lit('').alias('Feb_rev'),lit('').alias('Mar_rev'),lit('').alias('Apr_rev')\n",
    "       ,lit('').alias('May_rev'),lit('').alias('Jun_rev'),lit('').alias('Jul_rev'),lit('').alias('Aug_rev')\n",
    "       ,lit('').alias('Sep_rev'),lit('').alias('Oct_rev'),lit('').alias('Nov_rev'),lit('').alias('Dec_rev')   \n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2b225264",
   "metadata": {},
   "outputs": [],
   "source": [
    "feb_df = department_df.withColumn('Feb_rev',when( (col('month') == lit('Feb')),col('revenue') )) \\\n",
    ".groupBy(col('id')).agg(max(col('Feb_rev')).alias('Feb_rev'))\\\n",
    ".select('id',lit('').alias('Jan_rev'),'Feb_rev',lit('').alias('Mar_rev'),lit('').alias('Apr_rev') \n",
    "       ,lit('').alias('May_rev'),lit('').alias('Jun_rev'),lit('').alias('Jul_rev'),lit('').alias('Aug_rev') \n",
    "       ,lit('').alias('Sep_rev'),lit('').alias('Oct_rev'),lit('').alias('Nov_rev'),lit('').alias('Dec_rev')  \n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "02135423",
   "metadata": {},
   "outputs": [],
   "source": [
    "mar_df = department_df.withColumn('Mar_rev',when( (col('month') == lit('Mar')),col('revenue') )) \\\n",
    ".groupBy(col('id')).agg(max(col('Mar_rev')).alias('Mar_rev'))\\\n",
    ".select('id',lit('').alias('Jan_rev'),lit('').alias('Feb_rev'),'Mar_rev',lit('').alias('Apr_rev')\n",
    "       ,lit('').alias('May_rev'),lit('').alias('Jun_rev'),lit('').alias('Jul_rev'),lit('').alias('Aug_rev')\n",
    "       ,lit('').alias('Sep_rev'),lit('').alias('Oct_rev'),lit('').alias('Nov_rev'),lit('').alias('Dec_rev')    \n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0ff65609",
   "metadata": {},
   "outputs": [],
   "source": [
    "apr_df = department_df.withColumn('Apr_rev',when( (col('month') == lit('Apr')),col('revenue') )) \\\n",
    ".groupBy(col('id')).agg(max(col('Apr_rev')).alias('Apr_rev'))\\\n",
    ".select('id',lit('').alias('Jan_rev'),lit('').alias('Feb_rev'),lit('').alias('Mar_rev'),'Apr_rev'\n",
    "       ,lit('').alias('May_rev'),lit('').alias('Jun_rev'),lit('').alias('Jul_rev'),lit('').alias('Aug_rev')\n",
    "       ,lit('').alias('Sep_rev'),lit('').alias('Oct_rev'),lit('').alias('Nov_rev'),lit('').alias('Dec_rev') \n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "caf9a36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "| id|Jan_rev|Feb_rev|Mar_rev|Apr_rev|May_rev|Jun_rev|Jul_rev|Aug_rev|Sep_rev|Oct_rev|Nov_rev|Dec_rev|\n",
      "+---+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "|  1|   8000|   7000|   6000|   null|   null|   null|   null|   null|   null|   null|   null|   null|\n",
      "|  3|   null|  10000|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|\n",
      "|  2|   9000|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|\n",
      "+---+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jan_df\\\n",
    ".union(feb_df)\\\n",
    ".union(mar_df)\\\n",
    ".union(apr_df)\\\n",
    ".groupBy('id').agg(sum(col('Jan_rev')).cast('int').alias('Jan_rev'), \\\n",
    "                   sum(col('Feb_rev')).cast('int').alias('Feb_rev'), \\\n",
    "                   sum(col('Mar_rev')).cast('int').alias('Mar_rev'), \\\n",
    "                   sum(col('Apr_rev')).cast('int').alias('Apr_rev'), \\\n",
    "                   sum(col('May_rev')).cast('int').alias('May_rev'), \\\n",
    "                   sum(col('Jun_rev')).cast('int').alias('Jun_rev'), \\\n",
    "                   sum(col('Jul_rev')).cast('int').alias('Jul_rev'), \\\n",
    "                   sum(col('Aug_rev')).cast('int').alias('Aug_rev'), \\\n",
    "                   sum(col('Sep_rev')).cast('int').alias('Sep_rev'), \\\n",
    "                   sum(col('Oct_rev')).cast('int').alias('Oct_rev'), \\\n",
    "                   sum(col('Nov_rev')).cast('int').alias('Nov_rev'), \\\n",
    "                   sum(col('Dec_rev')).cast('int').alias('Dec_rev'), \\\n",
    "                  ).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1918dd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "| id|Jan_rev|Feb_rev|Mar_rev|Apr_rev|May_rev|Jun_rev|Jul_rev|Aug_rev|Sep_rev|Oct_rev|Nov_rev|Dec_rev|\n",
      "+---+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "|  1|   8000|   7000|   6000|   null|   null|   null|   null|   null|   null|   null|   null|   null|\n",
      "|  3|   null|  10000|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|\n",
      "|  2|   9000|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|\n",
      "+---+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "department_df.withColumn('Jan_rev',when( (col('month') == lit('Jan')),col('revenue') ))\\\n",
    ".withColumn('Feb_rev',when( (col('month') == lit('Feb')),col('revenue') ))\\\n",
    ".withColumn('Mar_rev',when( (col('month') == lit('Mar')),col('revenue') ))\\\n",
    ".withColumn('Apr_rev',when( (col('month') == lit('Apr')),col('revenue') ))\\\n",
    ".select('id','Jan_rev','Feb_rev','Mar_rev','Apr_rev',lit('').alias('May_rev'),lit('').alias('Jun_rev'),lit('').alias('Jul_rev'),lit('').alias('Aug_rev')\n",
    "       ,lit('').alias('Sep_rev'),lit('').alias('Oct_rev'),lit('').alias('Nov_rev'),lit('').alias('Dec_rev') \n",
    "       ).groupBy('id').agg(sum(col('Jan_rev')).cast('int').alias('Jan_rev'), \\\n",
    "                   sum(col('Feb_rev')).cast('int').alias('Feb_rev'), \\\n",
    "                   sum(col('Mar_rev')).cast('int').alias('Mar_rev'), \\\n",
    "                   sum(col('Apr_rev')).cast('int').alias('Apr_rev'), \\\n",
    "                   sum(col('May_rev')).cast('int').alias('May_rev'), \\\n",
    "                   sum(col('Jun_rev')).cast('int').alias('Jun_rev'), \\\n",
    "                   sum(col('Jul_rev')).cast('int').alias('Jul_rev'), \\\n",
    "                   sum(col('Aug_rev')).cast('int').alias('Aug_rev'), \\\n",
    "                   sum(col('Sep_rev')).cast('int').alias('Sep_rev'), \\\n",
    "                   sum(col('Oct_rev')).cast('int').alias('Oct_rev'), \\\n",
    "                   sum(col('Nov_rev')).cast('int').alias('Nov_rev'), \\\n",
    "                   sum(col('Dec_rev')).cast('int').alias('Dec_rev'), \\\n",
    "                  ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "7c563c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "| id|Jan_rev|Feb_rev|Mar_rev|Apr_rev|May_rev|Jun_rev|Jul_rev|Aug_rev|Sep_rev|Oct_rev|Nov_rev|Dec_rev|\n",
      "+---+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "|  1|   8000|   7000|   6000|   null|   null|   null|   null|   null|   null|   null|   null|   null|\n",
      "|  2|   9000|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|\n",
      "|  3|   null|  10000|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|\n",
      "+---+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "department_df.createOrReplaceTempView('departmentdf')\n",
    "\n",
    "spark.sql('''\n",
    "          select id, Min(case when month == \"Jan\" then revenue end) Jan_rev,\n",
    "                     Min(case when month == \"Feb\" then revenue end) Feb_rev,\n",
    "                     Min(case when month == \"Mar\" then revenue end) Mar_rev,\n",
    "                     Min(case when month == \"Apr\" then revenue end) Apr_rev, \n",
    "                     Min(case when month == \"May\" then revenue end) May_rev,\n",
    "                     Min(case when month == \"Jun\" then revenue end) Jun_rev,\n",
    "                     Min(case when month == \"Jul\" then revenue end) Jul_rev,\n",
    "                     Min(case when month == \"Aug\" then revenue end) Aug_rev,\n",
    "                     Min(case when month == \"Sep\" then revenue end) Sep_rev,\n",
    "                     Min(case when month == \"Oct\" then revenue end) Oct_rev,\n",
    "                     Min(case when month == \"Nov\" then revenue end) Nov_rev,\n",
    "                     Min(case when month == \"Dec\" then revenue end) Dec_rev\n",
    "                     from departmentdf group by id order by id\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5021a2b",
   "metadata": {},
   "source": [
    "#### 38 84 Queries Quality and Percentage E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dc340c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/38_queries.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "988bbc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- query_name: string (nullable = true)\n",
      " |-- result: string (nullable = true)\n",
      " |-- position: integer (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queries_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7801e2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------------------+\n",
      "|query_name|quality|poor_query_percentage|\n",
      "+----------+-------+---------------------+\n",
      "|       Cat|   0.66|                 0.33|\n",
      "|       Dog|    2.5|                 0.33|\n",
      "+----------+-------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queries_df.withColumn('poor_query_percentage',round(count(when(col('rating') < 3,col('rating') )).over(window_spec) \n",
    "                     / count('rating').over(window_spec),2 ))\\\n",
    "          .withColumn('quality', round(sum(col('rating')/col('position')).over(window_spec)\n",
    "                      / count(col('query_name')).over(window_spec),2) ) \\\n",
    ".select('query_name','quality','poor_query_percentage').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "32b9a559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------------------+\n",
      "|query_name|quality|poor_query_percentage|\n",
      "+----------+-------+---------------------+\n",
      "|       Cat|   0.66|                 0.33|\n",
      "|       Dog|    2.5|                 0.33|\n",
      "+----------+-------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queries_df.createOrReplaceTempView('queriesdf')\n",
    "\n",
    "spark.sql('''\n",
    "          select query_name, round(sum(rating/position)/count(rating),2) as quality,\n",
    "         round((count(case when rating < 3 then rating  end ) / count(rating)),2) as poor_query_percentage\n",
    "          from queriesdf group by query_name\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcfc84b",
   "metadata": {},
   "source": [
    "#### 39 87 Number of Comments per Post E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7c0b3841",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/39_submissions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "61f496a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sub_id: integer (nullable = true)\n",
      " |-- parent_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "submissions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cb4b60d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|ids|number_of_comments|\n",
      "+---+------------------+\n",
      "|  1|                 3|\n",
      "|  2|                 2|\n",
      "| 12|                 0|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = submissions_df.withColumn('ids',when(col('parent_id').isNull(),col('sub_id'))).select('ids')\n",
    "x.join(submissions_df,col('ids') == col('parent_id'),'left').distinct()\\\n",
    ".groupBy(col('ids')).agg(count(col('parent_id')).alias('number_of_comments')).filter(~col('ids').isNull())\\\n",
    ".select('ids','number_of_comments').orderBy('ids').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ef57d376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|number_of_comments|\n",
      "+---+------------------+\n",
      "|  1|                 8|\n",
      "|  2|                 2|\n",
      "| 12|                 0|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "submissions_df.createOrReplaceTempView('submissionsdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with main as (select sub_id  as id from submissionsdf where parent_id is null)\n",
    "               select id,count(parent_id) as number_of_comments from submissionsdf \n",
    "                     right join main on id == parent_id group by id order by id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0fa859",
   "metadata": {},
   "source": [
    "#### 40 88 Average Selling Price E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "94f981ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/40_prices.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d150ba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unitsold_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/40_unitsold.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e8d156f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      " |-- end_date: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prices_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "292642e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- purchase_date: string (nullable = true)\n",
      " |-- units: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unitsold_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0b92d737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|prdid|average_price|\n",
      "+-----+-------------+\n",
      "|    1|         6.96|\n",
      "|    2|        16.96|\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prices_df.withColumnRenamed('product_id','prdid').join(unitsold_df,(col('prdid')== unitsold_df.product_id) & \n",
    "               (unitsold_df.purchase_date.between(prices_df.start_date,prices_df.end_date)),'inner')\\\n",
    ".select('prdid', (col('price') * col('units') ).alias('salsprice'),'units' ).groupBy('prdid')\\\n",
    ".agg(round(sum(col('salsprice'))/sum('units'),2 ).alias('average_price')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "890ac1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|product_id|average_price|\n",
      "+----------+-------------+\n",
      "|         1|         6.96|\n",
      "|         2|        16.96|\n",
      "+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prices_df.createOrReplaceTempView('pricesdf')\n",
    "unitsold_df.createOrReplaceTempView('unitsolddf')\n",
    "\n",
    "spark.sql('''\n",
    " select pricesdf.product_id as product_id, round(sum(price * units)/sum(units),2) as average_price from pricesdf  \n",
    "        join unitsolddf on unitsolddf.product_id == pricesdf.product_id \n",
    "        and purchase_date between start_date and end_date\n",
    "        group by pricesdf.product_id\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd27628b",
   "metadata": {},
   "source": [
    "#### 41 91 Students and Examinations E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "88581588",
   "metadata": {},
   "outputs": [],
   "source": [
    "students_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/41_students.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "00a3fd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/41_subjects.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4c4bcd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "examinations_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/41_examinations.csv')\n",
    "                 .withColumnRenamed('student_id','studentid')\n",
    "                 .withColumnRenamed('subject_name','subjectname') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "8c7427fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- student_id: integer (nullable = true)\n",
      " |-- student_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "9794a3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- subject_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subjects_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "9700eebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- studentid: integer (nullable = true)\n",
      " |-- subjectname: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examinations_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "85a5c07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+--------------+\n",
      "|student_id|student_name|subject_name|attended_exams|\n",
      "+----------+------------+------------+--------------+\n",
      "|         1|       Alice|        Math|             3|\n",
      "|         1|       Alice| Programming|             1|\n",
      "|         1|       Alice|     Physics|             2|\n",
      "|         2|         Bob|     Physics|             0|\n",
      "|         2|         Bob| Programming|             1|\n",
      "|         2|         Bob|        Math|             1|\n",
      "|         6|        Alex| Programming|             0|\n",
      "|         6|        Alex|     Physics|             0|\n",
      "|         6|        Alex|        Math|             0|\n",
      "|        13|        John|        Math|             1|\n",
      "|        13|        John| Programming|             1|\n",
      "|        13|        John|     Physics|             1|\n",
      "+----------+------------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students_df.join(subjects_df)\\\n",
    "           .join(examinations_df,(col('student_id') == col('studentid')) &\n",
    "            (col('subject_name') == col('subjectname')) ,'left')\\\n",
    "            .groupBy('student_id','student_name','subject_name')\\\n",
    "            .agg(count(col('studentid')).alias('attended_exams')).orderBy('student_id').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "f3bab301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+--------------+\n",
      "|student_id|student_name|subject_name|attended_exams|\n",
      "+----------+------------+------------+--------------+\n",
      "|         1|       Alice| Programming|             1|\n",
      "|         1|       Alice|     Physics|             2|\n",
      "|         1|       Alice|        Math|             3|\n",
      "|         2|         Bob|     Physics|             0|\n",
      "|         2|         Bob| Programming|             1|\n",
      "|         2|         Bob|        Math|             1|\n",
      "|         6|        Alex|        Math|             0|\n",
      "|         6|        Alex| Programming|             0|\n",
      "|         6|        Alex|     Physics|             0|\n",
      "|        13|        John|        Math|             1|\n",
      "|        13|        John| Programming|             1|\n",
      "|        13|        John|     Physics|             1|\n",
      "+----------+------------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students_df.createOrReplaceTempView('studentsdf')\n",
    "subjects_df.createOrReplaceTempView('subjects_df')\n",
    "examinations_df.createOrReplaceTempView('examinations_df')\n",
    "\n",
    "spark.sql('''\n",
    "          select student_id,student_name,subject_name,count(subjectname) as attended_exams from studentsdf \n",
    "          left join subjects_df \n",
    "          left join examinations_df on student_id = studentid and subject_name = subjectname\n",
    "          group by student_id,student_name,subject_name order by student_id\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dbe1fe",
   "metadata": {},
   "source": [
    "#### 42 93 Weather Type in Each Country E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4c24963f",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/42_countries.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0f2c9be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/42_weather.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "19f9949c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country_id: integer (nullable = true)\n",
      " |-- country_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "22eb0b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country_id: integer (nullable = true)\n",
      " |-- weather_state: integer (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "cb3de52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 841:==============================================>      (174 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|country_name|weather_type|\n",
      "+------------+------------+\n",
      "|        Peru|         Hot|\n",
      "|     Morocco|         Hot|\n",
      "|   Australia|        cold|\n",
      "|         USA|        cold|\n",
      "|       China|        warm|\n",
      "+------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('country_id')\n",
    "weather_df.filter( (year(col('day')) == lit('2019')) & (month(col('day')) == lit('11'))  )\\\n",
    ".withColumn('weather_avg',round(sum('weather_state').over(window_spec)/count('country_id').over(window_spec),2) )\\\n",
    ".withColumn('weather_type',when( col('weather_avg') <= 15, lit('cold'))\n",
    "                          .when( col('weather_avg') >= 25, lit('Hot')).otherwise('warm'))\\\n",
    ".join(countries_df, on = 'country_id')\\\n",
    ".select('country_name','weather_type').distinct().orderBy('weather_type').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "17b31a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|country_name|weather_type|\n",
      "+------------+------------+\n",
      "|   Australia|        Cold|\n",
      "|         USA|        Cold|\n",
      "|     Morocco|         Hot|\n",
      "|        Peru|         Hot|\n",
      "|       China|        Warm|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries_df.createOrReplaceTempView('countriesdf')\n",
    "weather_df.createOrReplaceTempView('weatherdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with main as (select country_id, round(sum(weather_state)/count(country_id),2) as weather_avg \n",
    "                     from weatherdf where year(day) = 2019 and month(day) = 11 group by country_id)\n",
    "               select country_name, case when weather_avg <= 15 then \"Cold\"\n",
    "                                          when weather_avg >= 25 then \"Hot\"\n",
    "                                          else \"Warm\" end as weather_type\n",
    "                      from main join countriesdf on main.country_id == countriesdf.country_id order by weather_type\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052c9c5b",
   "metadata": {},
   "source": [
    "#### 43 94 Find the Team Size E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2bdbaca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/43_employee.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "2275376e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- team_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "cc106851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "|employee_id|team_id|\n",
      "+-----------+-------+\n",
      "|          1|      8|\n",
      "|          2|      8|\n",
      "|          3|      8|\n",
      "|          4|      7|\n",
      "|          5|      9|\n",
      "|          6|      9|\n",
      "+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "0843cb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n",
      "|employee_id|team_size|\n",
      "+-----------+---------+\n",
      "|          1|        3|\n",
      "|          2|        3|\n",
      "|          3|        3|\n",
      "|          4|        1|\n",
      "|          5|        2|\n",
      "|          6|        2|\n",
      "+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('team_id')\n",
    "employee_df.withColumn('team_size',count('team_id').over(window_spec))\\\n",
    ".select('employee_id','team_size').orderBy('employee_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "97633952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n",
      "|employee_id|team_size|\n",
      "+-----------+---------+\n",
      "|          1|        3|\n",
      "|          2|        3|\n",
      "|          3|        3|\n",
      "|          4|        1|\n",
      "|          5|        2|\n",
      "|          6|        2|\n",
      "+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.createOrReplaceTempView('employeedf')\n",
    "\n",
    "spark.sql('''\n",
    "        select employee_id,count(team_id) over(partition by team_id) as  team_size from employeedf order by employee_id\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b81489",
   "metadata": {},
   "source": [
    "#### 44 97 Ads Performance E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "60ad3ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ads_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/44_ads.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "3bb31aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ad_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- action: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ads_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39b187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ad_id = 1, ctr = (2/(2+1)) * 100 = 66.67\n",
    "for ad_id = 2, ctr = (1/(1+2)) * 100 = 33.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "118099e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|ad_id|  ctr|\n",
      "+-----+-----+\n",
      "|    1|66.67|\n",
      "|    3| 50.0|\n",
      "|    2|33.33|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('ad_id')\n",
    "windowspec = Window.partitionBy('ad_id','action')\n",
    "\n",
    "ads_df.filter(col('action') != lit('Ignored'))\\\n",
    ".withColumn('ctr',(when(col('action') == lit('Clicked'),count('action').over(windowspec) )\n",
    "                 / when(col('action') != lit('Ignored'),count('action').over(window_spec)))) \\\n",
    ".filter(~col('ctr').isNull()).select('ad_id',(round(col('ctr')*100,2) ).alias('ctr') ).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "a85250bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|ad_id|  ctr|\n",
      "+-----+-----+\n",
      "|    1| 50.0|\n",
      "|    3| 50.0|\n",
      "|    2|33.33|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ads_df.createOrReplaceTempView('adsdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as(select distinct ad_id,action ,case when action == \"Clicked\" then count(action) over(partition by ad_id,action) end as up,\n",
    "                 case when action != 'Ignored' then count(action) over(partition by ad_id) end as div from adsdf)\n",
    "    select ad_id,round((up/div)*100,2) as ctr from main where up is not null\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb18bf3",
   "metadata": {},
   "source": [
    "#### 45 98 List the Products Ordered in a Period E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7b200896",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/45_products.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "43b75d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/45_orders.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "78bfdd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "fad6428a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- unit: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "1c38e176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----+\n",
      "|      product_name|unit|\n",
      "+------------------+----+\n",
      "|Leetcode Solutions| 130|\n",
      "|      Leetcode Kit| 100|\n",
      "+------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.filter( (year(col('order_date')) == lit('2020')) & (month(col('order_date')) == lit('02') )  ) \\\n",
    ".groupBy('product_id').agg(sum(col('unit')).alias('unit')).filter(col('unit') >= 100)\\\n",
    ".join(products_df, on = 'product_id').select('product_name','unit').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "b1455516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----+\n",
      "|      product_name|unit|\n",
      "+------------------+----+\n",
      "|Leetcode Solutions| 130|\n",
      "|      Leetcode Kit| 100|\n",
      "+------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.createOrReplaceTempView('productsdf')\n",
    "orders_df.createOrReplaceTempView('ordersdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select product_id,sum(unit) as unit from ordersdf where \n",
    "                      year(order_date) == \"2020\" and month(order_date) == \"02\" group by product_id)\n",
    "               select product_name,unit from main join productsdf on productsdf.product_id == main.product_id\n",
    "                      where unit >= 100\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2fcfb6",
   "metadata": {},
   "source": [
    "#### 46 101 Students With Invalid Departments E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4ffc7087",
   "metadata": {},
   "outputs": [],
   "source": [
    "departments_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/46_departments.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2fc7c5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "students_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/46_students.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b696585f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departments_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f5fed4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- deepartment_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a19aced4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  2|   John|\n",
      "|  3|  Steve|\n",
      "|  4|Jasmine|\n",
      "|  7| Daiana|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students_df.join(departments_df,students_df.deepartment_id == departments_df.id,'leftanti')\\\n",
    ".select('id','name').orderBy('id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d53682ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  2|   John|\n",
      "|  3|  Steve|\n",
      "|  4|Jasmine|\n",
      "|  7| Daiana|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departments_df.createOrReplaceTempView('departmentsdf')\n",
    "students_df.createOrReplaceTempView('studentsdf')\n",
    "\n",
    "spark.sql('''\n",
    "             select studentsdf.id as id,studentsdf.name as name from studentsdf left anti join\n",
    "                   departmentsdf on studentsdf.deepartment_id = departmentsdf.id order by id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b647ba2",
   "metadata": {},
   "source": [
    "#### 47 105 Replace Employee ID With The Unique Identifier E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5f4f35d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/47_employee.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "dff0dfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/47_employees.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5597d158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- unique_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68535d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e61e421d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|unique_id|    name|\n",
      "+---------+--------+\n",
      "|     null|   Alice|\n",
      "|     null|     Bob|\n",
      "|        2|    Meir|\n",
      "|        3| Winston|\n",
      "|        1|Jonathan|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.join(employee_df,employees_df.id == employee_df.id,'left' )\\\n",
    ".select('unique_id','name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c23e505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|unique_id|    name|\n",
      "+---------+--------+\n",
      "|     null|   Alice|\n",
      "|     null|     Bob|\n",
      "|        2|    Meir|\n",
      "|        3| Winston|\n",
      "|        1|Jonathan|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.createOrReplaceTempView('employeedf')\n",
    "employees_df.createOrReplaceTempView('employeesdf')\n",
    "\n",
    "spark.sql('''\n",
    "          select unique_id,name from employeedf right outer join employeesdf on employeedf.id == employeesdf.id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0e1c41",
   "metadata": {},
   "source": [
    "#### 48 109 Top Travellers E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5f2357e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/48_users.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2509ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/48_rides.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4e81e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24df1d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rides_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca4720d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|    name|distance|\n",
      "+--------+--------+\n",
      "|   Elvis|     450|\n",
      "|     Lee|     450|\n",
      "|     Bob|     317|\n",
      "|Jonathan|     312|\n",
      "|    Alex|     222|\n",
      "|   Alice|     120|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.join(rides_df,users_df.id == rides_df.user_id,'inner').select('name','distance').groupBy('name')\\\n",
    ".agg(sum(col('distance')).alias('distance')).orderBy(col('distance').desc(),col('name').asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5336d455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|    name|distance|\n",
      "+--------+--------+\n",
      "|   Elvis|     450|\n",
      "|     Lee|     450|\n",
      "|     Bob|     317|\n",
      "|Jonathan|     312|\n",
      "|    Alex|     222|\n",
      "|   Alice|     120|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.createOrReplaceTempView('usersdf')\n",
    "rides_df.createOrReplaceTempView('ridesdf')\n",
    "\n",
    "spark.sql('''\n",
    "        select name,sum(distance) as distance from usersdf join ridesdf on usersdf.id ==  ridesdf.user_id\n",
    "               group by name order by distance desc, name asc\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3600e4d",
   "metadata": {},
   "source": [
    "####  49 111 NPV Queries E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d78e4833",
   "metadata": {},
   "outputs": [],
   "source": [
    "npv_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/49_npv.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b2d13415",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/49_queries.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fdda2cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- npv: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "npv_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de083258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queries_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "51ae5c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n",
      "| id|year|npv|\n",
      "+---+----+---+\n",
      "|  1|2019|113|\n",
      "|  2|2008|121|\n",
      "|  3|2009| 12|\n",
      "|  7|2019|  0|\n",
      "|  7|2018|  0|\n",
      "|  7|2020| 30|\n",
      "| 13|2019| 40|\n",
      "+---+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "npv_df.join(queries_df, (npv_df.id == queries_df.id) & ( npv_df.year == queries_df.year),'right' )\\\n",
    ".select(queries_df.id ,queries_df.year,coalesce(col('npv'),lit('0')).alias('npv')).orderBy('id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7faa56ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n",
      "| id|year|npv|\n",
      "+---+----+---+\n",
      "|  1|2019|113|\n",
      "|  2|2008|121|\n",
      "|  3|2009| 12|\n",
      "|  7|2018|  0|\n",
      "|  7|2019|  0|\n",
      "|  7|2020| 30|\n",
      "| 13|2019| 40|\n",
      "+---+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "npv_df.createOrReplaceTempView('npvdf')\n",
    "queries_df.createOrReplaceTempView('queriesdf')\n",
    "\n",
    "spark.sql('''\n",
    "          select queriesdf.id,queriesdf.year,coalesce(npv,0) as npv from queriesdf \n",
    "              left join npvdf on queriesdf.id == npvdf.id and queriesdf.year == npvdf.year\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081b4e11",
   "metadata": {},
   "source": [
    "#### 50 112 Create a Session Bar Chart E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "741d032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/50_session.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "874a0e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: integer (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "69969775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|        bin|total|\n",
      "+-----------+-----+\n",
      "|        0-5|    3|\n",
      "|       5-10|    1|\n",
      "|      10-15|    0|\n",
      "|15 or above|    1|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "missing_bin_df = spark.createDataFrame([Row(bin='10-15', total=0, row='3')])\n",
    "\n",
    "window_spec = Window.partitionBy(col('row'))\n",
    "session_df.withColumn('bin',(when(col('duration').between(0,300),lit('0-5'))\n",
    "                            .when(col('duration').between(300,600),lit('5-10'))  \n",
    "                            .when(col('duration').between(600,900),lit('10-15'))\n",
    "                            .when(col('duration') > 900,lit('15 or above') )             \n",
    "                            ).otherwise(0))\\\n",
    ".withColumn('row',(when(col('duration').between(0,300),lit('1'))\n",
    "                            .when(col('duration').between(300,600),lit('2'))  \n",
    "                            .when(col('duration').between(600,900),lit('3'))\n",
    "                            .when(col('duration') > 900,lit('4') )             \n",
    "                            ).otherwise(0))\\\n",
    ".withColumn('total',count('session_id').over(window_spec))\\\n",
    ".select('bin','total','row').distinct().union(missing_bin_df).select('bin','total').orderBy('row').show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "812c773b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 267:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+\n",
      "| bin|total|row|\n",
      "+----+-----+---+\n",
      "|5-10|    0|  2|\n",
      "+----+-----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "missing_bin_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "926a1425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|        bin|total|\n",
      "+-----------+-----+\n",
      "|        0-5|    3|\n",
      "|       5-10|    1|\n",
      "|      10-15|    0|\n",
      "|15 or above|    1|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session_df.createOrReplaceTempView('sessiondf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select bin,count(\"session_id\") as total from \n",
    "              (select case when duration between 0   and 300 then \"0-5\"\n",
    "                      when duration between 301 and 600 then \"5-10\"\n",
    "                      when duration between 601 and 900 then \"10-15\"\n",
    "                      when duration >= 901 then \"15 or above\" end as bin,\n",
    "                      session_id from sessiondf) group by bin),\n",
    "       one as (select bin,total from main union(select \"10-15\" as bin, \"0\" as total)),\n",
    "       two as (select bin,total,case when bin =  \"0-5\"   then \"0\"\n",
    "                                     when bin =  \"5-10\"  then \"1\"\n",
    "                                     when bin = \"10-15\"  then \"2\"\n",
    "                                     when bin = \"15 or above\" then \"3\" \n",
    "                                     end as row from one)\n",
    "               select bin,total from two order by row\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "265a2cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+\n",
      "| bin|row|session_id|\n",
      "+----+---+----------+\n",
      "|5-10|  1|         1|\n",
      "|5-10|  1|         2|\n",
      "|5-10|  1|         3|\n",
      "| 0-5|  0|         4|\n",
      "| 0-5|  0|         5|\n",
      "+----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "          select case when duration not between 0   and 300 then \"0-5\"\n",
    "                      when duration not between 301 and 600 then \"5-10\"\n",
    "                      when duration not between 601 and 900 then \"10-15\"\n",
    "                      else \"0\" end as bin,\n",
    "                 case when duration not between 0   and 300 then \"0\"\n",
    "                      when duration not between 301 and 600 then \"1\"\n",
    "                      when duration not between 601 and 900 then \"2\"\n",
    "                      else \"0\" end as row,  \n",
    "                    session_id from sessiondf\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b0b88eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+\n",
      "|  bin|row|session_id|\n",
      "+-----+---+----------+\n",
      "|10-15|  3|         0|\n",
      "+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "          \n",
    "          select \"10-15\" as bin,\"3\" as row, \"0\" as session_id     \n",
    "                    \n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9142444",
   "metadata": {},
   "source": [
    "#### 51 119 Group Sold Products By The Date E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0db5c734",
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/51_activities.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "e2551915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sell_date: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activities_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "f1dc3120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "| sell_date|   product|\n",
      "+----------+----------+\n",
      "|2020-05-30| Headphone|\n",
      "|2020-06-01|    Pencil|\n",
      "|2020-06-02|      Mask|\n",
      "|2020-05-30|Basketball|\n",
      "|2020-06-01|     Bible|\n",
      "|2020-06-02|      Mask|\n",
      "|2020-05-30|   T-Shirt|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activities_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "b995d1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------------------------+\n",
      "|sell_date |num_sold|product                     |\n",
      "+----------+--------+----------------------------+\n",
      "|2020-05-30|3       |Headphone,Basketball,T-Shirt|\n",
      "|2020-06-01|2       |Bible,Pencil                |\n",
      "|2020-06-02|1       |Mask                        |\n",
      "+----------+--------+----------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 783:=================================================>   (185 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('sell_date')\n",
    "\n",
    "activities_df.distinct().withColumn('num_sold',count('product').over(window_spec)) \\\n",
    ".withColumn('product',array_join(collect_list(col('product')).over(window_spec),','))\\\n",
    ".select('sell_date','num_sold','product').distinct().orderBy(col('num_sold').desc()).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "b3786529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------------------------+\n",
      "|sell_date |num_sold|products                    |\n",
      "+----------+--------+----------------------------+\n",
      "|2020-05-30|3       |Headphone,T-Shirt,Basketball|\n",
      "|2020-06-01|2       |Pencil,Bible                |\n",
      "|2020-06-02|1       |Mask                        |\n",
      "+----------+--------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activities_df.createOrReplaceTempView('activitiesdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select distinct sell_date,count(distinct product) as num_sold,product \n",
    "                    from activitiesdf group by sell_date,product)\n",
    "               select distinct sell_date,sum(num_sold ) over(partition by sell_date) as num_sold,\n",
    "                   array_join (collect_list(product) over(partition by sell_date),\",\") as products from main\n",
    "        ''').show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5fe475",
   "metadata": {},
   "source": [
    "#### 52 120 Friendly Movies Streamed Last Month E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d772053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tvprogram_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/52_tvprogram.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b71e7c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/52_content.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "fdaf200d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- program_date: string (nullable = true)\n",
      " |-- content_id: integer (nullable = true)\n",
      " |-- channel: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tvprogram_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "79b4d5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- content_id: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- Kids_content: string (nullable = true)\n",
      " |-- content_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "content_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "320cabb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|  title|\n",
      "+-------+\n",
      "|Aladdin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tvprogram_df.filter((year(col('program_date')) == lit('2020')) & (month(col('program_date')) == lit('06')))\\\n",
    ".join(content_df,(tvprogram_df.content_id == content_df.content_id) &(content_df.Kids_content == 'Y'),'inner' )\\\n",
    ".select('title').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "a8f586fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|  title|\n",
      "+-------+\n",
      "|Aladdin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tvprogram_df.createOrReplaceTempView('tvprogramdf')\n",
    "content_df.createOrReplaceTempView('contentdf')\n",
    "\n",
    "spark.sql('''\n",
    "         select title from contentdf join \n",
    "         (select  content_id from tvprogramdf where year(program_date) == \"2020\" and month(program_date) == \"06\") as x\n",
    "         on contentdf.content_id = x.content_id and contentdf.Kids_content == \"Y\"\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24187668",
   "metadata": {},
   "source": [
    "#### 53 122 Customer Order Frequency E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6e5210ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/53_customers.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "51ea6f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/53_product.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0f31c466",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/53_orders.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "099ff35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "84bf0c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "81e698bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "a1327949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "|customer_id|   name|\n",
      "+-----------+-------+\n",
      "|          1|Winston|\n",
      "+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('customer_id','order_date')\n",
    "window_rkspec = Window.partitionBy('customer_id').orderBy(col('gpprice').desc())\n",
    "\n",
    "orders_df.filter( (year(col('order_date')) == lit('2020')) & (month(col('order_date')).isin([6,7])))\\\n",
    ".join(product_df,orders_df.product_id == product_df.product_id,'inner')\\\n",
    ".select('customer_id',substring('order_date',1,7).alias('order_date'),expr(\"quantity * price\").alias('saleprice'))\\\n",
    ".withColumn('gpprice',sum(col('saleprice')).over(window_spec))\\\n",
    ".select('customer_id','gpprice').withColumn('rnk',rank().over(window_rkspec))\\\n",
    ".filter( (col('rnk') == lit('2')) & (col('gpprice') >= lit('100'))  )\\\n",
    ".join(customers_df, on = 'customer_id').select('customer_id','name').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "bfe95f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "|customer_id|   name|\n",
      "+-----------+-------+\n",
      "|          1|Winston|\n",
      "+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df.createOrReplaceTempView('customersdf')\n",
    "product_df.createOrReplaceTempView('productdf')\n",
    "orders_df.createOrReplaceTempView('ordersdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with main as (select customer_id,substring(order_date,1,7) as order_date,quantity,product_id \n",
    "                    from ordersdf where year(order_date) == 2020 and month(order_date) in (6,7)),\n",
    "        one as (select customer_id,order_date, sum(quantity * price) over(partition by customer_id,order_date) as salesprice \n",
    "                    from main join productdf on main.product_id == productdf.product_id),\n",
    "        two as (select customer_id,salesprice ,rank() over(partition by customer_id order by salesprice desc) as x from one)\n",
    "                select distinct two.customer_id,customersdf.name from two join\n",
    "                    customersdf on two.customer_id == customersdf.customer_id and two.x = 2 and two.salesprice >= 100\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe73e11",
   "metadata": {},
   "source": [
    "#### 54 123 Find Users With Valid E-Mails E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "60f44aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/54_users.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "944335bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- mail: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "674156b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------------+\n",
      "|user_id|     name|                mail|\n",
      "+-------+---------+--------------------+\n",
      "|      1|  Winston|winston@leetcode.com|\n",
      "|      3|Annabelle| bella-@leetcode.com|\n",
      "|      4|    Sally|sally.come@leetco...|\n",
      "+-------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.filter(  (col('mail').rlike('@leetcode.com')) \n",
    "                & (col('mail').rlike('^[A-Za-z]'))  \n",
    "                &(~col('mail').contains('#'))  ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "1ff5c050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------------+\n",
      "|user_id|     name|                mail|\n",
      "+-------+---------+--------------------+\n",
      "|      1|  Winston|winston@leetcode.com|\n",
      "|      3|Annabelle| bella-@leetcode.com|\n",
      "|      4|    Sally|sally.come@leetco...|\n",
      "+-------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.createOrReplaceTempView('usersdf')\n",
    "\n",
    "spark.sql('''\n",
    "           select user_id,name,mail from usersdf where \n",
    "                mail rlike(\"@leetcode.com\") and mail rlike('^[A-Za-z]') and mail  not like '%#%'       \n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7a52ee",
   "metadata": {},
   "source": [
    "#### 55 124 Patients With a Condition E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1f62309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/55_patients.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "5a1e3773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- patient_id: integer (nullable = true)\n",
      " |-- patient_name: string (nullable = true)\n",
      " |-- conditions: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "patients_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "5d8319ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+\n",
      "|patient_id|patient_name|  conditions|\n",
      "+----------+------------+------------+\n",
      "|         3|         Bob|DIAB100 MYOP|\n",
      "|         4|      George|ACNE DIAB100|\n",
      "+----------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "patients_df.filter(col('conditions').contains('DIAB1')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "a2a3430c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+\n",
      "|patient_id|patient_name|  conditions|\n",
      "+----------+------------+------------+\n",
      "|         3|         Bob|DIAB100 MYOP|\n",
      "|         4|      George|ACNE DIAB100|\n",
      "+----------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "patients_df.createOrReplaceTempView('patientsdf')\n",
    "\n",
    "spark.sql('''\n",
    "             select * from patientsdf where conditions like '%DIAB1%'\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2882f84f",
   "metadata": {},
   "source": [
    "#### 56 126  Fix Product Name Format E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8bf256dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/56_sales.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "333edafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sale_id: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- sale_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "68dcf6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-----+\n",
      "|product_name|sale_date|total|\n",
      "+------------+---------+-----+\n",
      "|  lckeychain|  2000-02|    2|\n",
      "|     lcphone|  2000-01|    2|\n",
      "|     lcphone|  2000-02|    1|\n",
      "|  matryoshka|  2000-03|    1|\n",
      "+------------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('product_name','sale_date')\n",
    "sales_df.select(lower(trim('product_name')).alias('product_name'),substring('sale_date',1,7).alias('sale_date') )\\\n",
    ".withColumn('total',count('product_name').over(window_spec)).distinct()\\\n",
    ".orderBy(col('product_name').asc(),col('sale_date').asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "11d2a397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----+\n",
      "|product_name|   date|total|\n",
      "+------------+-------+-----+\n",
      "|  lckeychain|2000-02|    2|\n",
      "|     lcphone|2000-01|    2|\n",
      "|     lcphone|2000-02|    1|\n",
      "|  matryoshka|2000-03|    1|\n",
      "+------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.createOrReplaceTempView('patientsdf')\n",
    "\n",
    "spark.sql('''\n",
    "            select product_name,date,count(product_name) as total from\n",
    "            (select lower(trim(product_name)) as product_name,substring(sale_date,1,7) as date from patientsdf)\n",
    "                 group by product_name,date order by product_name,date\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9cb099",
   "metadata": {},
   "source": [
    "#### 57 129 Unique Orders and Customers Per Month E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "aa95d674",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/57_orders.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "a8d629ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- invoice: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "4aff301d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------+\n",
      "|order_date|order_count|customer_count|\n",
      "+----------+-----------+--------------+\n",
      "|   2020-09|          2|             2|\n",
      "|   2020-10|          1|             1|\n",
      "|   2020-12|          2|             1|\n",
      "|   2021-01|          1|             1|\n",
      "+----------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('order_date')\n",
    "\n",
    "orders_df.filter( col('invoice') > lit('20') ).select(substring('order_date',1,7).alias('order_date'),'customer_id','invoice')\\\n",
    ".withColumn('order_count',count(col('invoice')).over(window_spec))\\\n",
    ".select('order_date','order_count','customer_id').distinct()\\\n",
    ".withColumn('customer_count',count(col('customer_id')).over(window_spec))\\\n",
    ".select('order_date','order_count','customer_count').distinct().orderBy('order_date').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "701510bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------+\n",
      "|order_date|order_count|customer_count|\n",
      "+----------+-----------+--------------+\n",
      "|   2020-09|          2|             2|\n",
      "|   2020-10|          1|             1|\n",
      "|   2020-12|          2|             1|\n",
      "|   2021-01|          1|             1|\n",
      "+----------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.createOrReplaceTempView('ordersdf')\n",
    "\n",
    "spark.sql('''\n",
    "          select order_date,count(invoice) as order_count,count(distinct customer_id) as customer_count from\n",
    "          (select substring(order_date,1,7) as order_date,customer_id,invoice \n",
    "               from ordersdf where invoice > 20)\n",
    "                     group by order_date order by order_date\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f99e6b0",
   "metadata": {},
   "source": [
    "#### 58 130 Warehouse Manager E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5d66a479",
   "metadata": {},
   "outputs": [],
   "source": [
    "warehouse_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/58_warehouse.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9fe9f2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/58_products.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "6dc73b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- units: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warehouse_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "05d9f83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- Width: integer (nullable = true)\n",
      " |-- Length: integer (nullable = true)\n",
      " |-- Height: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "54a85947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|    name|volume|\n",
      "+--------+------+\n",
      "|LCHouse1| 12250|\n",
      "|LCHouse2| 20250|\n",
      "|LCHouse3|   800|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.select('product_id',expr(\"Width * Length * Height\").alias('size'))\\\n",
    ".join(warehouse_df,products_df.product_id == warehouse_df.product_id,'inner')\\\n",
    ".select('name',expr(\"size * units\").alias('volume'))\\\n",
    ".groupBy('name').agg(sum(col('volume')).alias('volume')).orderBy('name').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "5f98c212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|    name|sum(volume)|\n",
      "+--------+-----------+\n",
      "|LCHouse1|      12250|\n",
      "|LCHouse2|      20250|\n",
      "|LCHouse3|        800|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warehouse_df.createOrReplaceTempView('warehousedf')\n",
    "products_df.createOrReplaceTempView('productsdf')\n",
    "\n",
    "spark.sql('''\n",
    "with main as (select name, (Width * Length * Height) * (units) as volume\n",
    "              from productsdf join warehousedf on productsdf.product_id = warehousedf.product_id)\n",
    "              select name, sum(volume) from main group by name order by name\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88de86a",
   "metadata": {},
   "source": [
    "#### 59 131  Customer Who Visited but Did Not Make Any Transactions E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2d9510da",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/59_visits.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fdfbef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/59_transactions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "f20bf887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- visit_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visits_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "df7865f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- visit_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "9ce9eb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|customer_id|count_no_trans|\n",
      "+-----------+--------------+\n",
      "|         54|             2|\n",
      "|         96|             1|\n",
      "|         30|             1|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visits_df.join(transactions_df,visits_df.visit_id == transactions_df.visit_id,'leftanti')\\\n",
    ".groupBy('customer_id').agg(count(col('visit_id')).alias('count_no_trans')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "af57a0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|customer_id|count_no_trans|\n",
      "+-----------+--------------+\n",
      "|         54|             2|\n",
      "|         96|             1|\n",
      "|         30|             1|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visits_df.createOrReplaceTempView('visitsdf')\n",
    "transactions_df.createOrReplaceTempView('transactionsdf')\n",
    "\n",
    "spark.sql('''\n",
    "         select customer_id,count(visit_id) as count_no_trans \n",
    "              from visitsdf left anti join transactionsdf on visitsdf.visit_id = transactionsdf.visit_id\n",
    "              group by customer_id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47946aab",
   "metadata": {},
   "source": [
    "#### 60 132 Bank Account Summary II E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d41907f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/60_users.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1c29245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/60_transactions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "c2c64291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- account: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "64942e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trans_id: integer (nullable = true)\n",
      " |-- account: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- transacted_on: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "36f1bf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "| name|balance|\n",
      "+-----+-------+\n",
      "|Alice|  11000|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.groupBy(col('account')).agg(sum(col('amount')).alias('balance')).filter(col('balance') > lit('10000'))\\\n",
    ".join(users_df, on = 'account').select('name','balance').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "332df3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "| name|balance|\n",
      "+-----+-------+\n",
      "|Alice|  11000|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.createOrReplaceTempView('usersdf')\n",
    "transactions_df.createOrReplaceTempView('transactionsdf')\n",
    "\n",
    "spark.sql('''\n",
    "           select name,sum(amount) as balance from usersdf join transactionsdf on usersdf.account = transactionsdf.account\n",
    "                group by name having balance > 10000\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f578e4",
   "metadata": {},
   "source": [
    "#### 61 134 Sellers With No Sales E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f337eb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/61_customer.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6711642e",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/61_orders.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "89230161",
   "metadata": {},
   "outputs": [],
   "source": [
    "seller_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/61_seller.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6f4e274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "473b0582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- sale_date: string (nullable = true)\n",
      " |-- order_cost: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- seller_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52bc4eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- seller_id: integer (nullable = true)\n",
      " |-- seller_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seller_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66815335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|seller_name|\n",
      "+-----------+\n",
      "|      Frank|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.filter(year(col('sale_date')) == lit('2020'))\\\n",
    ".join(seller_df,seller_df.seller_id == orders_df.seller_id,'right')\\\n",
    ".filter(col('order_id').isNull()).select('seller_name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0c5117b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|seller_name|\n",
      "+-----------+\n",
      "|      Frank|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.createOrReplaceTempView('ordersdf')\n",
    "seller_df.createOrReplaceTempView('sellerdf')\n",
    "\n",
    "spark.sql('''\n",
    "          select seller_name from sellerdf left join \n",
    "          (select seller_id from ordersdf where year(sale_date) = 2020) as x on x.seller_id == sellerdf.seller_id\n",
    "          where x.seller_id is null\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc13f2c8",
   "metadata": {},
   "source": [
    "#### 62 136 All Valid Triplets That Can Represent a Country E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "fac2b128",
   "metadata": {},
   "outputs": [],
   "source": [
    "schoola_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/62_schoola.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5fc766bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "schoolb_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/62_schoolb.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8ea3608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "schoolc_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/62_schoolc.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "42818575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- student_id: integer (nullable = true)\n",
      " |-- student_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schoola_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3b3418ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- student_id: integer (nullable = true)\n",
      " |-- student_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schoolb_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f17ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "92917059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- student_id: integer (nullable = true)\n",
      " |-- student_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schoolc_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a1b6d964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+\n",
      "|member_A|member_B|member_C|\n",
      "+--------+--------+--------+\n",
      "|     Bob|     Tom|   Alice|\n",
      "|   Alice|     Tom|   Jerry|\n",
      "|     Bob|     Tom|   Jerry|\n",
      "+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schoola_df.join(schoolb_df).join(schoolc_df)\\\n",
    ".select((schoola_df.student_name).alias('member_A'),(schoolb_df.student_name).alias('member_B'),(schoolc_df.student_name).alias('member_C') ).distinct()\\\n",
    ".withColumn('sorted_row',size(array_sort(array_distinct(array('member_A','member_B','member_C')))))\\\n",
    ".filter(col('sorted_row') == 3).select('member_A','member_B','member_C').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "158ef680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+\n",
      "|member_A|member_B|member_C|\n",
      "+--------+--------+--------+\n",
      "|   Alice|     Tom|   Jerry|\n",
      "|     Bob|     Tom|   Jerry|\n",
      "|     Bob|     Tom|   Alice|\n",
      "+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schoola_df.createOrReplaceTempView('schooladf')\n",
    "schoolb_df.createOrReplaceTempView('schoolbdf')\n",
    "schoolc_df.createOrReplaceTempView('schoolcdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select schooladf.student_name as member_A, schoolbdf.student_name as member_B, schoolcdf.student_name as member_C\n",
    "                    from schooladf,schoolbdf,schoolcdf),\n",
    "       one as (select member_A,member_B,member_C,size(array_distinct(array(member_A,member_B,member_c))) as arry_value from main)\n",
    "              select  member_A,member_B,member_C from one where arry_value = 3       \n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d437cd4",
   "metadata": {},
   "source": [
    "#### 63 137 Percentage of Users Attended a Contest E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7140054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/63_users.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "50b5a233",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/63_register.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "45959a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a660849b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contest_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "register_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4a31ea85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|contest_id|percentage|\n",
      "+----------+----------+\n",
      "|       208|     100.0|\n",
      "|       209|     100.0|\n",
      "|       210|     100.0|\n",
      "|       215|     66.67|\n",
      "|       207|     33.33|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "register_df.groupBy('contest_id').agg(round((count(col('user_id'))/3)*100,2).alias('percentage') )\\\n",
    ".orderBy(col('percentage').desc(),col('contest_id').asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c945b04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|contest_id|percentage|\n",
      "+----------+----------+\n",
      "|       208|     100.0|\n",
      "|       209|     100.0|\n",
      "|       210|     100.0|\n",
      "|       215|     66.67|\n",
      "|       207|     33.33|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.createOrReplaceTempView('usersdf')\n",
    "register_df.createOrReplaceTempView('registerdf')\n",
    "\n",
    "spark.sql('''\n",
    "     select contest_id, round((count(user_id)/3)*100,2) as percentage from registerdf group by contest_id\n",
    "          order by percentage desc,contest_id asc\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f01efa3",
   "metadata": {},
   "source": [
    "#### 64 138 Average Time of Process per Machine E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f6a8c037",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/64_actiivity.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f77b4d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- machine_id: integer (nullable = true)\n",
      " |-- process_id: integer (nullable = true)\n",
      " |-- activity_type: string (nullable = true)\n",
      " |-- timestamp: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ca574a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n",
      "|machine_id|processing_time|\n",
      "+----------+---------------+\n",
      "|         0|          0.894|\n",
      "|         1|          0.995|\n",
      "|         2|          1.456|\n",
      "+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_df = activity_df.filter(col('activity_type') == lit('start'))\n",
    "end_df = activity_df.filter(col('activity_type') == lit('end')).withColumnRenamed('timestamp','end_timestamp')\n",
    "\n",
    "start_df.join(end_df,( (start_df.machine_id == end_df.machine_id) & (start_df.process_id == end_df.process_id)),'inner' )\\\n",
    ".select(start_df.machine_id,(end_df.end_timestamp - start_df.timestamp).alias('time_diff'))\\\n",
    ".groupBy('machine_id').agg((sum(col('time_diff'))/count(col('machine_id'))).alias('processing_time'))\\\n",
    ".orderBy('machine_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "acd0a28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n",
      "|machine_id|processing_time|\n",
      "+----------+---------------+\n",
      "|         0|          0.894|\n",
      "|         1|          0.995|\n",
      "|         2|          1.456|\n",
      "+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity_df.createOrReplaceTempView('activitydf')\n",
    "\n",
    "spark.sql('''\n",
    "             select start.machine_id, sum(end.timestamp - start.timestamp)/count(start.machine_id) as processing_time from \n",
    "             (select machine_id,process_id,timestamp from activitydf where activity_type == \"end\") as end join\n",
    "             (select machine_id,process_id,timestamp from activitydf where activity_type == \"start\") as start\n",
    "             on start.machine_id = end.machine_id and start.process_id = end.process_id group by start.machine_id\n",
    "             order by machine_id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee33330",
   "metadata": {},
   "source": [
    "#### 65 139 Fix Names in a Table E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2996cf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/65_users.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d7aed8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "cade3a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|user_id| name|\n",
      "+-------+-----+\n",
      "|      1|Alice|\n",
      "|      2|  Bob|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.select('user_id',initcap(lower('name')).alias('name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "640ccc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|user_id| name|\n",
      "+-------+-----+\n",
      "|      1|Alice|\n",
      "|      2|  Bob|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.createOrReplaceTempView('usersdf')\n",
    "\n",
    "spark.sql('''\n",
    "           select user_id,initcap(lower(name)) as name from usersdf\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb397209",
   "metadata": {},
   "source": [
    "#### 66 140 Product's Worth Over Invoices E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ee54b0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/66_product.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7fe4ffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "invoice_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/66_invoice.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "24cc0e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "24b33806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- invoice_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- rest: integer (nullable = true)\n",
      " |-- paid: integer (nullable = true)\n",
      " |-- canceled: integer (nullable = true)\n",
      " |-- refunded: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "invoice_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "8546c007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+--------+--------+\n",
      "| name|rest|paid|canceled|refunded|\n",
      "+-----+----+----+--------+--------+\n",
      "|bacon|   3|   3|       3|       3|\n",
      "|  ham|   2|   4|       5|       3|\n",
      "+-----+----+----+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "invoice_df.groupBy('product_id').agg(sum(col('rest')).alias('rest'),sum(col('paid')).alias('paid'),\n",
    " sum(col('canceled')).alias('canceled'),sum(col('refunded')).alias('refunded')).join(product_df,on = 'product_id')\\\n",
    ".select('name','rest','paid','canceled','refunded').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "7bc7d26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+--------+--------+\n",
      "| name|rest|pain|canceled|refunded|\n",
      "+-----+----+----+--------+--------+\n",
      "|bacon|   3|   3|       3|       3|\n",
      "|  ham|   2|   4|       5|       3|\n",
      "+-----+----+----+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_df.createOrReplaceTempView('productdf')\n",
    "invoice_df.createOrReplaceTempView('invoicedf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select product_id,sum(rest) as rest,sum(paid) as pain,\n",
    "                    sum(canceled) as canceled,sum(refunded) as refunded from invoicedf group by product_id)\n",
    "               select name,rest,pain,canceled,refunded \n",
    "                    from main join productdf on productdf.product_id = main.product_id\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1556b0",
   "metadata": {},
   "source": [
    "#### 67 141 Invalid Tweets E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0f3d5305",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/67_tweets.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "0190a59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tweet_id: integer (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "e015494c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------------+\n",
      "|tweet_id|content                         |\n",
      "+--------+--------------------------------+\n",
      "|1       |Vote for Biden                  |\n",
      "|2       |Let us make America great again!|\n",
      "+--------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "3efbdf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|tweet_id|\n",
      "+--------+\n",
      "|       2|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_df.filter(length(col('content')) > 20).select('tweet_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "0a68fec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|tweet_id|\n",
      "+--------+\n",
      "|       2|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_df.createOrReplaceTempView('tweetsdf')\n",
    "\n",
    "spark.sql('''\n",
    "          select tweet_id from tweetsdf where length(content) > 20\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53603d90",
   "metadata": {},
   "source": [
    "#### 68 142 Daily Leads and Partners E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "bc44e8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dailysales_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/68_dailysales.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "f9bff323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date_id: string (nullable = true)\n",
      " |-- make_name: string (nullable = true)\n",
      " |-- lead_id: integer (nullable = true)\n",
      " |-- partner_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dailysales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "7acb91d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 695:===============================================>     (181 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------+---------------+\n",
      "|  date_id|make_name|unique_leads|unique_partners|\n",
      "+---------+---------+------------+---------------+\n",
      "|2020-12-8|   toyota|           2|              3|\n",
      "|2020-12-7|    honda|           3|              2|\n",
      "|2020-12-7|   toyota|           1|              2|\n",
      "|2020-12-8|    honda|           2|              2|\n",
      "+---------+---------+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dailysales_df.groupBy('date_id','make_name').agg(countDistinct(col('lead_id')).alias('unique_leads'),\n",
    "                                                 countDistinct(col('partner_id')).alias('unique_partners')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "60270c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------+---------------+\n",
      "|  date_id|make_name|unique_leads|unique_partners|\n",
      "+---------+---------+------------+---------------+\n",
      "|2020-12-8|   toyota|           2|              3|\n",
      "|2020-12-7|    honda|           3|              2|\n",
      "|2020-12-7|   toyota|           1|              2|\n",
      "|2020-12-8|    honda|           2|              2|\n",
      "+---------+---------+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dailysales_df.createOrReplaceTempView('dailysalesdf')\n",
    "\n",
    "spark.sql('''\n",
    "             select date_id,make_name,count(distinct lead_id) as unique_leads, count(distinct partner_id ) as unique_partners\n",
    "                  from dailysalesdf group by date_id,make_name\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5fa1dd",
   "metadata": {},
   "source": [
    "#### 69 146 Find Followers Count E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "04830d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "followers_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/69_followers.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "57bf5611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- follower_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "followers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "1571a06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\n",
      "|user_id|followers_count|\n",
      "+-------+---------------+\n",
      "|      0|              1|\n",
      "|      1|              1|\n",
      "|      2|              2|\n",
      "+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "followers_df.groupBy('user_id').agg(count(col('follower_id')).alias('followers_count')).orderBy('user_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "3145b465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\n",
      "|user_id|followers_count|\n",
      "+-------+---------------+\n",
      "|      0|              1|\n",
      "|      1|              1|\n",
      "|      2|              2|\n",
      "+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "followers_df.createOrReplaceTempView('followersdf')\n",
    "\n",
    "spark.sql('''\n",
    "           select user_id,count(follower_id) as followers_count from followersdf group by user_id order by user_id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3915fb5",
   "metadata": {},
   "source": [
    "#### 70 147  The Number of Employees Which Report to Each Employee E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "93ce4b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/70_employees.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "595efa45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- reports_to: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "66ebae3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+-----------+\n",
      "|employee_id|   name|reports_count|average_age|\n",
      "+-----------+-------+-------------+-----------+\n",
      "|          1|Michael|            2|         40|\n",
      "|          2|  Alice|            2|         37|\n",
      "|          3|    Bob|            1|         37|\n",
      "+-----------+-------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logic_df = employees_df.filter(~col('reports_to').isNull() ).groupBy('reports_to')\\\n",
    ".agg(count(col('reports_to')).alias('reports_count'),\n",
    "    ((sum(col('age'))/count(col('reports_to'))).cast(IntegerType())).alias('average_age'))\\\n",
    ".withColumnRenamed('reports_to','reportsto')\n",
    "\n",
    "employees_df.join(logic_df,employees_df.employee_id == logic_df.reportsto,'inner')\\\n",
    ".select('employee_id','name','reports_count','average_age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "5501d0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+-----------+\n",
      "|employee_id|   name|reports_count|average_age|\n",
      "+-----------+-------+-------------+-----------+\n",
      "|          1|Michael|            2|         40|\n",
      "|          2|  Alice|            2|         37|\n",
      "|          3|    Bob|            1|         37|\n",
      "+-----------+-------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.createOrReplaceTempView('employeesdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select reports_to,count(reports_to) as reports_count,(sum(age)/count(reports_to))as average_age\n",
    "                    from employeesdf where reports_to is not null group by reports_to)\n",
    "               select employee_id,name,reports_count,cast(average_age as int)\n",
    "                    from employeesdf join main on employeesdf.employee_id == main.reports_to\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad334e9",
   "metadata": {},
   "source": [
    "#### 71 148 Find Total Time Spent by Each Employee E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "254cef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/71_employees.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "0940ace0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: integer (nullable = true)\n",
      " |-- event_day: string (nullable = true)\n",
      " |-- in_time: integer (nullable = true)\n",
      " |-- out_time: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "0aec7cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------+\n",
      "| event_day|emp_id|total_time|\n",
      "+----------+------+----------+\n",
      "|2020-11-28|     1|       173|\n",
      "|2020-11-28|     2|        30|\n",
      "|2020-12-03|     1|        41|\n",
      "|2020-12-09|     2|        27|\n",
      "+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.groupBy('event_day','emp_id').agg(sum(col('out_time') - col('in_time')).alias('total_time'))\\\n",
    ".orderBy('event_day','emp_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "7dcb30cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------+\n",
      "| event_day|emp_id|total_time|\n",
      "+----------+------+----------+\n",
      "|2020-11-28|     1|       173|\n",
      "|2020-11-28|     2|        30|\n",
      "|2020-12-03|     1|        41|\n",
      "|2020-12-09|     2|        27|\n",
      "+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.createOrReplaceTempView('employeesdf')\n",
    "\n",
    "spark.sql('''\n",
    "            select event_day,emp_id, sum(out_time - in_time) as total_time from employeesdf \n",
    "                 group by event_day,emp_id order by event_day,emp_id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e866ec39",
   "metadata": {},
   "source": [
    "#### 72 150 Recyclable and Low Fat Products E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1a632c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/72_products.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "2a79414a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- low_fats: string (nullable = true)\n",
      " |-- recyclable: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "4df771c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|product_id|\n",
      "+----------+\n",
      "|         1|\n",
      "|         3|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.filter((col('low_fats') == lit('Y')) & (col('recyclable') == lit('Y'))).select('product_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "e7b2bea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|product_id|\n",
      "+----------+\n",
      "|         1|\n",
      "|         3|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.createOrReplaceTempView('productsdf')\n",
    "\n",
    "spark.sql('''\n",
    "          select product_id from productsdf where low_fats = \"Y\" and recyclable = \"Y\"\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103e8255",
   "metadata": {},
   "source": [
    "#### 73 151 Product's Price for Each Store E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "48a418e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/73_products.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "f0ab979a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- store: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "fcf92338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+\n",
      "|product_id| store|price|\n",
      "+----------+------+-----+\n",
      "|         0|store1|   95|\n",
      "|         0|store3|  105|\n",
      "|         0|store2|  100|\n",
      "|         1|store1|   70|\n",
      "|         1|store3|   80|\n",
      "+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "cdd0a324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+\n",
      "|product_id|store1|store2|store3|\n",
      "+----------+------+------+------+\n",
      "|         0|    95|   100|   105|\n",
      "|         1|    70|  null|    80|\n",
      "+----------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.groupBy('product_id').pivot('store').sum('price').orderBy('product_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "4e7d212a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+\n",
      "|product_id|store1|store2|store3|\n",
      "+----------+------+------+------+\n",
      "|         0|    95|   100|   105|\n",
      "|         1|    70|  null|    80|\n",
      "+----------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql('''\n",
    "with main as((select product_id,case when (product_id in (0,1)) and (store = \"store1\") then price end as store1,'' as store2,'' as store3 from productsdf)\n",
    "              union all (select product_id,'' as store1,case when (product_id in (0,1)) and (store = \"store2\") then price end as store2,'' as store3 from productsdf)\n",
    "              union all (select product_id,'' as store1,'' as store2,case when (product_id in (0,1)) and (store = \"store3\") then price end as store3 from productsdf) )\n",
    "             select product_id,cast(sum(store1) as int) as store1, cast(sum(store2) as int) as store2, cast(sum(store3) as int) as store3 from main\n",
    "                  group by product_id order by product_id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f562b164",
   "metadata": {},
   "source": [
    "#### 74 153 Primary Department for Each Employee E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e47dddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/74_employee.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "4d16aec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- department_id: integer (nullable = true)\n",
      " |-- primary_flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "e5a1bba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|employee_id|department_id|\n",
      "+-----------+-------------+\n",
      "|          1|            1|\n",
      "|          2|            1|\n",
      "|          3|            3|\n",
      "|          4|            3|\n",
      "+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_df = employee_df.filter(col('primary_flag') == lit('Y')).withColumnRenamed('department_id','departmentid')\\\n",
    ".select('employee_id','departmentid')\n",
    "\n",
    "employee_df.filter(col('primary_flag') == lit('N')).select('employee_id','department_id')\\\n",
    ".join(y_df,y_df.employee_id == employee_df.employee_id,'leftanti' ).union(y_df).orderBy('employee_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "e54c6772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|employee_id|department_id|\n",
      "+-----------+-------------+\n",
      "|          1|            1|\n",
      "|          2|            1|\n",
      "|          3|            3|\n",
      "|          4|            3|\n",
      "+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.createOrReplaceTempView('employeedf')\n",
    "\n",
    "spark.sql('''\n",
    "  with Y as (select employee_id,department_id from employeedf where primary_flag = \"Y\"),\n",
    "       N as (select employee_id,department_id from employeedf where primary_flag = \"N\")\n",
    "             (select N.employee_id,N.department_id from N \n",
    "                  left anti join Y on N.employee_id == Y.employee_id) union all (select * from Y) order by employee_id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67358905",
   "metadata": {},
   "source": [
    "#### 75 154  Rearrange Products Table E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d824d5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/75_products.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "50a8bf24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- store1: integer (nullable = true)\n",
      " |-- store2: integer (nullable = true)\n",
      " |-- store3: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "300f85a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+\n",
      "|product_id| store|price|\n",
      "+----------+------+-----+\n",
      "|         0|store1|   95|\n",
      "|         0|store2|  100|\n",
      "|         0|store3|  105|\n",
      "|         1|store1|   70|\n",
      "|         1|store3|   80|\n",
      "+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unpivot_expr = \"stack(3, 'store1', store1, 'store2', store2, 'store3', store3) as (store, price)\"\n",
    "unpivot_df = products_df.select(\"product_id\", expr(unpivot_expr)).filter(\"price is not null\")\n",
    "unpivot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "ce1eedee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+\n",
      "|product_id| store|price|\n",
      "+----------+------+-----+\n",
      "|         0|store1|   95|\n",
      "|         0|store2|  100|\n",
      "|         0|store3|  105|\n",
      "|         1|store1|   70|\n",
      "|         1|store3|   80|\n",
      "+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.createOrReplaceTempView('productsdf')\n",
    "\n",
    "spark.sql('''\n",
    "           SELECT product_id,store,price FROM productsdf \n",
    "                 LATERAL VIEW STACK(3,'store1',store1,'store2',store2,'store3',store3) AS store,price \n",
    "                 where price is not null\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1ea1a1",
   "metadata": {},
   "source": [
    "#### 76 155 Ad-Free Sessions E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "fada0354",
   "metadata": {},
   "outputs": [],
   "source": [
    "playback_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/76_playback.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2bdccd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ads_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/76_ads.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "982de1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- start_time: integer (nullable = true)\n",
      " |-- end_time: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "playback_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "e33811d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ad_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ads_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "d3e33c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|session_id|\n",
      "+----------+\n",
      "|         2|\n",
      "|         3|\n",
      "|         5|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "playback_df.join(ads_df,playback_df.customer_id == ads_df.customer_id,'inner')\\\n",
    ".filter(\"timestamp not between start_time and end_time\").select('session_id').distinct().orderBy('session_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "2628e0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|session_id|\n",
      "+----------+\n",
      "|         3|\n",
      "|         5|\n",
      "|         2|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "playback_df.createOrReplaceTempView('playbackdf')\n",
    "ads_df.createOrReplaceTempView('adsdf')\n",
    "\n",
    "spark.sql('''\n",
    "          select distinct session_id from playbackdf join adsdf on playbackdf.customer_id = adsdf.customer_id\n",
    "               where adsdf.timestamp not between start_time and end_time \n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a52ed2",
   "metadata": {},
   "source": [
    "#### 77 157 Find Customers With Positive Revenue this Year E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "63efa9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/77_customers.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "54ceb098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- revenue: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "5b1b8c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|customer_id|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          4|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df.filter(\"year == 2021 and revenue > 0 \").select('customer_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "46850b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|customer_id|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          4|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df.createOrReplaceTempView('customersdf')\n",
    "\n",
    "spark.sql('''\n",
    "           select customer_id from customersdf where year == 2021 and revenue > 0\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9af5a54",
   "metadata": {},
   "source": [
    "#### 78 161 Convert Date Format E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ad97f861",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/78_days.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "903f2ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- day: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "days_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "dfeba03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|day                  |\n",
      "+---------------------+\n",
      "|Tuesday,April 12 2022|\n",
      "|Monday,August 9 2021 |\n",
      "|Friday,June 26 2020  |\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "days_df.select(date_format('day',\"EEEE,MMMM d yyyy\").alias('day')).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "c681680f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|day                  |\n",
      "+---------------------+\n",
      "|Tuesday,April 12 2022|\n",
      "|Monday,August 9 2021 |\n",
      "|Friday,June 26 2020  |\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "days_df.createOrReplaceTempView('daysdf')\n",
    "\n",
    "spark.sql('''\n",
    "           select date_format(day,\"EEEE,MMMM d yyyy\") as day from daysdf\n",
    "         ''').show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a455274f",
   "metadata": {},
   "source": [
    "#### 79 163 Calculate Special Bonus E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b7f0b2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/79_employees.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "9edbc356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "958074d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|employee_id|bonus|\n",
      "+-----------+-----+\n",
      "|          2|    0|\n",
      "|          3|    0|\n",
      "|          7| 7400|\n",
      "|          8|    0|\n",
      "|          9| 7700|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.withColumn('bonus',when( ((col('employee_id')%2 == lit('1')) \n",
    "                                & (substring(col('name'),1,1) != lit('M')) ),col('salary')).otherwise(0))\\\n",
    "             .select('employee_id','bonus').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "be24a8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|employee_id|bonus|\n",
      "+-----------+-----+\n",
      "|          2|    0|\n",
      "|          3|    0|\n",
      "|          7| 7400|\n",
      "|          8|    0|\n",
      "|          9| 7700|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.createOrReplaceTempView('employeesdf')\n",
    "\n",
    "spark.sql('''\n",
    "           select employee_id, case when ((employee_id%2 = 1) and (substring(name,1,1) != 'M')) then salary \n",
    "                                    else 0 end bonus from employeesdf\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8824ab62",
   "metadata": {},
   "source": [
    "#### 80 165 The Latest Login in 2020 E\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9abc72b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logins_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/80_logins.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "298a9893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- time_stamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logins_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "555fe906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|user_id|         last_stamp|\n",
      "+-------+-------------------+\n",
      "|      6|2020-06-30 15:06:07|\n",
      "|      8|2020-12-30 00:46:50|\n",
      "|      2|2020-01-16 02:49:50|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logins_df.filter(year(col('time_stamp')) == lit('2020') ).groupBy('user_id')\\\n",
    "                 .agg(max(col('time_stamp')).alias('last_stamp')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "6e30baaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|user_id|         last_stamp|\n",
      "+-------+-------------------+\n",
      "|      6|2020-06-30 15:06:07|\n",
      "|      8|2020-12-30 00:46:50|\n",
      "|      2|2020-01-16 02:49:50|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logins_df.createOrReplaceTempView('loginsdf')\n",
    "\n",
    "spark.sql('''\n",
    "          select user_id,max(time_stamp) as last_stamp from loginsdf where year(time_stamp) == \"2020\"\n",
    "                  group by user_id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6b3edc",
   "metadata": {},
   "source": [
    "#### 81 168 Users That Actively Request Confirmation Messages E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "31429569",
   "metadata": {},
   "outputs": [],
   "source": [
    "signups_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/81_signups.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "fe68d08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmations_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/81_confirmations.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64c136d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- time_stamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "signups_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2096382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- time_stamp: string (nullable = true)\n",
      " |-- action: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confirmations_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14c7aff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|user_id|           time_diff|\n",
      "+-------+--------------------+\n",
      "|      2|            24 hours|\n",
      "|      3|6 minutes 59 seconds|\n",
      "|      6|23 hours 59 minut...|\n",
      "|      7|  24 hours 1 seconds|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 11:00:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/29 11:00:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.orderBy('user_id','time_stamp')\n",
    "\n",
    "main_df = confirmations_df.withColumn('rownum',row_number().over(window_spec))\n",
    "\n",
    "odd_df = main_df.filter(col('rownum')%2 == 1)\n",
    "even_df = main_df.filter(col('rownum')%2 == 0).withColumnRenamed('time_stamp','even_timestamp')\n",
    "\n",
    "odd_df.join(even_df,odd_df.user_id == even_df.user_id,'inner')\\\n",
    ".select(odd_df.user_id, (to_timestamp('even_timestamp','yyyy-MM-dd HH:mm:ss') \n",
    "        - to_timestamp('time_stamp','yyyy-MM-dd HH:mm:ss') ).alias('time_diff')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "13007439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|user_id|time_diff|\n",
      "+-------+---------+\n",
      "|      2|     24.0|\n",
      "|      3|   0.1164|\n",
      "|      6|  23.9997|\n",
      "+-------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 11:07:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/29 11:07:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "odd_df.join(even_df,odd_df.user_id == even_df.user_id,'inner')\\\n",
    ".select(odd_df.user_id, ( round((unix_timestamp('even_timestamp','yyyy-MM-dd HH:mm:ss') \n",
    "        - unix_timestamp('time_stamp','yyyy-MM-dd HH:mm:ss'))/3600 ,4)).alias('time_diff'))\\\n",
    ".filter(col('time_diff') <= lit('24.0')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b9c2be3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|user_id|time_diff|\n",
      "+-------+---------+\n",
      "|      2|     24.0|\n",
      "|      3|   0.1164|\n",
      "|      6|  23.9997|\n",
      "+-------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 11:22:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/29 11:22:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "confirmations_df.createOrReplaceTempView('confirmationsdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select user_id,time_stamp,row_number() over(order by user_id,time_stamp) as rownum from confirmationsdf),\n",
    "       odd as (select user_id,time_stamp from main where rownum%2 = 1),\n",
    "      even as (select user_id,time_stamp as even_timestamp from main where rownum%2 = 0) \n",
    "              select odd.user_id, round((unix_timestamp(even_timestamp,'yyyy-MM-dd HH:mm:ss') -\n",
    "                                   unix_timestamp(time_stamp,'yyyy-MM-dd HH:mm:ss'))/3600,4) as time_diff \n",
    "                   from odd join even on odd.user_id = even.user_id \n",
    "                        group by odd.user_id,even_timestamp,time_stamp having time_diff <= 24\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b16a9df",
   "metadata": {},
   "source": [
    "#### 82 171 Employees With Missing Information E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4aeffa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/82_employees.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "8cf779a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/82_salaries.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9ab92908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "90963df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salaries_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8f446cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|employee_id|\n",
      "+-----------+\n",
      "|          2|\n",
      "|          1|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.select('employee_id').join(salaries_df,employees_df.employee_id == salaries_df.employee_id,'leftanti')\\\n",
    ".union(salaries_df.select('employee_id').join(employees_df,employees_df.employee_id == salaries_df.employee_id,'leftanti')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e590fc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|employee_id|\n",
      "+-----------+\n",
      "|          2|\n",
      "|          1|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.createOrReplaceTempView('employeesdf')\n",
    "salaries_df.createOrReplaceTempView('salariesdf')\n",
    "\n",
    "spark.sql('''\n",
    " (select employeesdf.employee_id from employeesdf left anti join salariesdf on employeesdf.employee_id = salariesdf.employee_id)\n",
    " union all (\n",
    "select salariesdf.employee_id from salariesdf left anti join employeesdf on employeesdf.employee_id = salariesdf.employee_id)\n",
    "\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4489dbfd",
   "metadata": {},
   "source": [
    "#### 83 172 Employees Whose Manager Left the Company E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "735818f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/83_employees.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "adb41d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- manager_id: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cb6a8d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|employee_id|\n",
      "+-----------+\n",
      "|         11|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out = employees_df.select('employee_id').collect()\n",
    "x = [out[i][0] for i in range(0,len(out))]\n",
    "\n",
    "employees_df.filter(\"salary < 30000 \").select('employee_id','manager_id').filter(~col('manager_id').isin(x)) \\\n",
    ".select('employee_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "051d66a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|employee_id|\n",
      "+-----------+\n",
      "|         11|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.createOrReplaceTempView('employeesdf')\n",
    "\n",
    "spark.sql('''\n",
    "        select employee_id from employeesdf where salary < 30000 \n",
    "             and manager_id not in (select employee_id from employeesdf)\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a6eef3",
   "metadata": {},
   "source": [
    "#### 84 176 . Low-Quality Problems E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ff71dabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "problems_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/84_problems.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d4acf35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- problem_id: integer (nullable = true)\n",
      " |-- likes: integer (nullable = true)\n",
      " |-- dislikes: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "problems_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "81001639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|problem_id|\n",
      "+----------+\n",
      "|         7|\n",
      "|        10|\n",
      "|        11|\n",
      "|        13|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "problems_df.select( 'problem_id', expr(\"( (likes)/(likes + dislikes) ) * 100\").alias('perecentate')  )\\\n",
    ".filter(col('perecentate') < 60 ).select('problem_id').orderBy('problem_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "dd681442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|problem_id|\n",
      "+----------+\n",
      "|         7|\n",
      "|        10|\n",
      "|        11|\n",
      "|        13|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "problems_df.createOrReplaceTempView('problemsdf')\n",
    "\n",
    "spark.sql('''\n",
    "           select problem_id from \n",
    "           (select problem_id,((likes/(likes+dislikes))*100) as percentage from problemsdf)\n",
    "                where percentage < 60 order by problem_id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed6285",
   "metadata": {},
   "source": [
    "#### 85 180 The Winner University E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "48a79d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "newyork_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/85_newyork.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "36224339",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "california_df= (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/85_california.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "6fd2960e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- student_id: integer (nullable = true)\n",
      " |-- score: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newyork_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "d80edb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- student_id: integer (nullable = true)\n",
      " |-- score: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "california_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "dd2c8459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|   winner|\n",
      "+---------+\n",
      "|No Winner|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newyork = newyork_df.withColumn('uni',lit('newyork university'))\n",
    "california = california_df.withColumn('uni',lit('california university'))\n",
    "\n",
    "window_spec = Window.partitionBy('winner')\n",
    "windowspecwin = Window.partitionBy('win')\n",
    "newyork.withColumn('winner',when( col('score') >= 90,col('uni'))).distinct()\\\n",
    ".union(california.withColumn('winner',when( col('score') >= 90,col('uni'))).distinct()).filter(~col('winner').isNull())\\\n",
    ".withColumn('win',count(col('winner')).over(window_spec))\\\n",
    ".withColumn('fwin',when(count(col('win')).over(windowspecwin) == 1,col('uni')) \n",
    "                  .when(count(col('win')).over(windowspecwin)  > 1,lit('No Winner')))\\\n",
    ".select(col('fwin').alias('winner')).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "a22c5eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|winner            |\n",
      "+------------------+\n",
      "|newyork university|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newyork_df.createOrReplaceTempView('newyorkdf')\n",
    "california_df.createOrReplaceTempView('californiadf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select distinct uni from (\n",
    "                    (select student_id,score, 'newyork university' as uni from newyorkdf) union \n",
    "                    (select student_id,score, 'california university' as uni from californiadf) )\n",
    "                        where score >= 90),\n",
    "       one as (select case \n",
    "                         when (uni = \"newyork university\")     then uni\n",
    "                         when (uni = \"california university'\") then uni\n",
    "                         when (uni = \"newyork university\") and (uni = \"newyork university\") then \"No Winner\"\n",
    "                          end as winner  from main)\n",
    "               select winner from one where winner is not null\n",
    "       ''').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "c7b5d50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 16:07:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---+\n",
      "|uni                  |rnk|\n",
      "+---------------------+---+\n",
      "|california university|1  |\n",
      "|newyork university   |2  |\n",
      "+---------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    " with main as (select distinct uni from (\n",
    "                    (select student_id,score, 'newyork university' as uni from newyorkdf) union \n",
    "                    (select student_id,score, 'california university' as uni from californiadf) )\n",
    "                        where score >= 90),\n",
    "       one as (select uni, dense_rank() over(order by uni) rnk from main)\n",
    "               select * from one\n",
    "  \n",
    "       ''').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b507926",
   "metadata": {},
   "source": [
    "#### 86 181 The Number of Rich Customers E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "dc51a84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/86_store.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "819e6242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bill_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "store_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "75df14ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|rich_count|\n",
      "+----------+\n",
      "|         2|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "store_df.filter(col('amount') > 500).groupBy('customer_id')\\\n",
    ".agg(countDistinct(col('customer_id')).alias('cnt')).select(sum('cnt').alias('rich_count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "43dea5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df.createOrReplaceTempView('storedf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "47927be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|rich_count|\n",
      "+----------+\n",
      "|         2|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "         select count(distinct customer_id) as rich_count from storedf where amount > 500\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c7431f",
   "metadata": {},
   "source": [
    "#### 87 187  The Number of Users That Are Eligible for Discount E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b329982c",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/87_purchases.csv'))\n",
    "\n",
    "#startDate = 2022-03-08, endDate = 2022-03-20, minAmount = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "02c96f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- time_stamp: string (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purchases_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "17c578eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|user_cnt|\n",
      "+--------+\n",
      "|       1|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_date = '2022-03-08 00:00:00'\n",
    "end_date =   '2022-03-20 00:00:00'\n",
    "purchases_df.select(to_timestamp(col('time_stamp'),'yyyy-MM-dd HH:mm:ss').alias('timestamp'))\\\n",
    ".filter( (col('timestamp').between(to_timestamp(lit(start_date),'yyyy-MM-dd HH:mm:ss'),\n",
    "                                 to_timestamp(lit(end_date),'yyyy-MM-dd HH:mm:ss'))) & (col('amount') > 1000))\\\n",
    "       .groupBy(col('timestamp')).agg(count(col('timestamp')).alias('user_cnt')).select('user_cnt').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "c1529f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|user_cnt|\n",
      "+--------+\n",
      "|       1|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purchases_df.createOrReplaceTempView('purchasesdf')\n",
    "\n",
    "spark.sql('''\n",
    "            select count(*) as user_cnt from purchasesdf where\n",
    "                  time_stamp between to_timestamp('2022-03-08 00:00:00','yyyy-MM-dd HH:mm:ss')\n",
    "                                and  to_timestamp('2022-03-20 00:00:00','yyyy-MM-dd HH:mm:ss')\n",
    "                   and amount > 1000\n",
    "\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8472ad2",
   "metadata": {},
   "source": [
    "#### 88 189 The Users That Are Eligible for Discount E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f53c89eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/88_purchases.csv'))\n",
    "# startDate = 2022-03-08, endDate = 2022-03-20, minAmount = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "f0da6712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- time_stamp: string (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purchases_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "2a50639f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|user_id|\n",
      "+-------+\n",
      "|      3|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_date = '2022-03-08 00:00:00'\n",
    "end_date =   '2022-03-20 00:00:00'\n",
    "purchases_df.select('user_id',to_timestamp(col('time_stamp'),'yyyy-MM-dd HH:mm:ss').alias('timestamp'))\\\n",
    ".filter( (col('timestamp').between(to_timestamp(lit(start_date),'yyyy-MM-dd HH:mm:ss'),\n",
    "                                 to_timestamp(lit(end_date),'yyyy-MM-dd HH:mm:ss'))) & (col('amount') > 1000))\\\n",
    ".select('user_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "8696fc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|user_cnt|\n",
      "+--------+\n",
      "|       3|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purchases_df.createOrReplaceTempView('purchasesdf')\n",
    "\n",
    "spark.sql('''\n",
    "            select user_id as user_cnt from purchasesdf where\n",
    "                  time_stamp between to_timestamp('2022-03-08 00:00:00','yyyy-MM-dd HH:mm:ss')\n",
    "                                and  to_timestamp('2022-03-20 00:00:00','yyyy-MM-dd HH:mm:ss')\n",
    "                   and amount > 1000\n",
    "\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2d6816",
   "metadata": {},
   "source": [
    "#### 89 196 Product Sales Analysis V E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "396f7a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/89_sales.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "dd45cc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/89_products.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "7291fc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sale_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "5c333db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "be1bb261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|user_id|spending|\n",
      "+-------+--------+\n",
      "|    101|     125|\n",
      "|    102|      75|\n",
      "|    103|      75|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.join(product_df,sales_df.product_id == product_df.product_id,'inner')\\\n",
    ".select('user_id',expr(\"quantity * price\").alias('sales_price')).groupBy('user_id')\\\n",
    ".agg(sum(col(('sales_price'))).alias('spending')).orderBy(col('spending').desc(),col('user_id').asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "73aa9e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|user_id|spending|\n",
      "+-------+--------+\n",
      "|    101|     125|\n",
      "|    102|      75|\n",
      "|    103|      75|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.createOrReplaceTempView('sales_df')\n",
    "product_df.createOrReplaceTempView('product_df')\n",
    "\n",
    "spark.sql('''\n",
    "        select user_id, sum((quantity * price)) as spending from sales_df\n",
    "             join product_df on sales_df.product_id == product_df.product_id group by user_id\n",
    "             order by spending desc, user_id asc\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bbecde",
   "metadata": {},
   "source": [
    "#### 90 197 All the Matches of the League E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0a1330f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/90_teams.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "d7c4ffa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- team_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teams_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "4106ff0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|  home_team|  away_team|\n",
      "+-----------+-----------+\n",
      "|Leetcode FC|    Ahly SC|\n",
      "|Leetcode FC|Real Madrid|\n",
      "|    Ahly SC|Leetcode FC|\n",
      "|    Ahly SC|Real Madrid|\n",
      "|Real Madrid|Leetcode FC|\n",
      "|Real Madrid|    Ahly SC|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "home_team = teams_df.withColumnRenamed('team_name','home_team')\n",
    "away_team = teams_df.withColumnRenamed('team_name','away_team')\n",
    "\n",
    "home_team.join(away_team).filter(col('home_team') != col('away_team')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "4cd72020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|  home_team|  away_team|\n",
      "+-----------+-----------+\n",
      "|    Ahly SC|Leetcode FC|\n",
      "|Real Madrid|Leetcode FC|\n",
      "|Leetcode FC|    Ahly SC|\n",
      "|Real Madrid|    Ahly SC|\n",
      "|Leetcode FC|Real Madrid|\n",
      "|    Ahly SC|Real Madrid|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teams_df.createOrReplaceTempView('teamsdf')\n",
    "\n",
    "spark.sql('''\n",
    "           select home_team,away_team from \n",
    "           (select team_name as away_team from teamsdf) as yy,\n",
    "           (select team_name as home_team from teamsdf) as xx \n",
    "           where home_team != away_team\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6390d36c",
   "metadata": {},
   "source": [
    "#### 91 199 Number of Unique Subjects Taught by Each Teacher E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ba765847",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/91_teacher.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "93b51c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- teacher_id: integer (nullable = true)\n",
      " |-- subject_id: integer (nullable = true)\n",
      " |-- dept_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teacher_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "7036a476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|teacher_id|cnt|\n",
      "+----------+---+\n",
      "|         1|  2|\n",
      "|         2|  4|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teacher_df.groupBy('teacher_id').agg(countDistinct(col('subject_id')).alias('cnt')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "cceae0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|teacher_id|cnt|\n",
      "+----------+---+\n",
      "|         1|  2|\n",
      "|         2|  4|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teacher_df.createOrReplaceTempView('teacherdf')\n",
    "\n",
    "spark.sql('''\n",
    "          select teacher_id,count(distinct subject_id) as cnt from teacherdf group by teacher_id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e386247b",
   "metadata": {},
   "source": [
    "#### 92 201  Sort the Olympic Table E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "cc2d8ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "olympic_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/92_olympic.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "b87575d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country: string (nullable = true)\n",
      " |-- gold_medals: integer (nullable = true)\n",
      " |-- silver_medals: integer (nullable = true)\n",
      " |-- bronze_medals: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "olympic_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "9b2b7814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-------------+-------------+\n",
      "|    country|gold_medals|silver_medals|bronze_medals|\n",
      "+-----------+-----------+-------------+-------------+\n",
      "|      China|         10|           10|           20|\n",
      "|        USA|         10|           10|           20|\n",
      "|     Israel|          2|            2|            3|\n",
      "|      Egypt|          2|            2|            2|\n",
      "|South Sudan|          0|            0|            1|\n",
      "+-----------+-----------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "olympic_df.select('country','gold_medals','silver_medals','bronze_medals')\\\n",
    ".orderBy(col('gold_medals').desc(),col('silver_medals').desc(),col('bronze_medals').desc(),col('country').asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "1215e481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-------------+-------------+\n",
      "|    country|gold_medals|silver_medals|bronze_medals|\n",
      "+-----------+-----------+-------------+-------------+\n",
      "|      China|         10|           10|           20|\n",
      "|        USA|         10|           10|           20|\n",
      "|     Israel|          2|            2|            3|\n",
      "|      Egypt|          2|            2|            2|\n",
      "|South Sudan|          0|            0|            1|\n",
      "+-----------+-----------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "olympic_df.createOrReplaceTempView('olympicdf')\n",
    "\n",
    "spark.sql('''\n",
    "          select country,gold_medals,silver_medals,bronze_medals from olympicdf\n",
    "          order by gold_medals desc,silver_medals desc,bronze_medals desc,country asc\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aa262d",
   "metadata": {},
   "source": [
    "#### 93 204 Form a Chemical Bond E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "46795efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/93_elements.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "fe13b32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- symbol: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- electrons: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "elements_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "73bade5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Nonmetal|Metal|\n",
      "+--------+-----+\n",
      "|      Cl|   Na|\n",
      "|      Cl|   Ca|\n",
      "|      Cl|   La|\n",
      "|       o|   Na|\n",
      "|       o|   Ca|\n",
      "|       o|   La|\n",
      "|       N|   Na|\n",
      "|       N|   Ca|\n",
      "|       N|   La|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Nonmetal_df = elements_df.filter(col('type') == lit('Nonmetal')).select(col('symbol').alias('Nonmetal'))\n",
    "Metal_df = elements_df.filter(col('type') == lit('Metal')).select(col('symbol').alias('Metal'))\n",
    "Nonmetal_df.join(Metal_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "a8a09eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|NonMetal|Metal|\n",
      "+--------+-----+\n",
      "|      Cl|   Na|\n",
      "|       o|   Na|\n",
      "|       N|   Na|\n",
      "|      Cl|   Ca|\n",
      "|       o|   Ca|\n",
      "|       N|   Ca|\n",
      "|      Cl|   La|\n",
      "|       o|   La|\n",
      "|       N|   La|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "elements_df.createOrReplaceTempView('elementsdf')\n",
    "\n",
    "spark.sql('''\n",
    "             select NonMetal,Metal from \n",
    "            (select symbol as Metal from elementsdf where type = 'Metal'),\n",
    "            (select symbol as NonMetal from elementsdf where type = 'Nonmetal')\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de979d4",
   "metadata": {},
   "source": [
    "#### 94 205 Concatenate the Name and the Profession E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "3be13410",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/94_person.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "f8ec90fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- person_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- profession: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "98deac61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|person_id|    name|\n",
      "+---------+--------+\n",
      "|        1| Alex(S)|\n",
      "|        3|Alice(A)|\n",
      "|        2|  Bob(P)|\n",
      "|        4|Messi(D)|\n",
      "|        6|Tyson(E)|\n",
      "|        5| Meir(L)|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person_df.select('person_id',concat('name',lit('('),substring('profession',1,1),lit(')') ).alias('name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "4ffbd436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|person_id|    name|\n",
      "+---------+--------+\n",
      "|        1| Alex(S)|\n",
      "|        3|Alice(A)|\n",
      "|        2|  Bob(P)|\n",
      "|        4|Messi(D)|\n",
      "|        6|Tyson(E)|\n",
      "|        5| Meir(L)|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person_df.createOrReplaceTempView('persondf')\n",
    "\n",
    "spark.sql('''\n",
    "        select person_id, concat(name,'(',substring(profession,1,1),')') as name from persondf\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f291216e",
   "metadata": {},
   "source": [
    "#### 95 206 Find Latest Salaries E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b2763f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/95_salary.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "a1eee87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: integer (nullable = true)\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- department_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salary_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "89aa63d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------+------+-------------+\n",
      "|emp_id|firstname|lastname|salary|department_id|\n",
      "+------+---------+--------+------+-------------+\n",
      "|     1|     Todd|  Wilson|110000|        D1006|\n",
      "|     2|   Justin|   Simon|130000|        D1005|\n",
      "|     3|    Kelly| Rosario| 42689|        D1002|\n",
      "|     4| Patricia|  Powell|170000|        D1004|\n",
      "|     5|   Sherry|  Golden| 44101|        D1002|\n",
      "|     6|  Natasha| Swanson| 90000|        D1005|\n",
      "+------+---------+--------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('emp_id')\n",
    "\n",
    "salary_df.withColumn('salary',max(col('salary')).over(window_spec)).distinct().orderBy('emp_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "47f07a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------+------+-------------+\n",
      "|emp_id|firstname|lastname|salary|department_id|\n",
      "+------+---------+--------+------+-------------+\n",
      "|     1|     Todd|  Wilson|110000|        D1006|\n",
      "|     2|   Justin|   Simon|130000|        D1005|\n",
      "|     3|    Kelly| Rosario| 42689|        D1002|\n",
      "|     4| Patricia|  Powell|170000|        D1004|\n",
      "|     5|   Sherry|  Golden| 44101|        D1002|\n",
      "|     6|  Natasha| Swanson| 90000|        D1005|\n",
      "+------+---------+--------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salary_df.createOrReplaceTempView('salarydf')\n",
    "\n",
    "spark.sql('''\n",
    "          select distinct emp_id,firstname,lastname,max(salary) over(partition by emp_id) as salary,department_id \n",
    "               from salarydf order by emp_id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49cfca4",
   "metadata": {},
   "source": [
    "#### 96 207 Count Artist Occurrences On Spotify Ranking List E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ca5bca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/96_spotify.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "a18fbe25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- artist: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spotify_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "f7172206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|    artist|occurrences|\n",
      "+----------+-----------+\n",
      "| DJ Khalid|          2|\n",
      "|Ed Sheeran|          2|\n",
      "|       Sia|          1|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spotify_df.groupBy('artist').agg(count(col('artist')).alias('occurrences'))\\\n",
    "                            .orderBy(col('occurrences').desc(),col('artist').asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "08fb9125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|    artist|occurrences|\n",
      "+----------+-----------+\n",
      "| DJ Khalid|          2|\n",
      "|Ed Sheeran|          2|\n",
      "|       Sia|          1|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spotify_df.createOrReplaceTempView('spotifydf')\n",
    "\n",
    "spark.sql('''\n",
    "         select artist,count(artist) as occurrences from spotifydf group by artist\n",
    "              order by occurrences desc, artist asc\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebadd14",
   "metadata": {},
   "source": [
    "#### 97 209 Bikes Last Time Used  E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "72c27d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/97_bikes.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "4d695bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ride_id: integer (nullable = true)\n",
      " |-- bike_number: string (nullable = true)\n",
      " |-- start_time: string (nullable = true)\n",
      " |-- end_time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bikes_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "fa6f612d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+\n",
      "|bike_number|           end_time|\n",
      "+-----------+-------------------+\n",
      "|     W00576|2012-03-28 02:50:00|\n",
      "|     W00455|2012-03-26 17:40:00|\n",
      "|     W00300|2012-03-25 10:50:00|\n",
      "+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bikes_df.groupBy('bike_number').agg(max(col('end_time')).alias('end_time'))\\\n",
    ".orderBy(col('end_time').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "a9b39edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+\n",
      "|bike_number|           end_time|\n",
      "+-----------+-------------------+\n",
      "|     W00576|2012-03-28 02:50:00|\n",
      "|     W00455|2012-03-26 17:40:00|\n",
      "|     W00300|2012-03-25 10:50:00|\n",
      "+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bikes_df.createOrReplaceTempView('bikesdf')\n",
    "\n",
    "spark.sql('''\n",
    "          select bike_number, max(end_time) as end_time from bikesdf group by bike_number order by end_time desc\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad64069",
   "metadata": {},
   "source": [
    "#### 98 214 Total Traveled Distance E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d412e20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/98_users.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "83001bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/98_rides.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "f73bd363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df .printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "9712b604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ride_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rides_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "67f93146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+\n",
      "|ride_id|user_id|distance|\n",
      "+-------+-------+--------+\n",
      "|     72|     17|     160|\n",
      "|     42|     14|     161|\n",
      "|     45|      4|      59|\n",
      "|     32|      2|     197|\n",
      "|     15|      4|     357|\n",
      "|     56|      2|     196|\n",
      "|     10|     14|      25|\n",
      "+-------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rides_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "6d611735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------------+\n",
      "|user_id|   name|traveled distance|\n",
      "+-------+-------+-----------------+\n",
      "|      2|  Avery|              393|\n",
      "|      4|Michael|              416|\n",
      "|     10|Eleanor|                0|\n",
      "|     14|  Ethan|              186|\n",
      "|     17|Addison|              160|\n",
      "+-------+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rides_df.groupBy('user_id').agg(sum(col('distance')).alias('traveled distance'))\\\n",
    ".join(users_df,rides_df.user_id == users_df.user_id, 'right')\\\n",
    ".select(users_df.user_id,'name',coalesce('traveled distance',lit('0')).alias('traveled distance')).orderBy('user_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "4dea2bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------------+\n",
      "|user_id|   name|traveled_distanct|\n",
      "+-------+-------+-----------------+\n",
      "|      2|  Avery|              393|\n",
      "|      4|Michael|              416|\n",
      "|     10|Eleanor|                0|\n",
      "|     14|  Ethan|              186|\n",
      "|     17|Addison|              160|\n",
      "+-------+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.createOrReplaceTempView('usersdf')\n",
    "rides_df.createOrReplaceTempView('ridesdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with main as (select user_id as userid,sum(distance) as traveleddis from ridesdf group by user_id)\n",
    "               select user_id,name,coalesce(traveleddis,'0') as traveled_distanct  \n",
    "                    from main right join usersdf on main.userid = usersdf.user_id order by user_id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733e54fd",
   "metadata": {},
   "source": [
    "#### 99 215 Highest Salaries Difference E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "49b194fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/99_salaries.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "ba27940f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salaries_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "c9e197e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|salary_difference|\n",
      "+-----------------+\n",
      "|            49000|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eng_salry = salaries_df.filter(col('department') == lit('Engineering'))\\\n",
    ".groupBy('department').agg(max(col('salary')).alias('salary')).select('salary')\n",
    "\n",
    "mar_salry = salaries_df.filter(col('department') == lit('Marketing'))\\\n",
    ".groupBy('department').agg(max(col('salary')).alias('salary')).select('salary')\n",
    "\n",
    "eng_salry.join(mar_salry).select(abs(mar_salry.salary - eng_salry.salary).alias('salary_difference') ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "a84f0a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|(salary - salary)|\n",
      "+-----------------+\n",
      "|            49000|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salaries_df.createOrReplaceTempView('salariesdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with eng as (select max(salary) as salary from salariesdf where department = \"Engineering\"),\n",
    "       mrk as (select max(salary) as salary from salariesdf where department = \"Marketing\")\n",
    "              select eng.salary - mrk.salary from mrk,eng\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85354202",
   "metadata": {},
   "source": [
    "#### 100 221 Calculate Compressed Mean E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a3abafa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/100_orders.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "1f4e20f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- item_count: integer (nullable = true)\n",
      " |-- order_occurrences: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "4c0c870c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|average_items_per_order|\n",
      "+-----------------------+\n",
      "|                    2.7|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('dummy')\n",
    "orders_df.withColumn('dummy',lit('1'))\\\n",
    ".withColumn('average_items_per_order',round((sum(col('item_count') * col('order_occurrences')).over(window_spec))/\n",
    " (sum(col('order_occurrences')).over(window_spec)),2)).select('average_items_per_order').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "f2945e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|average_items_per_order|\n",
      "+-----------------------+\n",
      "|                    2.7|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.createOrReplaceTempView('ordersdf')\n",
    "\n",
    "spark.sql('''\n",
    "           select average_items_per_order from\n",
    "          (select 1 as dummy,round(sum(item_count * order_occurrences)/sum(order_occurrences),2) as average_items_per_order \n",
    "               from ordersdf group by dummy)\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bac5cb",
   "metadata": {},
   "source": [
    "#### 101 223 Find Expensive Cities E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "be026a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/101_listiings.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "c83ae41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- listing_id: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "listings_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "b81d5b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2085:==========================================>         (164 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      city|\n",
      "+----------+\n",
      "|   Chicago|\n",
      "|LosAngeles|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('city')\n",
    "window_dummy = Window.partitionBy('dummy')\n",
    "\n",
    "listings_df.withColumn('dummy',lit('1'))\\\n",
    ".withColumn('state',sum(col('price')).over(window_spec)/count(col('city')).over(window_spec))\\\n",
    ".withColumn('national',sum(col('state')).over(window_dummy)/count(col('dummy')).over(window_dummy))\\\n",
    ".select('city',(col('state') - col('national')).alias('diff') ).filter(col('diff') > 0)\\\n",
    ".select('city').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "0f53ce37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      city|\n",
      "+----------+\n",
      "|   Chicago|\n",
      "|LosAngeles|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "listings_df.createOrReplaceTempView('listingsdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with one as (select city, sum(price)/count(city) as state from listingsdf group by city),\n",
    "       two as (select sum(price)/count(*) as national from listingsdf)\n",
    "               select city from \n",
    "                    (select city,(state - national) as diff from one,two)\n",
    "                    where diff > 0\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512baf85",
   "metadata": {},
   "source": [
    "#### 102 226 Loan Types E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "92f2a16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/102_loans.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fb6f980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- loan_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- loan_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loans_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "784c7ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|user_id|\n",
      "+-------+\n",
      "|    102|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loans_df.filter(\"loan_type in ('Refinance','Mortgage')\")\\\n",
    ".groupBy('user_id').agg(count(col('user_id')).alias('cnt')).filter(col('cnt') == 2).select('user_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "203e2d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|user_id|\n",
      "+-------+\n",
      "|    102|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loans_df.createOrReplaceTempView('loansdf')\n",
    "\n",
    "spark.sql('''\n",
    "          select user_id from \n",
    "          (select user_id, count(user_id) as cnt from loansdf where loan_type in ('Refinance','Mortgage')\n",
    "                  group by user_id) where cnt = 2\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c58ab6",
   "metadata": {},
   "source": [
    "#### 103 229 Find Candidates for Data Scientist Position E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "1f837a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/103_candidates.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d650b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- candidate_id: integer (nullable = true)\n",
      " |-- skill: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "candidates_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5456c15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|candidate_id|\n",
      "+------------+\n",
      "|         123|\n",
      "|         147|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "candidates_df.filter(\"skill in ('Python','Tableau','PostgreSQL')\").groupBy('candidate_id').agg(count(col('candidate_id')).alias('cnt'))\\\n",
    ".filter(col('cnt') == 3).select('candidate_id').orderBy(col('candidate_id').asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51f9ee30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|candidate_id|\n",
      "+------------+\n",
      "|         123|\n",
      "|         147|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "candidates_df.createOrReplaceTempView('candidatesdf')\n",
    "\n",
    "spark.sql('''\n",
    "          select candidate_id from \n",
    "          (select candidate_id, count(candidate_id) as cnt from candidatesdf \n",
    "               where skill in ('Python','Tableau','PostgreSQL') group by candidate_id)\n",
    "            where cnt = 3 order by candidate_id asc\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2da8af6",
   "metadata": {},
   "source": [
    "#### 104 230 Classifying Triangles by Lengths E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "0070fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "triangles_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/104_tringles.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7bd510fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- A: integer (nullable = true)\n",
      " |-- B: integer (nullable = true)\n",
      " |-- C: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "triangles_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7b9d764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  A|  B|  C|\n",
      "+---+---+---+\n",
      "| 20| 20| 23|\n",
      "| 20| 20| 20|\n",
      "| 20| 21| 22|\n",
      "| 13| 14| 30|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "triangles_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2661fbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "| triangle_type|\n",
      "+--------------+\n",
      "|     Isosceles|\n",
      "|   Equilateral|\n",
      "|Not A Triangle|\n",
      "|       Scalene|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "triangles_df.withColumn('triangle_type',\n",
    "    when(((col('A') == col('B')) & (col('B') == col('C'))),lit('Equilateral'))\n",
    "   .when(((col('A') == col('B')) & (col('B') != col('C'))),lit('Isosceles')) \n",
    "   .when((((col('A')+col('B') ) > col('C') ) & ((col('B')+col('C')) > col('A')) & ((col('A')+col('C')) >col('B'))),lit('Not A Triangle'))\n",
    "   .when((col('A') != col('B')) & (col('B') != col('C')) & (col('A') != col('C')),lit('Scalene')))\\\n",
    "   .select('triangle_type').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6d91424e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "| triangle_type|\n",
      "+--------------+\n",
      "|     Isosceles|\n",
      "|   Equilateral|\n",
      "|Not A Triangle|\n",
      "|       Scalene|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "triangles_df.createOrReplaceTempView('trianglesdf')\n",
    "\n",
    "spark.sql('''\n",
    "           select case when A = B and B  = C then \"Equilateral\" \n",
    "                       when A = B and B != C then \"Isosceles\" \n",
    "                       when ((A+B) > C) and ((B+C) > A) and ((A+C) >B) then \"Not A Triangle\"\n",
    "                       when A !=B and B != C and A != C then \"Scalene\"\n",
    "                       end triangle_type from trianglesdf\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fb9dad",
   "metadata": {},
   "source": [
    "#### 105 235 Find All Unique Email Domains E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "fbf8aa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/105_emails.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ef9f96b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emails_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "64f7e7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+\n",
      "|email_domain|cnt|\n",
      "+------------+---+\n",
      "| outlook.com|  2|\n",
      "|   yahoo.com|  1|\n",
      "+------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pattern = r'@([a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})'\n",
    "\n",
    "emails_df.filter(col('email').rlike('.com')).withColumn('email_domain',regexp_extract(\"email\", pattern, 1))\\\n",
    ".groupBy('email_domain').agg(count(col('email_domain')).alias('cnt')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2c3de25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+\n",
      "|email_domain|cnt|\n",
      "+------------+---+\n",
      "| outlook.com|  2|\n",
      "|   yahoo.com|  1|\n",
      "+------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emails_df.createOrReplaceTempView('emailsdf')\n",
    "\n",
    "spark.sql('''\n",
    "          select regexp_extract(email, '@([a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,})', 1) AS email_domain,count(*) as cnt\n",
    "               from emailsdf where email rlike '.com' group by email_domain\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d439ed5d",
   "metadata": {},
   "source": [
    "#### 106 242 Invalid Tweets II E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e375038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/106_tweets.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "cf3f996e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tweet_id: integer (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a3e419d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|tweet_id|\n",
      "+--------+\n",
      "|       1|\n",
      "|       4|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_df.withColumn('len',length(col('content')).alias('len'))\\\n",
    ".withColumn('cnt_mentions',col('len') - length(regexp_replace(col('content'),'@','')))\\\n",
    ".withColumn('hashtags',col('len') - length(regexp_replace(col('content'),'#','')))\\\n",
    ".withColumn('tweet',when( (col('len') > 140) \n",
    "                       | (col('cnt_mentions') > 3) \n",
    "                       | (col('hashtags') > 3), col('tweet_id')).otherwise(0)).filter(col('tweet') != 0)\\\n",
    ".select('tweet_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "bcff8bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|tweet_id|\n",
      "+--------+\n",
      "|       1|\n",
      "|       4|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_df.createOrReplaceTempView('tweetsdf')\n",
    "\n",
    "spark.sql('''\n",
    "        select tweet_id from \n",
    "        (select tweet_id, case\n",
    "                         when length(content) > 140 then tweet_id\n",
    "                         when length(content) - length(regexp_replace(content,'@','')) > 3 then tweet_id\n",
    "                         when length(content) - length(regexp_replace(content,'#','')) > 3 then tweet_id\n",
    "                         else 0 end tweet from tweetsdf) \n",
    "            where tweet > 0\n",
    "        \n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88458859",
   "metadata": {},
   "source": [
    "#### 107 244 Second Day Verification E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "bc0d9d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/107_emails.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "331a1932",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/107_texts.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "bd270f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- email_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- signup_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emails_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "dab22c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text_id: integer (nullable = true)\n",
      " |-- email_id: integer (nullable = true)\n",
      " |-- signup_action: string (nullable = true)\n",
      " |-- action_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "8a71f5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|user_id|\n",
      "+-------+\n",
      "|   7005|\n",
      "|   7771|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emails_df.join(texts_df,emails_df.email_id == texts_df.email_id,'inner').filter(col('signup_action') != lit('Not Verified'))\\\n",
    ".withColumn('time_diff',datediff('action_date','signup_date') ).select('user_id').orderBy(col('user_id').asc()).show()\n",
    "                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "69b32f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|user_id|diff|\n",
      "+-------+----+\n",
      "|   7005|   1|\n",
      "|   7771|   1|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emails_df.createOrReplaceTempView('emailsdf')\n",
    "texts_df.createOrReplaceTempView('textsdf')\n",
    "\n",
    "spark.sql('''\n",
    "           select user_id,datediff(action_date,signup_date) as diff from emailsdf\n",
    "                join textsdf on emailsdf.email_id = textsdf.email_id where signup_action != \"Not Verified\"\n",
    "                order by user_id asc\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2df6d4",
   "metadata": {},
   "source": [
    "#### 108 246 Find Cities in Each State E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52dc4b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/108_cities.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "d32bc4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- state: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cities_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "b0b7a07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------------------+\n",
      "|state     |cities                             |\n",
      "+----------+-----------------------------------+\n",
      "|Texas     |Houston,Austin                     |\n",
      "|California|Los Angeles,San Francisco,San Diego|\n",
      "|New York  |New York City,Buffalo,Rochester    |\n",
      "+----------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cities_df.groupBy('state').agg(array_join(collect_list(col('city')),',').alias('cities')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "20e344b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------------------+\n",
      "|state     |cities                             |\n",
      "+----------+-----------------------------------+\n",
      "|Texas     |Houston,Austin                     |\n",
      "|California|Los Angeles,San Francisco,San Diego|\n",
      "|New York  |New York City,Buffalo,Rochester    |\n",
      "+----------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cities_df.createOrReplaceTempView('citiesdf')\n",
    "\n",
    "spark.sql('''\n",
    "          select state,array_join(collect_list(city),',') as cities from citiesdf group by state\n",
    "         ''').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e3b744",
   "metadata": {},
   "source": [
    "#### 109 251 remier League Table Ranking E 3246"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d0c1875",
   "metadata": {},
   "outputs": [],
   "source": [
    "teamstats_df = (spark.read\n",
    "                 .option('header',True)\n",
    "                 .option('inferSchema',True)\n",
    "                 .format('csv')\n",
    "                 .load('../../data/database/109_teamstats.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "45d22be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- team_id: integer (nullable = true)\n",
      " |-- team_name: string (nullable = true)\n",
      " |-- matches_played: integer (nullable = true)\n",
      " |-- wins: integer (nullable = true)\n",
      " |-- draws: integer (nullable = true)\n",
      " |-- losses: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teamstats_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "fbe14621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------+----+-----+------+\n",
      "|team_id|      team_name|matches_played|wins|draws|losses|\n",
      "+-------+---------------+--------------+----+-----+------+\n",
      "|      1|Manchester City|            10|   6|    2|     2|\n",
      "|      2|      Liverpool|            10|   6|    2|     2|\n",
      "|      3|        Chelsea|            10|   5|    3|     2|\n",
      "|      4|        Arsenal|            10|   4|    4|     2|\n",
      "|      5|      Tottenham|            10|   3|    5|     2|\n",
      "+-------+---------------+--------------+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teamstats_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "2a097a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+------+--------+\n",
      "|team_id|      team_name|points|position|\n",
      "+-------+---------------+------+--------+\n",
      "|      2|      Liverpool|    20|       1|\n",
      "|      1|Manchester City|    20|       1|\n",
      "|      3|        Chelsea|    18|       3|\n",
      "|      4|        Arsenal|    16|       4|\n",
      "|      5|      Tottenham|    14|       5|\n",
      "+-------+---------------+------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/30 20:35:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.orderBy(col('points').desc())\n",
    "\n",
    "teamstats_df.select('team_id','team_name',expr(\"((wins * 3)+(draws * 1))\" ).alias('points'))\\\n",
    ".withColumn('position',rank().over(window_spec)).orderBy(col('position').asc(),col('team_id').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "7b56bcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+------+--------+\n",
      "|team_id|      team_name|points|position|\n",
      "+-------+---------------+------+--------+\n",
      "|      2|      Liverpool|    20|       1|\n",
      "|      1|Manchester City|    20|       1|\n",
      "|      3|        Chelsea|    18|       3|\n",
      "|      4|        Arsenal|    16|       4|\n",
      "|      5|      Tottenham|    14|       5|\n",
      "+-------+---------------+------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/30 20:40:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "teamstats_df.createOrReplaceTempView('teamstatsdf')\n",
    "\n",
    "spark.sql('''\n",
    "          \n",
    "          select team_id,team_name, ((wins * 3 )+(draws * 1)) as points,\n",
    "               rank() over(order by ((wins * 3 )+(draws * 1)) desc) as position from teamstatsdf \n",
    "               order by position asc, team_id desc\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed22d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
