{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b107754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('database').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3c6981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (col,expr,count,countDistinct,datediff,to_date,date_add,year,month,lag,lead,rank,max,min,round,\n",
    "        sum,when,lit,desc,coalesce,abs,greatest,least,array,array_sort,substring, explode,explode_outer,posexplode,\n",
    "        posexplode_outer,collect_list,array_intersect,concat_ws,sequence,\n",
    "        unix_timestamp,rank,dense_rank,least,greatest,row_number,array_join,expr,trim,lower,array,sort_array,\n",
    "        array_distinct,size,initcap,length,date_format,to_timestamp,concat,regexp_extract,length,regexp_replace,\n",
    "        sum as spark_sum,sqrt,length,last,abs,avg,flatten,to_timestamp,array_contains,array_intersect,array_union,\n",
    "        array_distinct,element_at,array_remove,row_number,ceil,floor,dayofweek,monotonically_increasing_id,last,first,\n",
    "        asc_nulls_last,hours,hour,aggregate,weekofyear,dayofmonth,date_sub,array_distinct,percentile_approx,from_unixtime,\n",
    "        lpad,rpad,array_intersect,array_except,dayofmonth,dayofyear,weekofyear,month,quarter,year,last_day,count,ceil\n",
    "         )\n",
    "from pyspark.sql.types import (StructField,StructType,\n",
    "                    IntegerType,StringType,DateType,TimestampType )\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import Row\n",
    "#rlike,like,contains,array_join,collect_list,substring,array_size,cast,stack(to unpivot),LATERAL VIEW \n",
    "#Window.orderBy(\"id\").rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "#Window.partitionBy(\"category\").orderBy(\"id\").rangeBetween(Window.currentRow, 1)\n",
    "#activity_df.withColumn('lead',lead('event_date',1).over(window_spec))\n",
    "#withColumn('prev_num',lag(col('num'),1,0).over(window_spec))\n",
    "#row_number(), rank, dense_rank(), sum(), count(), max(), min()\n",
    "#, expr(\"sequence(start_year, end_year)\"))\n",
    "#.withColumn('yeareday',concat_ws('-',lit(year('start')),lit(12),lit(31)))\n",
    "#cntcat_df.filter( cntcat_df['income'].between(*x) ).select(count('*').alias('cnt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1177d5a5",
   "metadata": {},
   "source": [
    "#### 1 10 Department Top Three Salaries H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c6909f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "department_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/1h_department.csv')\n",
    "             .withColumnRenamed('id','dep_id')\n",
    "             .withColumnRenamed('name','dep_name'))\n",
    "\n",
    "employee_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/1h_employee.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f754f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dep_id: integer (nullable = true)\n",
      " |-- dep_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "department_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28ad6fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- departmentId: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e4f97090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+-----+------+------------+---+\n",
      "|dep_id|dep_name| id| name|salary|departmentId|rnk|\n",
      "+------+--------+---+-----+------+------------+---+\n",
      "|     1|      IT|  4|  Max| 90000|           1|  1|\n",
      "|     1|      IT|  1|  Joe| 85000|           1|  2|\n",
      "|     1|      IT|  6|Randy| 85000|           1|  2|\n",
      "|     1|      IT|  7| Will| 70000|           1|  3|\n",
      "|     1|      IT|  5|Janet| 69000|           1|  4|\n",
      "|     2|   Sales|  2|Henry| 80000|           2|  1|\n",
      "|     2|   Sales|  3|  Sam| 60000|           2|  2|\n",
      "+------+--------+---+-----+------+------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('departmentId').orderBy(col('salary').desc())\n",
    "department_df.join(employee_df,department_df.dep_id == employee_df.departmentId,'inner' ).withColumn('rnk',dense_rank().over(window_spec)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "03095cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+-----+------+---+\n",
      "|dep_id|dep_name| id| name|salary|rnk|\n",
      "+------+--------+---+-----+------+---+\n",
      "|     1|      IT|  4|  Max| 90000|  1|\n",
      "|     1|      IT|  1|  Joe| 85000|  2|\n",
      "|     1|      IT|  6|Randy| 85000|  2|\n",
      "|     1|      IT|  7| Will| 70000|  4|\n",
      "|     1|      IT|  5|Janet| 69000|  5|\n",
      "|     2|   Sales|  2|Henry| 80000|  1|\n",
      "|     2|   Sales|  3|  Sam| 60000|  2|\n",
      "+------+--------+---+-----+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "department_df.createOrReplaceTempView('department')\n",
    "employee_df.createOrReplaceTempView('employee')\n",
    "\n",
    "spark.sql('''\n",
    " with one as (select dep_id,dep_name,id,name,salary,departmentId from department \n",
    "                 inner join employee on department.dep_Id == employee.departmentId)\n",
    "             select dep_id,dep_name,id,name,salary,rank() over(partition by departmentId order by salary desc) as rnk\n",
    "                 from one\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ceab79",
   "metadata": {},
   "source": [
    "#### 2 13 Trips and Users H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52f29386",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/2h_trips.csv')\n",
    "             .withColumnRenamed('client_id','cltId')\n",
    "             .withColumnRenamed('driver_id','drId')\n",
    "             .withColumnRenamed('city_id','ctyId')\n",
    "             .withColumnRenamed('request_at','reqat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7040eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/2h_users.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "8c067f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- cltId: integer (nullable = true)\n",
      " |-- drId: integer (nullable = true)\n",
      " |-- ctyId: integer (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- reqat: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trips_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "ce47aa01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- users_id: integer (nullable = true)\n",
      " |-- banned: string (nullable = true)\n",
      " |-- role: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "685410d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = users_df.filter(col('banned') == 'Yes').select('users_id').collect()\n",
    "usrs = [x[i][0] for i in range(0,len(x))]\n",
    "usrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "109cf296",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy('reqat').orderBy('reqat')\n",
    "two_spec = Window.partitionBy('reqat','id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "ed776f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+\n",
      "|       Day|CancellationRate|\n",
      "+----------+----------------+\n",
      "|2013-10-01|            0.33|\n",
      "|2013-10-02|             0.0|\n",
      "|2013-10-03|             0.5|\n",
      "+----------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 948:========================================>            (153 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trips_df.join(users_df,trips_df.id == users_df.users_id,'left_outer')\\\n",
    ".withColumn('totcnt',count('reqat').over(window_spec))\\\n",
    ".withColumn('cnt',when(col('status').like('cancelled%'),count('status').over(window_spec)).otherwise(0))\\\n",
    ".withColumn('bntclt',when((col('cltId').isin(usrs) | col('drId').isin(usrs)),count('id').over(two_spec)).otherwise(0))\\\n",
    ".withColumn('diff',when(col('bntclt')>0,(col('totcnt')-col('bntclt'))).otherwise(0))\\\n",
    ".withColumn('sm',sum('cnt').over(window_spec))\\\n",
    ".filter( (col('bntclt') != 0) & (col('diff') != 0) )\\\n",
    ".withColumn('CancellationRate',when(col('sm') == 0,col('cnt')/col('diff')).otherwise(round(col('bntclt')/col('diff'),2)))\\\n",
    ".select(col('reqat').alias('Day'),'CancellationRate').orderBy(col('reqat').asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "4dd8617c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+\n",
      "|     reqat|CancellationRate|\n",
      "+----------+----------------+\n",
      "|2013-10-01|            0.33|\n",
      "|2013-10-02|             0.0|\n",
      "|2013-10-03|             0.5|\n",
      "+----------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1114:==============================================>     (180 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trips_df.createOrReplaceTempView('trips')\n",
    "users_df.createOrReplaceTempView('users')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select id,cltId,drId,ctyId,status,reqat,users_id,banned,\n",
    "                 count(reqat) over(partition by reqat) as totcnt,\n",
    "                 case when status like 'cancelled%' then count('status') over(partition by reqat) else 0 end cnt,\n",
    "                 case when cltId in (2) then count('id') over(partition by reqat,id) else 0 end bntclt\n",
    "                 from trips left join users on trips.id == users.users_id),\n",
    "       one as (select *,sum(cnt) over(partition by reqat) as sm,\n",
    "                 case when bntclt > 0 then (totcnt - bntclt) else 0 end as diff  from main)\n",
    "               select reqat,case when sm == 0 then round(sm/diff,2) else round(bntclt/diff,2) end CancellationRate \n",
    "                 from one  where bntclt != 0 and diff != 0 order by reqat asc\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75237f61",
   "metadata": {},
   "source": [
    "#### 3 18 Median Employee Salary H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ebb7eac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/3h_employee.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d11b021b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- company: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38e1f8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy('company').orderBy(col('salary').asc())\n",
    "cnt_spec = Window.partitionBy('company')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1b2804ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "| id|company|salary|\n",
      "+---+-------+------+\n",
      "|  6|      A|   513|\n",
      "|  5|      A|   451|\n",
      "| 12|      B|   234|\n",
      "|  9|      B|  1154|\n",
      "| 14|      C|  2645|\n",
      "+---+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('company').orderBy(col('salary').asc())\n",
    "cnt_spec = Window.partitionBy('company')\n",
    "mx_spec =  Window.partitionBy('company','num').orderBy(col('id').asc())\n",
    "\n",
    "employee_df.withColumn('rnk',rank().over(window_spec)).withColumn('cnt',count('company').over(cnt_spec))\\\n",
    ".withColumn('mx',max('rnk').over(cnt_spec))\\\n",
    ".withColumn('num',when( (col('mx')%2 == 0) & ( ((col('mx') - col('mx')/2) == (col('rnk')))\\\n",
    "                                           | ( ((col('mx') - col('mx')/2)+1 == (col('rnk'))) ) ),col('rnk') )\\\n",
    "                 .when( (col('mx')%2 == 1) & ( ceil(col('mx') - col('mx')/2) == col('rnk')),ceil(col('mx') - col('mx')/2) ).otherwise(0))\\\n",
    ".withColumn('ids',when( (col('num') != 0) & (col('mx')%2 == 1), min('id').over(mx_spec)) \\\n",
    "                 .when( (col('num') != 0),col('id')))\\\n",
    ".filter(col('id') == col('ids')).select('id','company','salary').orderBy('company').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c13d7cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "| id|company|salary|\n",
      "+---+-------+------+\n",
      "|  6|      A|   513|\n",
      "|  5|      A|   451|\n",
      "| 12|      B|   234|\n",
      "|  9|      B|  1154|\n",
      "| 14|      C|  2645|\n",
      "+---+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.createOrReplaceTempView('employee')\n",
    "\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select id,company,salary,rank() over(partition by company order by salary asc ) as rnk from employee),\n",
    "       one as (select *,max(rnk) over(partition by company) as mx from main),\n",
    "       two as (select *,case when mx%2 == 0 and ( (mx - ceil(mx/2) == rnk) or (mx - ceil(mx/2)+1 ==rnk ) ) then rnk \n",
    "                             when mx%2 == 1 and (  mx - floor(mx/2) == rnk) then rnk \n",
    "                        else 0 end num from one),\n",
    "     three as (select *,case when num != 0 and mx%2 ==1 then min(id) over(partition by company,num)\n",
    "                             when num != 0 then id else 0 end ids from two)\n",
    "               select id,company,salary from three where id == ids order by company\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0404688",
   "metadata": {},
   "source": [
    "#### 4 20 Find Median Given Frequency of Numbers  H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54a0465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/4h_numbers.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8c767d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- num: integer (nullable = true)\n",
      " |-- frequency: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numbers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "98bf639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {i['num']:i['frequency'] for i in numbers_df.collect()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "4a56043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst =  sorted([i for i,j in dic.items() for k in range(0,j)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "f0257b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median of the given Numbers:0\n"
     ]
    }
   ],
   "source": [
    "str = ''\n",
    "if len(lst)%2 == 0:\n",
    "    even = int(len(lst) - (len(lst)/2))\n",
    "    str =  int((lst[even]+lst[even-1])/2)\n",
    "elif len(lst)%2 == 1:\n",
    "    odd = int(len(lst)/2)\n",
    "    str = odd\n",
    "print(f'Median of the given Numbers:{str}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ce89da",
   "metadata": {},
   "source": [
    "#### 5 28 Find Cumulative Salary of an Employee H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65faeb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/5h_employee.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b903e003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e585021d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 570:===================================>                 (135 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('id').orderBy(col('id').asc(),col('month').desc())\n",
    "\n",
    "df = employee_df.withColumn('diff',col('month') - lead('month',1,0).over(window_spec))\\\n",
    ".withColumn('rnk',rank().over(window_spec)).filter(col('rnk') != 1).select('id','month','salary','diff').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "0bae21eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_months():\n",
    "    lst =  []\n",
    "    lst2 = []\n",
    "    columns = [\"id\", \"month\", \"salary\"]\n",
    "    for i in range(0,len(df)):   \n",
    "        if df[i]['diff'] > 1:\n",
    "            lst.append( (df[i][0],df[i][1],df[i][2],df[i][-1]))\n",
    "\n",
    "    for i in range(0,len(lst)):\n",
    "        for j in range(1,lst[i][3]):\n",
    "            out = (lst[i][0],lst[i][1]-j,0)\n",
    "            lst2.append(out)\n",
    "\n",
    "    return  spark.createDataFrame(lst2, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f4811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy('id').orderBy(col('id').asc(),col('month').desc())\n",
    "bt_spec = Window.partitionBy('id').orderBy(col('id').asc(),col('month').desc()).rowsBetween(0,2)\n",
    "\n",
    "employee_df.withColumn('rnk',rank().over(window_spec)).filter(col('rnk') != 1)\\\n",
    ".select('id','month','salary').union(missing_months()).withColumn('cumulative_sum',sum('salary').over(bt_spec))\\\n",
    ".filter(col('salary') != 0).select('id','month',col('cumulative_sum').alias('salary') ).orderBy(col('id').asc(),col('month').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a207f50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id|month|salary|\n",
      "+---+-----+------+\n",
      "|  1|    7|    90|\n",
      "|  1|    4|   130|\n",
      "|  1|    3|    90|\n",
      "|  1|    2|    50|\n",
      "|  1|    1|    20|\n",
      "|  2|    1|    20|\n",
      "|  3|    3|   100|\n",
      "|  3|    2|    40|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.createOrReplaceTempView('employee')\n",
    "missing_months().createOrReplaceTempView('missing_months')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select id,month,salary,rank() over(partition by id order by id asc,month desc) as rnk from employee),\n",
    "       one as ((select id,month,salary from main where rnk != 1) UNION (select * from missing_months)),\n",
    "       two as (select id,month,salary,sum(salary) over(partition by id order by id asc,month desc ROWS BETWEEN 0 PRECEDING AND 2 FOLLOWING) as cumulative_sum from one)\n",
    "               select id,month,cumulative_sum as salary from two where salary != 0 order by id asc,month desc\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4352a2d4",
   "metadata": {},
   "source": [
    "#### 6 36 Human Traffic of Stadium H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7f83a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stadium_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/6h_stadium.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "3bad0e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- visit_date: string (nullable = true)\n",
      " |-- people: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stadium_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "68369e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_df = stadium_df.collect()\n",
    "\n",
    "def consecutive_id():\n",
    "    lst = []\n",
    "    for i in range(0,len(std_df)):\n",
    "        x = (std_df[i][0],std_df[i][2])\n",
    "        lst.append(x)\n",
    "\n",
    "    id_lst = set()\n",
    "    in_lst = []\n",
    "    cnt = 1\n",
    "    for i in range(0,len(lst)):\n",
    "        if lst[i][1] >= 100:\n",
    "            in_lst.append(lst[i][0])\n",
    "            cnt +=1\n",
    "            if cnt >= 3:\n",
    "                id_lst.update(in_lst)\n",
    "        else:\n",
    "            cnt = 0\n",
    "            in_lst = []\n",
    "    return list(id_lst)\n",
    "\n",
    "def to_dataframe():\n",
    "    return spark.createDataFrame([ (i,) for i in consecutive_id()],['con_id'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "ab348122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+\n",
      "| id|visit_date|people|\n",
      "+---+----------+------+\n",
      "|  5|2017-01-05|   145|\n",
      "|  6|2017-01-06|  1455|\n",
      "|  7|2017-01-07|   199|\n",
      "|  8|2017-01-09|   188|\n",
      "+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stadium_df.filter(col('id').isin(consecutive_id())).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "bd969357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+\n",
      "| id|visit_date|people|\n",
      "+---+----------+------+\n",
      "|  5|2017-01-05|   145|\n",
      "|  6|2017-01-06|  1455|\n",
      "|  7|2017-01-07|   199|\n",
      "|  8|2017-01-09|   188|\n",
      "+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stadium_df.createOrReplaceTempView('stadium')\n",
    "to_dataframe().createOrReplaceTempView(('to_dataframe'))\n",
    "\n",
    "spark.sql('''\n",
    "          select id,visit_date,people \n",
    "              from stadium join to_dataframe on stadium.id == to_dataframe.con_id order by id asc\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cc3ab1",
   "metadata": {},
   "source": [
    "#### 7 45 Average Salary: Departments VS Company H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "43c31ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/7h_salary.csv'))\n",
    "\n",
    "employee_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/7h_employee.csv')\n",
    "             .withColumnRenamed('employee_id','emp_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "33da843e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- pay_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salary_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "cd4cdc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: integer (nullable = true)\n",
      " |-- department_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "97f5e9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 839:=====================================>               (142 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----------+\n",
      "|pay_date|department_id|comparison|\n",
      "+--------+-------------+----------+\n",
      "| 2017-02|            1|      same|\n",
      "| 2017-03|            1|    higher|\n",
      "| 2017-02|            2|      same|\n",
      "| 2017-03|            2|     lower|\n",
      "+--------+-------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 839:====================================================>(199 + 1) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('pay_date')\n",
    "avg_spec = Window.partitionBy('pay_date','department_id')\n",
    "\n",
    "salary_df.join(employee_df,salary_df.employee_id == employee_df.emp_id,'inner' ).orderBy('department_id','emp_id')\\\n",
    ".withColumn(\"pay_date\",to_date(\"pay_date\", \"yyyy/MM/dd\")).withColumn('pay_date',date_format('pay_date','yyyy-MM'))\\\n",
    ".withColumn('sm',round(sum('amount').over(window_spec)/count('amount').over(window_spec),2))\\\n",
    ".withColumn('avg',round(sum('amount').over(avg_spec)/count('amount').over(avg_spec),2))\\\n",
    ".withColumn('comparison',when((col('avg') > col('sm')), lit('higher'))\n",
    "                        .when((col('avg') < col('sm')), lit('lower'))\n",
    "                        .when((col('avg') == col('sm')), lit('same')))\\\n",
    ".select('pay_date','department_id','comparison').distinct().orderBy('department_id','pay_date').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "4bc88cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----------+\n",
      "|pay_date|department_id|comparison|\n",
      "+--------+-------------+----------+\n",
      "| 2017-02|            1|      same|\n",
      "| 2017-03|            1|    higher|\n",
      "| 2017-02|            2|      same|\n",
      "| 2017-03|            2|     lower|\n",
      "+--------+-------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 928:=======================================>             (150 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "salary_df.createOrReplaceTempView('salary')\n",
    "employee_df.createOrReplaceTempView('employee')\n",
    "\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select id,employee_id,amount,date_format(to_date(pay_date, \"yyyy/MM/dd\"),'yyyy-MM') as pay_date,emp_id,department_id from salary\n",
    "                    join employee on employee_id == emp_id),\n",
    "       one as (select pay_date,amount,emp_id,department_id,\n",
    "                  round(sum(amount) over(partition by pay_date)/count(amount) over(partition by pay_date),2)  as sm,\n",
    "                  round(sum(amount) over(partition by pay_date,department_id)/count(amount) over(partition by pay_date,department_id),2)  as avg from main),\n",
    "       two as (select pay_date,department_id,case when avg > sm then 'higher'\n",
    "                                                  when avg < sm then 'lower'\n",
    "                                                  when avg == sm then 'same' end comparison  from one)\n",
    "              select distinct pay_date,department_id,comparison from two order by department_id,pay_date\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ed831b",
   "metadata": {},
   "source": [
    "#### 8 46 Students Report By Geography H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86218d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/8h_student.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2a2d912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c09d47b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = student_df.groupBy().pivot(\"continent\").agg(collect_list(col('name')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "aa56bac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+\n",
      "|America|Asia|Europe|\n",
      "+-------+----+------+\n",
      "|   Jane|  Xi|Pascal|\n",
      "|   Jack|null|  null|\n",
      "+-------+----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 397:==================================================>  (190 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pivot_df = student_df.groupBy().pivot(\"continent\").agg(collect_list(col('name')))\n",
    "\n",
    "america_df = pivot_df.select(posexplode(\"America\")).select('pos',col('col').alias('America'))\n",
    "asia_df = pivot_df.select(posexplode(\"Asia\")).select(col('pos').alias('pos1'),col('col').alias('Asia'))\n",
    "europe_df = pivot_df.select(posexplode(\"Europe\")).select(col('pos').alias('pos1'),col('col').alias('Europe'))\n",
    "\n",
    "america_df.join(asia_df,america_df.pos == asia_df.pos1,'outer').join(europe_df, america_df.pos == europe_df.pos1,'outer')\\\n",
    ".orderBy(col('pos').asc()).drop('pos1','pos').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aaa418",
   "metadata": {},
   "source": [
    "#### 9 62 Game Play Analysis V H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4a83a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/9h_activity.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ecf8c868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- player_id: integer (nullable = true)\n",
      " |-- device_id: integer (nullable = true)\n",
      " |-- event_date: string (nullable = true)\n",
      " |-- games_played: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "b6594dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1017:================================================>   (187 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------+\n",
      "|install_dt|installs|Day1_retention|\n",
      "+----------+--------+--------------+\n",
      "|2016-03-01|       2|           0.5|\n",
      "|2017-06-25|       1|           0.0|\n",
      "+----------+--------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('player_id').orderBy(col('player_id').asc(),col('event_date').asc())\n",
    "cnt_spc =     Window.partitionBy('event_date')\n",
    "\n",
    "one = activity_df.withColumn('rnk',rank().over(window_spec)).filter(col('rnk') == 1).select('player_id','event_date')\n",
    "two = activity_df.withColumn('rnk',rank().over(window_spec)).filter(col('rnk') == 2)\\\n",
    "                     .select(col('player_id').alias('id'),col('event_date').alias('date'))\n",
    "\n",
    "one.join(two,one.player_id == two.id,'outer').withColumn('cnt',count('event_date').over(cnt_spc))\\\n",
    ".withColumn('diff',ceil(round((unix_timestamp(to_date(col('date'),'yyyy-MM-dd')) - unix_timestamp(to_date(col('event_date'),'yyyy-MM-dd')))/86400,0)) )\\\n",
    ".withColumn('agg',when( (col('cnt') >= 2) & (col('diff') == 1),count('event_date').over(cnt_spc) ))\\\n",
    ".withColumn('Day1_retention',when( (col('cnt') >= 2) & (col('diff') == 1),col('diff')/col('agg'))\n",
    "                 .when( (col('cnt') == 1), 0/col('cnt'))).filter(col('Day1_retention').isNotNull())\\\n",
    "                 .select(col('event_date').alias('install_dt'),col('cnt').alias('installs'),'Day1_retention').orderBy(col('install_dt').asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "032efb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1565:=========================================>          (160 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------+\n",
      "|install_dt|installs|Day1_retention|\n",
      "+----------+--------+--------------+\n",
      "|2016-03-01|       2|           0.5|\n",
      "|2017-06-25|       1|           0.0|\n",
      "+----------+--------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "one.createOrReplaceTempView('one')\n",
    "two.createOrReplaceTempView('two')\n",
    "\n",
    "def out():\n",
    "    x = spark.sql('''\n",
    "     with main as (select player_id,event_date,id,date,count(event_date) over(partition by event_date) as cnt,\n",
    "                       ceil(round((unix_timestamp(to_date(date,'yyyy-MM-dd')) - unix_timestamp(to_date(event_date,'yyyy-MM-dd')))/86400,0)) as diff\n",
    "                       from one full outer join two on player_id == id),\n",
    "           one as (select * , case when (cnt >= 2) and (diff == 1) then count(event_date) over(partition by event_date) end as agg  from main),\n",
    "           two as (select *,case when ((cnt >= 2) and (diff == 1)) then diff/agg\n",
    "                                 when  cnt ==1 then 0/cnt end Day1_retention from one)\n",
    "                   select event_date as install_dt,cnt as installs,Day1_retention from two \n",
    "                      where Day1_retention is not null order by event_date asc\n",
    "            ''')\n",
    "    return x\n",
    "\n",
    "out().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9c798e",
   "metadata": {},
   "source": [
    "#### 10 68 User Purchase Platform H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "20f15281",
   "metadata": {},
   "outputs": [],
   "source": [
    "spending_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/10h_spending.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b0a5b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- spend_date: string (nullable = true)\n",
      " |-- platform: string (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spending_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d0d65ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------------+-----------+\n",
      "|spend_date|platform|total_amount|total_users|\n",
      "+----------+--------+------------+-----------+\n",
      "|2019-07-01|  mobile|         100|          1|\n",
      "|2019-07-01| desktop|         100|          1|\n",
      "|2019-07-01|    both|         200|          1|\n",
      "|2019-07-02|  mobile|         100|          1|\n",
      "|2019-07-02| desktop|         100|          1|\n",
      "|2019-07-02|    both|           0|          0|\n",
      "+----------+--------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('user_id','spend_date')\n",
    "one_spec = Window.partitionBy('user_id','spend_date','platform')\n",
    "dt_spec = Window.partitionBy('spend_date').orderBy(col('spend_date').asc())\n",
    "\n",
    "df = spending_df.withColumn('cnt',count('user_id').over(window_spec)).withColumn('cntplt',count('user_id').over(one_spec))\\\n",
    ".withColumn('diff',col('cnt') - col('cntplt'))\\\n",
    ".withColumn('plt',when( col('cnt') > 1, lit('both')).otherwise(col('platform')))\\\n",
    ".withColumn('sm',when(col('cnt') > 1,sum('amount').over(window_spec)).otherwise(col('amount')))\\\n",
    ".select('spend_date','plt','sm','cntplt').distinct().orderBy('spend_date').collect()\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def cnt():\n",
    "    columns = ['spend_date','plt','sm','cntplt']\n",
    "    lst = []\n",
    "    for i in range(0,len(df)):\n",
    "        lst.append(df[i]['spend_date'])\n",
    "        \n",
    "    lst2 = []\n",
    "    date_counts = Counter(lst)\n",
    "    for i,j in date_counts.items():\n",
    "        if j < 3:\n",
    "            x =  (i,'both',0,0)\n",
    "            lst2.append(x)\n",
    "    return spark.createDataFrame(lst2, columns)\n",
    "\n",
    "spending_df.withColumn('cnt',count('user_id').over(window_spec)).withColumn('cntplt',count('user_id').over(one_spec))\\\n",
    ".withColumn('diff',col('cnt') - col('cntplt'))\\\n",
    ".withColumn('plt',when( col('cnt') > 1, lit('both')).otherwise(col('platform')))\\\n",
    ".withColumn('sm',when(col('cnt') > 1,sum('amount').over(window_spec)).otherwise(col('amount')))\\\n",
    ".select('spend_date','plt','sm','cntplt').distinct()\\\n",
    ".select('spend_date',col('plt').alias('platform'),col('sm').alias('total_amount'),col('cntplt').alias('total_users'))\\\n",
    ".orderBy('spend_date','total_amount').union(cnt()).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859b3d56",
   "metadata": {},
   "source": [
    "#### 11 75 Market Analysis II H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de771a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/11h_users.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a2404b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/11h_orders.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3aeab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/11h_items.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4906383f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- join_date: string (nullable = true)\n",
      " |-- favorite_brand: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c9eb2553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- buyer_id: integer (nullable = true)\n",
      " |-- seller_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a90935b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- item_brand: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3cc7ab34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|seller_id|2nd_item_fav_brand|\n",
      "+---------+------------------+\n",
      "|        1|                no|\n",
      "|        2|               yes|\n",
      "|        3|               yes|\n",
      "|        4|                no|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('user_id').orderBy('user_id',col('brand').asc()).rowsBetween(0,2)\n",
    "\n",
    "users_df.join(orders_df,users_df.user_id == orders_df.seller_id,'left')\\\n",
    ".join(items_df,orders_df.item_id == items_df.item_id,'left' )\\\n",
    ".withColumn('brand',when( col('item_brand').isNull(),lit('no'))\\\n",
    "                   .when( col('item_brand') == col('favorite_brand'),lit('yes')).otherwise('no'))\\\n",
    ".withColumn('cnt',count('brand').over(window_spec)).filter(col('cnt') == 1)\\\n",
    ".select(col('user_id').alias('seller_id'),col('brand').alias('2nd_item_fav_brand')).orderBy('seller_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4370952b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|seller_id|2nd_item_fav_brand|\n",
      "+---------+------------------+\n",
      "|        1|                no|\n",
      "|        2|               yes|\n",
      "|        3|               yes|\n",
      "|        4|                no|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.createOrReplaceTempView('users')\n",
    "orders_df.createOrReplaceTempView('orders')\n",
    "items_df.createOrReplaceTempView('items')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select user_id,favorite_brand,orders.item_id,seller_id,item_brand from users\n",
    "                  left join orders on users.user_id == orders.seller_id\n",
    "                  left join items  on items.item_id == orders.item_id),\n",
    "       one as (select user_id,favorite_brand,item_id,seller_id,item_brand,\n",
    "                  case when item_brand is null then 'no'\n",
    "                       when item_brand == favorite_brand then 'yes' else 'no' end brand from main),\n",
    "       two as (select * , count(brand) over(partition by user_id order by user_id,brand asc ROWS BETWEEN 0 PRECEDING AND 2 FOLLOWING) as cnt from one)\n",
    "               select user_id as seller_id,brand as 2nd_item_fav_brand from two where cnt == 1 order by  user_id              \n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c6f634",
   "metadata": {},
   "source": [
    "#### 12 81 Tournament Winners H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f4e4f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/12h_payers.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0677ba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/12h_matches.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abb4bb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- player_id: integer (nullable = true)\n",
      " |-- group_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "players_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34e3e7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- match_id: integer (nullable = true)\n",
      " |-- first_player: integer (nullable = true)\n",
      " |-- second_player: integer (nullable = true)\n",
      " |-- first_score: integer (nullable = true)\n",
      " |-- second_score: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matches_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66d25eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 122:===========================================>         (163 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|group_id|player_id|\n",
      "+--------+---------+\n",
      "|       1|       15|\n",
      "|       2|       35|\n",
      "|       3|       40|\n",
      "+--------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('group_id','player').orderBy('group_id','player')\n",
    "mx_spec = Window.partitionBy('group_id')\n",
    "\n",
    "x = players_df.join(matches_df,players_df.player_id == matches_df.first_player,'inner')\\\n",
    ".select('group_id','match_id',col('second_player').alias('player'),col('second_score').alias('score'))\n",
    "\n",
    "players_df.join(matches_df,players_df.player_id == matches_df.first_player,'inner')\\\n",
    ".select('group_id','match_id',col('first_player').alias('player'),col('first_score').alias('score'))\\\n",
    ".unionAll(x).withColumn('sm',sum('score').over(window_spec)).withColumn('mx',max('sm').over(mx_spec))\\\n",
    ".withColumn('diff',when( col('sm') == col('mx'),1).otherwise(0)).filter(col('diff') ==1 )\\\n",
    ".withColumn('mn',min('player').over(mx_spec)).select('group_id',col('mn').alias('player_id')).distinct().orderBy('group_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8f0890a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 264:====================================================>(199 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|group_id|player_id|\n",
      "+--------+---------+\n",
      "|       1|       15|\n",
      "|       2|       35|\n",
      "|       3|       40|\n",
      "+--------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 265:=========================================>           (157 + 5) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "players_df.createOrReplaceTempView('players')\n",
    "matches_df.createOrReplaceTempView('matches')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select * from players join matches on players.player_id == matches.first_player),\n",
    "       one as ((select group_id,first_player as player,first_score as score from main)\n",
    "                    UNION ALL (select group_id,second_player as player,second_score as score from main)),\n",
    "       two as (select group_id,player,score,sum(score) over(partition by group_id,player order by group_id,player) as sm from one),\n",
    "     three as (select group_id,player,score,sm,max(sm) over(partition by group_id) mx from two),\n",
    "      four as (select group_id,player,score,sm,mx,diff,min(player) over(partition by group_id ) mn from \n",
    "                    (select group_id,player,score,sm,mx,case when sm == mx then 1 else 0 end diff from three)\n",
    "                    where diff == 1)\n",
    "                select distinct group_id,mn as player_id from four order by group_id\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc40867",
   "metadata": {},
   "source": [
    "#### 13 86 Report Contiguous Dates H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "364b0e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/13h_failed.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "37c192ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "succeeded_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/13h_succeeded.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3853ef0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fail_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "failed_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "13e9b3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- success_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "succeeded_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "8a998775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------+\n",
      "|period_state|start_date|  end_date|\n",
      "+------------+----------+----------+\n",
      "|   succeeded|2019-01-01|2019-01-03|\n",
      "|        fail|2019-01-04|2019-01-05|\n",
      "|   succeeded|2019-01-06|2019-01-06|\n",
      "+------------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/15 18:04:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/11/15 18:04:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.orderBy('fail_date','state')\n",
    "sm_spec = Window.partitionBy('state').orderBy('cnt').rangeBetween(-1,1)\n",
    "\n",
    "x = failed_df.withColumn('state',lit('fail')).union(succeeded_df.withColumn('state',lit('succeeded') ))\\\n",
    ".withColumn('sbdate',substring(col('fail_date'),1,4)).filter(col('sbdate') != 2018 )\\\n",
    ".withColumn('cnt',count('fail_date').over(window_spec)).withColumn('mx',max('cnt').over(sm_spec))\\\n",
    ".withColumn('mn',min('cnt').over(sm_spec))\\\n",
    ".withColumn('start_date',when( col('cnt') == col('mn'),col('fail_date')))\\\n",
    ".withColumn('end_date',when(col('cnt') == col('mx'),col('fail_date') ))\\\n",
    ".withColumn('diff',col('mx') -  col('mn')).filter(col('diff') <= 1)\n",
    "\n",
    "one = x.filter(~col('start_date').isNull()).select('state','start_date','diff')\n",
    "two = x.filter(~col('end_date').isNull()).select( col('state').alias('st'),'end_date',col('diff').alias('di'))\n",
    "\n",
    "one.join(two, (one.diff == two.di) & (one.state == two.st) ,'inner')\\\n",
    ".select(col('state').alias('period_state'),'start_date','end_date').orderBy('start_date').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b21f835",
   "metadata": {},
   "source": [
    "#### 14 99 Number of Transactions per Visit H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6c41cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/14h_visits.csv')\n",
    "             .withColumnRenamed('user_id','id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a9545e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/14h_transactions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "0ba94b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- visit_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visits_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ec2e9d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- transaction_date: string (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    " transactions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "13721de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------+\n",
      "|transactions_count|visits_count|\n",
      "+------------------+------------+\n",
      "|                 0|           4|\n",
      "|                 1|           5|\n",
      "|                 2|           0|\n",
      "|                 3|           1|\n",
      "+------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('user_id','transaction_date').orderBy('user_id','transaction_date')\n",
    "cnt_spec = Window.partitionBy('t_count')\n",
    "\n",
    "x = visits_df.join(transactions_df,(visits_df.visit_date == transactions_df.transaction_date) \n",
    "               & (visits_df.id == transactions_df.user_id),'left')\\\n",
    ".withColumn('cnt',count('id').over(window_spec))\\\n",
    ".withColumn('t_count',when(col('amount').isNull(),lit(0))\\\n",
    "                     .when(col('cnt') == 1,lit(1))\\\n",
    "                     .when(col('cnt') == 2,lit(2))\n",
    "                     .when(col('cnt') == 3,lit(3)))\\\n",
    ".withColumn('rnk',rank().over(window_spec))\\\n",
    ".withColumn('bt_count',count('t_count').over(cnt_spec))\\\n",
    ".withColumn('v_count',when( col('t_count') == col('bt_count'),col('rnk'))\\\n",
    "                     .when( col('t_count') != col('bt_count'),col('bt_count')))\\\n",
    ".select('t_count','v_count').distinct().orderBy('t_count').collect()\n",
    "\n",
    "def miss_val():\n",
    "    lst = []\n",
    "    colums = ['t_count','v_count']\n",
    "    for i in range(0,len(x)-1):\n",
    "        if ((x[i+1]['t_count']) - (x[i]['t_count'])) > 1:\n",
    "            out = x[i]['t_count']+1,0\n",
    "            lst.append(out)\n",
    "    return spark.createDataFrame(lst,colums)\n",
    "\n",
    "spark.createDataFrame(x,['t_count','v_count']).union(miss_val())\\\n",
    ".select(col('t_count').alias('transactions_count'),col('v_count').alias('visits_count')).orderBy('transactions_count').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87231c51",
   "metadata": {},
   "source": [
    "#### 15 104  Get the Second Most Recent Activity H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9770144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "useractivity_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/15h_useractivity.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c8a5172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- username: string (nullable = true)\n",
      " |-- activity: string (nullable = true)\n",
      " |-- startDate: string (nullable = true)\n",
      " |-- endDate: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "useractivity_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7d567f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----------+----------+\n",
      "|username|activity| startDate|   endDate|\n",
      "+--------+--------+----------+----------+\n",
      "|   Alice| Dancing|2020-02-21|2020-02-23|\n",
      "|     Bob|  Travel|2020-02-11|2020-02-18|\n",
      "+--------+--------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 78:=============================================>        (169 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('username').orderBy('username','startDate')\n",
    "lag_spec =  Window.partitionBy('username')\n",
    "\n",
    "useractivity_df.withColumn('rnk',rank().over(window_spec)).withColumn('cnt',count('username').over(lag_spec))\\\n",
    ".withColumn('fil',when( (col('rnk') == 1) &  (col('cnt') == 1 ),lit(1))\n",
    "                 .when( (col('rnk') == 2) &  (col('cnt') >= 2 ),lit(1)).otherwise(lit(0)))\\\n",
    ".filter(col('fil') == 1).select('username','activity','startDate','endDate').orderBy('username').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8208dda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----------+----------+\n",
      "|username|activity| startDate|   endDate|\n",
      "+--------+--------+----------+----------+\n",
      "|   Alice| Dancing|2020-02-21|2020-02-23|\n",
      "|     Bob|  Travel|2020-02-11|2020-02-18|\n",
      "+--------+--------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "useractivity_df.createOrReplaceTempView('useractivity')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select username,activity,startDate,endDate,rank() over(partition By username order by username,startDate) as rnk,\n",
    "                    count(username) over(partition By username) as cnt from useractivity),\n",
    "       one as (select * ,case when rnk == 1 and cnt == 1 then 1\n",
    "                            when rnk == 2 and cnt >=  2 then 1 else 0 end fil from main)\n",
    "               select username,activity,startDate,endDate from one where fil == 1 order by username\n",
    "       ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5873fb3c",
   "metadata": {},
   "source": [
    "#### 16 106 Total Sales Amount by Year H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9ae6c6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_info = StructType([StructField('product_id',IntegerType()),\n",
    "                         StructField('period_start',DateType()),\n",
    "                         StructField('period_end',DateType()),\n",
    "                         StructField('average_daily_sales',IntegerType())] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a17a7443",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/16h_product.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cd01fe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sales_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .schema(schema_info)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/16h_sales.csv')\n",
    "             .withColumn('period_start',unix_timestamp('period_start'))\n",
    "             .withColumn('period_end',unix_timestamp('period_end'))\n",
    "             .withColumnRenamed('product_id','id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d207f9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "afb7df0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- period_start: long (nullable = true)\n",
      " |-- period_end: long (nullable = true)\n",
      " |-- average_daily_sales: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "b8daa914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+------------+\n",
      "|product_id|product_name|report_year|total_amount|\n",
      "+----------+------------+-----------+------------+\n",
      "|         1|    LC Phone|       2019|        3500|\n",
      "|         2|  LC T-Shirt|       2018|         310|\n",
      "|         2|  LC T-Shirt|       2019|        3650|\n",
      "|         2|  LC T-Shirt|       2020|          10|\n",
      "|         3| LC Keychain|       2019|          31|\n",
      "|         3| LC Keychain|       2020|          31|\n",
      "+----------+------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.withColumn('diff',(col('period_end') - col('period_start'))/86400  )\\\n",
    ".withColumn('start',from_unixtime(col('period_start'),'yyyy-MM-dd'))\\\n",
    ".withColumn('end',  from_unixtime(col('period_end'),'yyyy-MM-dd'))\\\n",
    ".select('id',col('average_daily_sales').alias('sales'),'start','end')\\\n",
    ".withColumn(\"start_year\", year(\"start\")) \\\n",
    "    .withColumn(\"end_year\", year(\"end\")) \\\n",
    "    .withColumn(\"years\", expr(\"sequence(start_year, end_year)\"))\\\n",
    "    .selectExpr(\"id\", \"explode(years) as report_year\", \"start\", \"end\", \"sales\")\\\n",
    ".withColumn(\"start_date\",greatest(to_date(col('start'),'yyyy-MM-dd'),to_date(expr(\"concat(report_year, '-01-01')\"),'yyyy-MM-dd')))\\\n",
    ".withColumn(\"end_date\",least(to_date(col('end'),'yyyy-MM-dd'),to_date(expr(\"concat(report_year, '-12-31')\"),'yyyy-MM-dd')))\\\n",
    ".withColumn(\"days\", datediff(col(\"end_date\"), col(\"start_date\")) + lit(1))\\\n",
    ".withColumn(\"total_amount\", col(\"days\") * col(\"sales\"))\\\n",
    ".join(product_df,product_df.product_id == sales_df.id,'inner')\\\n",
    ".select('product_id','product_name','report_year','total_amount').orderBy('product_id','report_year')\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1e0fff",
   "metadata": {},
   "source": [
    "#### 17 110 Find the Quiet Students in All Exams H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "33d1601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exam_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/17h_exam.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e3cd3300",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/17h_student.csv')\n",
    "             .withColumnRenamed('student_id','id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "c344862d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- exam_id: integer (nullable = true)\n",
      " |-- student_id: integer (nullable = true)\n",
      " |-- score: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exam_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "3416b384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- student_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "6143c375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|student_id|student_name|\n",
      "+----------+------------+\n",
      "|         2|        Jade|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spc = Window.partitionBy('exam_id')\n",
    "\n",
    "mx = exam_df.withColumn('mx',max('score').over(window_spc)).filter(col('score') == col('mx')).select('student_id')\n",
    "mn = exam_df.withColumn('mn',min('score').over(window_spc)).filter(col('score') == col('mn')).select('student_id')\n",
    "\n",
    "min_max = set([mx.union(mn).collect()[i]['student_id'] for i in range(0,len(mx.union(mn).collect()))] )\n",
    "\n",
    "min_max\n",
    "exam_df.filter(~col('student_id').isin(min_max)).join(student_df,student_df.id == exam_df.student_id,'inner')\\\n",
    ".select('student_id','student_name').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953c9009",
   "metadata": {},
   "source": [
    "#### 18 118 Sales by Day of the Week H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "622c5c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/18h_orders.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "779dca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/18h_items.csv')\n",
    "             .withColumnRenamed('item_id','id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "f93533b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "69bd80d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- item_name: string (nullable = true)\n",
      " |-- item_category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "3956a9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 540:================================================>    (184 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-------+---------+--------+------+--------+------+\n",
      "|category|Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday|\n",
      "+--------+------+-------+---------+--------+------+--------+------+\n",
      "|    Book|    20|      5|        0|       0|    10|       0|     0|\n",
      "| Glasses|     0|      0|        0|       0|     5|       0|     0|\n",
      "|   Phone|     0|      0|        5|       1|     0|       0|    10|\n",
      "| T-Shirt|     0|      0|        0|       0|     0|       0|     0|\n",
      "+--------+------+-------+---------+--------+------+--------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('days','item_category')\n",
    "\n",
    "items_df.join(orders_df,orders_df.item_id == items_df.id,'outer').select('item_category','order_date','quantity')\\\n",
    ".withColumn('day',dayofweek('order_date')).withColumn('days',when(col('day') == 1,lit('Sunday'))\n",
    "                                                            .when(col('day') == 2,lit('Monday'))\n",
    "                                                            .when(col('day') == 3,lit('Tuesday'))\n",
    "                                                            .when(col('day') == 4,lit('Wednesday'))\n",
    "                                                            .when(col('day') == 5,lit('Thursday'))                                                            .when(col('day') == 2,lit('Monday'))\n",
    "                                                            .when(col('day') == 6,lit('Friday'))\n",
    "                                                            .when(col('day') == 7,lit('Saturday')) )\\\n",
    ".withColumn('qty',sum('quantity').over(window_spec)).select('item_category','days','qty').distinct()\\\n",
    ".groupBy('item_category').pivot('days',[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]).sum('qty')\\\n",
    ".fillna(0).withColumnRenamed('item_category','category').orderBy('category').show()\n",
    "                               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc95a172",
   "metadata": {},
   "source": [
    "#### 19 250 CEO Subordinate Hierarchy H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "313346af",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/19h_employees.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e984a670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- manager_id: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "88421db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+---------------+------+\n",
      "|subordinate_id|subordinate_name|hierarchy_level|salary|\n",
      "+--------------+----------------+---------------+------+\n",
      "|             2|             Bob|              1|-30000|\n",
      "|             3|         Charlie|              1|-40000|\n",
      "|             4|           David|              2|-45000|\n",
      "|             5|             Eve|              2|-50000|\n",
      "|             6|           Frank|              2|-55000|\n",
      "|             7|           Grace|              2|-52000|\n",
      "|             8|           Helen|              3|-60000|\n",
      "+--------------+----------------+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.alias('e').join(employees_df.alias('m'),col('e.employee_id') == col('m.manager_id'),'inner')\\\n",
    ".select(col('m.employee_id').alias('subordinate_id'),col('m.employee_name').alias('subordinate_name'),\n",
    "       when( col('e.manager_id').isNull(),lit(1))\n",
    "      .when( col('e.manager_id') == 1, lit(2))\n",
    "      .when( col('e.manager_id') == 2, lit(3)).alias('hierarchy_level'),\n",
    "      (col('m.salary') - lit(150000) ).alias('salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af596687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
