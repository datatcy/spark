{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14d551ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('database').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "611dd748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (col,expr,count,countDistinct,datediff,to_date,date_add,year,month,lag,lead,rank,max,min,round,\n",
    "        sum,when,lit,desc,coalesce,abs,greatest,least,array,array_sort,substring, explode,collect_list,array_intersect,\n",
    "        unix_timestamp,rank,dense_rank,least,greatest,row_number,array_join,expr,trim,lower,array,sort_array,\n",
    "        array_distinct,size,initcap,length,date_format,to_timestamp,concat,regexp_extract,length,regexp_replace,\n",
    "        sum as spark_sum,sqrt,length,last,abs,avg,flatten,to_timestamp,array_contains,array_intersect,array_union,\n",
    "        array_distinct,element_at,array_remove,row_number,ceil,floor,dayofweek,monotonically_increasing_id,last,first,\n",
    "        asc_nulls_last,hours,hour,aggregate,weekofyear,dayofmonth,date_sub,array_distinct,percentile_approx,from_unixtime,\n",
    "        lpad,rpad,array_intersect,array_except,dayofmonth,dayofyear,weekofyear,month,quarter,year,last_day\n",
    "                                  )\n",
    "from pyspark.sql.types import (StructField,StructType,\n",
    "                    IntegerType,StringType,DateType,TimestampType )\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import Row\n",
    "#rlike,contains,array_join,collect_list,substring,array_size,cast,stack(to unpivot),LATERAL VIEW \n",
    "#Window.orderBy(\"id\").rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "#Window.partitionBy(\"category\").orderBy(\"id\").rangeBetween(Window.currentRow, 1)\n",
    "#row_number(), rank, dense_rank(), sum(), count(), max(), min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20e98d2",
   "metadata": {},
   "source": [
    "#### 1 2 Second Highest Salary M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa8cbd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/1m_employee.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1142d4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "edf6b7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|second_high_salary|\n",
      "+------------------+\n",
      "|              null|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 20:47:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.orderBy(col('salary').desc())\n",
    "\n",
    "employee_df.withColumn('rnk',rank().over(window_spec))\\\n",
    ".withColumn('rnksalary',when((col('rnk') == 2),col('salary'))).select(max(col('rnksalary')).alias('second_high_salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fa81a4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|second_high_salary|\n",
      "+------------------+\n",
      "|               200|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 21:26:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "employee_df.createOrReplaceTempView('employeedf')\n",
    "\n",
    "spark.sql('''\n",
    "            select max(rnksalary) as second_high_salary from \n",
    "           (select salary, case when rnk = 2 then salary\n",
    "                               else null end  as rnksalary from\n",
    "           (select salary,rank() over(order by salary desc) rnk from employeedf))\n",
    "\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70393aac",
   "metadata": {},
   "source": [
    "#### 2 3 Nth Highest Salary M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a403e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/2m_employee.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "edb8dcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "90dcd4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|second_high_salary|\n",
      "+------------------+\n",
      "|              null|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 21:27:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.orderBy(col('salary').desc())\n",
    "n = 0\n",
    "\n",
    "employee_df.withColumn('rnk',rank().over(window_spec))\\\n",
    ".withColumn('rnksalary',when((col('rnk') == n),col('salary'))).select(max(col('rnksalary')).alias('second_high_salary')).show()\\\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0a2f5209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|second_high_salary|\n",
      "+------------------+\n",
      "|               300|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 21:30:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "employee_df.createOrReplaceTempView('employeedf')\n",
    "\n",
    "n = 1\n",
    "\n",
    "spark.sql(f'''\n",
    "            select max(rnksalary) as second_high_salary from \n",
    "           (select salary, case when rnk = {n} then salary\n",
    "                               else null end  as rnksalary from\n",
    "           (select salary,rank() over(order by salary desc) rnk from employeedf))\n",
    "\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596487e8",
   "metadata": {},
   "source": [
    "#### 3 4 Rank Scores M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "422620fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/3m_scores.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "daee6442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- score: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d6781784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| id|score|rnk|\n",
      "+---+-----+---+\n",
      "|  3|  4.0|  1|\n",
      "|  5|  4.0|  1|\n",
      "|  4| 3.85|  2|\n",
      "|  2| 3.65|  3|\n",
      "|  6| 3.65|  3|\n",
      "|  1|  3.5|  4|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 21:17:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "window_spec  = Window.orderBy(col('score').desc())\n",
    "\n",
    "scores_df.withColumn('rnk',dense_rank().over(window_spec)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1b8d41b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|score|rnk|\n",
      "+-----+---+\n",
      "|  4.0|  1|\n",
      "|  4.0|  1|\n",
      "| 3.85|  2|\n",
      "| 3.65|  3|\n",
      "| 3.65|  3|\n",
      "|  3.5|  4|\n",
      "+-----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 21:34:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "scores_df.createOrReplaceTempView('scoresdf')\n",
    "\n",
    "spark.sql('''\n",
    "          select score,dense_rank() over(order by score desc) as rnk from scoresdf\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f06b8c",
   "metadata": {},
   "source": [
    "#### 4 5 Consecutive Numbers M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5423335d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/4m_logs.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "d8f76ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- num: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "2ae70c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|num|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 22:39:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.orderBy(col('id').asc())\n",
    "\n",
    "with_group_id = logs_df.withColumn('prev_num',lag(col('num'),1,0).over(window_spec))\\\n",
    ".withColumn('next_num',lead(col('num'),1,0).over(window_spec))\\\n",
    ".filter(\n",
    "    (col(\"num\") == col(\"prev_num\")) &\n",
    "    (col(\"num\") == col(\"next_num\"))\n",
    ").select('num').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "228998a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|num|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 22:49:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "logs_df.createOrReplaceTempView('logsdf')\n",
    "\n",
    "spark.sql(''' \n",
    "         select num from \n",
    "         (select num, lag(num,1,0) over(order by id asc) as pre_num, \n",
    "                   lead(num,1,0) over(order by id asc) as next_num from logsdf) \n",
    "                   where num == pre_num and num == next_num\n",
    "        '''\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4baab7",
   "metadata": {},
   "source": [
    "#### 5 9 Department Highest Salary M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7292e4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/5m_employee.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "517d9a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "department_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/5m_department.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "ae9d3c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- departmentId: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "66162f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "department_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "de6cf4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------+\n",
      "|department|employee|salary|\n",
      "+----------+--------+------+\n",
      "|     Sales|   Henry| 80000|\n",
      "|        IT|     Jim| 90000|\n",
      "|        IT|     Max| 90000|\n",
      "+----------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy(department_df.name)\n",
    "\n",
    "employee_df.join(department_df,employee_df.departmentId == department_df.id,'inner')\\\n",
    ".withColumn('maxsalary',max(employee_df.salary).over(window_spec) )\\\n",
    ".filter(col('salary') == col('maxsalary'))\\\n",
    ".select( (department_df.name).alias('department'),(employee_df.name).alias('employee'),'salary' ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "fa7a4945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+\n",
      "|department| name|salary|\n",
      "+----------+-----+------+\n",
      "|        IT|  Jim| 90000|\n",
      "|     Sales|Henry| 80000|\n",
      "|        IT|  Max| 90000|\n",
      "+----------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.createOrReplaceTempView('employeedf')\n",
    "department_df.createOrReplaceTempView('departmentdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with maxl as (select max(salary) over(partition by id) as salary from\n",
    "                      (select employeedf.salary as salary,departmentdf.id as id  from employeedf\n",
    "                      join departmentdf on employeedf.departmentId = departmentdf.id))\n",
    "            select departmentdf.name as department,employeedf.name,employeedf.salary as salary from employeedf\n",
    "                      join departmentdf on employeedf.departmentId = departmentdf.id\n",
    "                      where salary in (select * from maxl)\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800e9107",
   "metadata": {},
   "source": [
    "#### 6 16 Game Play Analysis III M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fd42fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/6m_activity.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83281a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- player_id: integer (nullable = true)\n",
      " |-- device_id: integer (nullable = true)\n",
      " |-- event_date: string (nullable = true)\n",
      " |-- games_played: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ba1ccc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------------------+\n",
      "|player_id|event_date|games_played_so_far|\n",
      "+---------+----------+-------------------+\n",
      "|        1|2016-03-01|                  5|\n",
      "|        1|2016-05-02|                 11|\n",
      "|        1|2017-06-25|                 12|\n",
      "|        3|2016-03-02|                  0|\n",
      "|        3|2018-07-03|                  5|\n",
      "+---------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('player_id').orderBy(col('event_date').asc())\n",
    "\n",
    "activity_df.withColumn('games_played_so_far',sum('games_played').over(window_spec))\\\n",
    ".select('player_id','event_date','games_played_so_far').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c25886bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------------------+\n",
      "|player_id|event_date|games_played_so_far|\n",
      "+---------+----------+-------------------+\n",
      "|        1|2016-03-01|                  5|\n",
      "|        1|2016-05-02|                 11|\n",
      "|        1|2017-06-25|                 12|\n",
      "|        3|2016-03-02|                  0|\n",
      "|        3|2018-07-03|                  5|\n",
      "+---------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity_df.createOrReplaceTempView('activitydf')\n",
    "\n",
    "\n",
    "spark.sql('''\n",
    "\n",
    "         select player_id,event_date,sum(games_played) over(partition By player_id order By event_date) as games_played_so_far\n",
    "              from activitydf\n",
    "         ''').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce2dd95",
   "metadata": {},
   "source": [
    "#### 7 17 Game Play Analysis IV M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afa61d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/7m_activity.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffb54e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- player_id: integer (nullable = true)\n",
      " |-- device_id: integer (nullable = true)\n",
      " |-- event_date: string (nullable = true)\n",
      " |-- games_played: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "29559de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|fraction|\n",
      "+--------+\n",
      "|    0.33|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('player_id').orderBy(col('event_date').asc())\n",
    "\n",
    "num_of_players = activity_df.select(countDistinct('player_id').alias('num_of_players')).collect()[0][0]\n",
    "\n",
    "activity_df.withColumn('lead',lead('event_date',1).over(window_spec))\\\n",
    ".withColumn('datediff',datediff('lead','event_date'))\\\n",
    ".withColumn('fraction',when((col('datediff') == 1) ,round((col('datediff').cast('double')/ num_of_players),2)))\\\n",
    ".select(col('fraction')).dropna().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "81be68c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|fraction|\n",
      "+--------+\n",
      "|    0.33|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity_df.createOrReplaceTempView('activitydf')\n",
    "\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select player_id,event_date,lead(event_date) over(partition by player_id order by event_date asc) as lead_date\n",
    "                    from activitydf),\n",
    "       one as (select count(distinct player_id) as cnt, '1' as id from main),\n",
    "       two as (select  datediff(lead_date,event_date) as date_dff, '1' as id from main)\n",
    "               select round(date_dff/cnt,2)  as fraction from one join two on one.id = two.id where date_dff = 1\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93586e9",
   "metadata": {},
   "source": [
    "#### 8 19 Managers with at Least 5 Direct Reports M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9530a44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/8m_employee.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f7586ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- managerId: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "e2ed4212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|John|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('managerId')\n",
    "mrg_count_df = employee_df.filter(~col('managerId').isNull()).withColumn('mgr_count',count('managerId').over(window_spec))\\\n",
    ".select('managerId','mgr_count')\n",
    "\n",
    "emp_df = employee_df.filter(col('managerId').isNull()).select('id','name')\n",
    "\n",
    "emp_df.join(mrg_count_df,mrg_count_df.managerId == emp_df.id,'inner').filter(col('mgr_count') >= 5).select('name').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "671545d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|John|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.createOrReplaceTempView('employeedf')\n",
    "\n",
    "spark.sql('''\n",
    "  with emp_id as (select managerId as mgr_id from employeedf where managerId is not null),\n",
    "          mgr as (select name, id from employeedf where managerId is null)\n",
    "                  select name from mgr join emp_id on mgr.id = emp_id.mgr_id group by name having count(name) >= 5 \n",
    "        ''').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9b8cb3",
   "metadata": {},
   "source": [
    "#### 9 25 Winning Candidate M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4bc6e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/9m_candidate.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c284f8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/9m_vote.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "1ad08715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "candidate_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "f68f7429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- candidateId: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vote_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "8aa08131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B'"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('name')\n",
    "result_df = candidate_df.join(vote_df,candidate_df.id == vote_df.candidateId,'inner')\\\n",
    ".withColumn('candiate_cnt',count('name').over(window_spec)).orderBy(col('candiate_cnt').desc())\n",
    "\n",
    "result_df.first()[1][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "8a8bc85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|   B|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "candidate_df.createOrReplaceTempView('candidatedf')\n",
    "vote_df.createOrReplaceTempView('votedf')\n",
    "\n",
    "spark.sql('''\n",
    "   with main as (select name,count(name) as cnt from candidatedf join votedf on candidatedf.id = votedf.candidateId\n",
    "                  group by name),\n",
    "         one as (select max(cnt) mx from main)\n",
    "                 select name from main where cnt in (select mx from one)\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf304801",
   "metadata": {},
   "source": [
    "#### 10 27 et Highest Answer Rate Question M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3a10fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "surveylog_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/10m_surveylog.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "9d34f359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- action: string (nullable = true)\n",
      " |-- question_id: integer (nullable = true)\n",
      " |-- answer_id: integer (nullable = true)\n",
      " |-- q_num: integer (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "surveylog_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "cb675938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|survey_log|\n",
      "+----------+\n",
      "|       285|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('qid')\n",
    "\n",
    "answer_df = surveylog_df.filter(col('action') == lit('answer')).select(col('question_id').alias('qid'))\n",
    "\n",
    "answer_df.join(surveylog_df,surveylog_df.question_id == answer_df.qid,'inner').select(col('qid')).distinct()\\\n",
    ".withColumn('cnt',count('qid').over(window_spec)).orderBy(col('cnt').desc(),col('qid').asc())\\\n",
    ".select(col('qid').alias('survey_log')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "4841a754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|question_id|\n",
      "+-----------+\n",
      "|        285|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "surveylog_df.createOrReplaceTempView('surveylogdf')\n",
    "\n",
    "spark.sql('''\n",
    "          select question_id from\n",
    "          (select question_id,count(question_id) over(partition by question_id, action) as surlog from surveylogdf\n",
    "              where action = \"answer\" order by surlog desc,question_id asc)\n",
    "       ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f54090",
   "metadata": {},
   "source": [
    "#### 11 29 Count Student Number in Departments M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91069259",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/11m_student.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e145f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "department_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/11m_department.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "300c9fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- student_id: integer (nullable = true)\n",
      " |-- student_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- dept_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a3710d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept_id: integer (nullable = true)\n",
      " |-- dept_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "department_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d122dce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|  dept_name|student_number|\n",
      "+-----------+--------------+\n",
      "|Engineering|             2|\n",
      "|    Science|             1|\n",
      "|        Law|             0|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 24:============================================>         (164 + 6) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "student_df.join(department_df,student_df.dept_id == department_df.dept_id,'right')\\\n",
    ".groupBy('dept_name').agg(count(student_df.dept_id).alias('student_number'))\\\n",
    ".orderBy(col('student_number').desc(),col('dept_name').asc()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9242efe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|  dept_name|student_number|\n",
      "+-----------+--------------+\n",
      "|Engineering|             2|\n",
      "|    Science|             1|\n",
      "|        Law|             0|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "department_df.createOrReplaceTempView('departmentdf')\n",
    "student_df.createOrReplaceTempView('studentdf')\n",
    "\n",
    "spark.sql('''\n",
    "         select departmentdf.dept_name,count(studentdf.dept_id) as student_number from studentdf\n",
    "              right join departmentdf on studentdf.dept_id = departmentdf.dept_id \n",
    "              group by departmentdf.dept_name order by student_number desc,dept_name asc\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee1fa6f",
   "metadata": {},
   "source": [
    "#### 12 31 Investments in 2016 M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0928ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/12m_insurance.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ffd459d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pid: integer (nullable = true)\n",
      " |-- tiv_2015: integer (nullable = true)\n",
      " |-- tiv_2016: integer (nullable = true)\n",
      " |-- lat: integer (nullable = true)\n",
      " |-- lon: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "insurance_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0bda2be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|tiv_2016|\n",
      "+--------+\n",
      "|    45.0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('tiv_2015')\n",
    "window_lon= Window.partitionBy('lon')\n",
    "\n",
    "insurance_df.withColumn('gp2015',count('tiv_2015').over(window_spec))\\\n",
    ".withColumn('gplat',count('lat').over(window_lat))\\\n",
    ".withColumn('gplon',count('lon').over(window_lon)).filter( (col('gp2015') > 1) & (col('gplat') == 1) )\\\n",
    ".select(round(sum(col('tiv_2016')).cast('float'),2).alias('tiv_2016')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f0ac07db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|tiv_2016|\n",
      "+--------+\n",
      "|    45.0|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 243:====================================================>(198 + 2) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "insurance_df.createOrReplaceTempView('insurancedf')\n",
    "\n",
    "spark.sql('''\n",
    "          \n",
    "          select round(cast(sum(tiv_2016) as float),2) as tiv_2016 from \n",
    "          (select tiv_2016,count(tiv_2015) over(partition by tiv_2015) as gp2015, count(lat) over(partition by lat) as gplat\n",
    "               from insurancedf) where gp2015 > 1 and gplat = 1\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7190901",
   "metadata": {},
   "source": [
    "#### 13 37 Friend Requests II: Who Has the Most Friends M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7202dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "requestaccepted_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/13m_requestaccepted.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1b01017d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- requester_id: integer (nullable = true)\n",
      " |-- accepter_id: integer (nullable = true)\n",
      " |-- accept_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "requestaccepted_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "39e12d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|accepter_id|count|\n",
      "+-----------+-----+\n",
      "|          3|    3|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('accepter_id')\n",
    "\n",
    "req_df = requestaccepted_df.withColumn('gp',count('accepter_id').over(window_spec)).filter(col('gp') == 1)\\\n",
    ".select( lit('0').alias('requester_id'),col('requester_id').alias('accepter_id'))\n",
    "\n",
    "acc_df = requestaccepted_df.withColumn('gp',count('accepter_id').over(window_spec)).filter(col('gp') > 1)\\\n",
    ".select(lit('0').alias('requester_id'),col('accepter_id'))\n",
    "\n",
    "req_df.union(acc_df).select('accepter_id').groupby('accepter_id').agg(count(col('accepter_id')).alias('count'))\\\n",
    ".filter(col('count') > 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4124480f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----------+\n",
      "|requester_id|accepter_id|accept_date|\n",
      "+------------+-----------+-----------+\n",
      "|           1|          2| 2016/06/03|\n",
      "|           1|          3| 2016/06/08|\n",
      "|           2|          3| 2016/06/08|\n",
      "|           3|          4| 2016/06/09|\n",
      "+------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "requestaccepted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "631f19b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|accepter_id|count|\n",
      "+-----------+-----+\n",
      "|          3|    3|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "requestaccepted_df.createOrReplaceTempView('requestaccepteddf')\n",
    "\n",
    "spark.sql(''' \n",
    "  with acc as (select '0' as requester_id, requester_id as accepter_id from \n",
    "                    (select requester_id,accepter_id,count(accepter_id) over(partition by accepter_id) as cnt from requestaccepteddf)\n",
    "               where cnt == 1),\n",
    "       req as (select '0' as  requester_id,accepter_id from \n",
    "                    (select requester_id,accepter_id,count(accepter_id) over(partition by accepter_id) as cnt from requestaccepteddf)\n",
    "               where cnt > 1)\n",
    "              select accepter_id,count(accepter_id) as count from \n",
    "                   ((select * from req) union all (select * from acc))\n",
    "              group by accepter_id having count > 1\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f752e6",
   "metadata": {},
   "source": [
    "#### 14 40 Tree Node M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51db8ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/14m_tree.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e30b9539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- p_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "6699e808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| type|\n",
      "+---+-----+\n",
      "|  1| Root|\n",
      "|  3|leaff|\n",
      "|  4|leaff|\n",
      "|  5|leaff|\n",
      "|  2|Inner|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "root = tree_df.filter(col('p_id').isNull()).select('id',lit('Root').alias('type'))\n",
    "\n",
    "x = tree_df.filter(~col('p_id').isNull()).select('id')\n",
    "y = tree_df.filter(~col('p_id').isNull()).select('p_id').collect()\n",
    "\n",
    "leaf = x.filter(~col('id').isin(set([y[i][0] for i in range(0,len(y))]))).select('id',lit('leaff').alias('type'))\n",
    "\n",
    "both = root.union(leaf).select('id').collect()\n",
    "\n",
    "inner = tree_df.filter(~col('id').isin([ both[i][0] for i in range(0,len(both))])).select('id',lit('Inner').alias('type'))\n",
    "\n",
    "root.union(leaf).union(inner).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "bdbefd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| type|\n",
      "+---+-----+\n",
      "|  1| Root|\n",
      "|  3| leaf|\n",
      "|  4| leaf|\n",
      "|  5| leaf|\n",
      "|  2|Inner|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree_df.createOrReplaceTempView('treedf')\n",
    "\n",
    "spark.sql('''\n",
    "  with main as ( (select id,'Root' as type from treedf where p_id is null) union all\n",
    "                 (select id,'leaf' as type from treedf where id not in\n",
    "                 (select p_id from treedf where p_id is not null)) ),\n",
    "      inner as (select id,'Inner' as Type from treedf where id  not in (select id from main))\n",
    "               (select * from main) union all (select * from inner)\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a449aa",
   "metadata": {},
   "source": [
    "#### 15 42 Shortest Distance in a Plane M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a958d227",
   "metadata": {},
   "outputs": [],
   "source": [
    "point2d_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/15m_point2d.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "098b1f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: integer (nullable = true)\n",
      " |-- y: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "point2d_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "4e01665c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+--------+\n",
      "|  x|  y|  x|  y|distance|\n",
      "+---+---+---+---+--------+\n",
      "| -1| -1| -1| -2|     1.0|\n",
      "+---+---+---+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "point2d_df.alias('df1').crossJoin(point2d_df.alias('df2'))\\\n",
    ".withColumn(\n",
    "    \"distance\",\n",
    "    sqrt(\n",
    "        pow(col(\"df2.x\") - col(\"df1.x\"), 2) + pow(col(\"df2.y\") - col(\"df1.y\"), 2)\n",
    "    )\n",
    ").filter(col('distance') > 0 ).orderBy('distance').limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "ef0587bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+--------+\n",
      "| x1| y1| x2| y2|distance|\n",
      "+---+---+---+---+--------+\n",
      "| -1| -1| -1| -2|     1.0|\n",
      "+---+---+---+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "point2d_df.createOrReplaceTempView('point2ddf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select x1,y1,x2,y2,sqrt(pow((x2 - x1),2) + pow((y2 - y1),2)) as distance  from \n",
    "                    (select df1.x as x1,df1.y as y1,df2.x as x2,df2.y as y2  from point2ddf as df1 cross join point2ddf as df2))\n",
    "               select x1,y1,x2,y2,distance from main where distance > 0 order by distance limit 1\n",
    "\n",
    "\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86cf199",
   "metadata": {},
   "source": [
    "#### 16 44 Second Degree Follower M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "162051d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "follow_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/16m_follow.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "659995eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- followee: string (nullable = true)\n",
      " |-- follower: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "follow_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "fbd85eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|followee|num|\n",
      "+--------+---+\n",
      "|     Bob|  2|\n",
      "|  Donald|  1|\n",
      "+--------+---+\n",
      "\n",
      "+--------+--------+\n",
      "|followee|follower|\n",
      "+--------+--------+\n",
      "|   Alice|     Bob|\n",
      "|     Bob|  Donald|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = follow_df.select('follower').collect()\n",
    "\n",
    "follower_num = follow_df.filter(col('followee').isin([ x[i][0] for i in range(0,len(x))]))\\\n",
    ".groupBy('followee').agg(count(col('follower')).alias('num'))\n",
    "follower_num.show()\n",
    "\n",
    "y = follower_num.select('followee').collect()\n",
    "follow_df.filter(col('follower').isin([ y[i][0] for i in range(0,len(y))])).select('followee','follower').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "51f2db92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|followee|\n",
      "+--------+\n",
      "|   Alice|\n",
      "|     Bob|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "follow_df.createOrReplaceTempView('followdf')\n",
    "\n",
    "\n",
    "spark.sql('''\n",
    "  with num_followee as (select followee,count(*) as num from followdf where followee in (select follower from followdf)\n",
    "                             group by followee)\n",
    "                       select followee from followdf where follower in (select followee from num_followee)\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19df9992",
   "metadata": {},
   "source": [
    "#### 17 49 Exchange Seats M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a610c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seat_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/17m_seat.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "74ec48ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- student: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seat_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "951fb29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|student|\n",
      "+---+-------+\n",
      "|  1|  Doris|\n",
      "|  2|  Abbot|\n",
      "|  3|  Green|\n",
      "|  4|Emerson|\n",
      "|  5| Jeames|\n",
      "+---+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/04 22:07:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.orderBy(\"id\").rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing) # (0,10)\n",
    "\n",
    "seat_df.select('id','student',last(col('id')).over(window_spec).alias('last'))\\\n",
    ".withColumn('ids',when( ( (col('last')%2 != 0) & (col('id') == col('last')) ),col('id') )\n",
    "                 .when(col('id')%2 == 0,col('id') - 1).otherwise(col('id') + 1))\\\n",
    ".select(col('ids').alias('id'),'student').orderBy('id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "b8351c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|student|\n",
      "+---+-------+\n",
      "|  1|  Doris|\n",
      "|  2|  Abbot|\n",
      "|  3|  Green|\n",
      "|  4|Emerson|\n",
      "|  5| Jeames|\n",
      "+---+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/04 22:26:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "seat_df.createOrReplaceTempView('seatdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select id,student,last(id) over(order by id rows between \n",
    "               UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ) as idlast from seatdf),\n",
    "       one as (select id,student,idlast,case when idlast%2 != 0 and idlast = id then id\n",
    "                                             when id%2 == 0 then id - 1\n",
    "                                             else id + 1 end ids from main)\n",
    "               select ids as id,student from one order by id\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3158889f",
   "metadata": {},
   "source": [
    "#### 18 51 Customers Who Bought All Products M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b242c40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/18m_customer.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "baf20d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/18m_product.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "05f4a3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- product_key: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "71037db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_key: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "0437eebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|customer_id|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          3|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prd_key = product_df.collect()\n",
    "len_key = product_df.count()\n",
    "\n",
    "customer_df.filter(col('product_key').isin([prd_key[i][0] for i in range(0,len(prd_key))]))\\\n",
    ".groupBy('customer_id').agg(count(col('product_key')).alias('cnt')).filter(col('cnt') == len_key).select('customer_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "0990d593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|customer_id|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          3|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.createOrReplaceTempView('customerdf')\n",
    "product_df.createOrReplaceTempView('productdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select customer_id from customerdf where product_key in (select product_key from productdf))\n",
    "               select customer_id from main group by customer_id having count(customer_id) \n",
    "                    in (select count(*) from productdf)\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563cfa6a",
   "metadata": {},
   "source": [
    "#### 19 55 Product Sales Analysis III M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "abce4c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/19m_sales.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60e5ce21",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/19m_product.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "7cfb7d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sale_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "3c27a33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "7b81bb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+-----+\n",
      "|product_id|first_year|quantity|price|\n",
      "+----------+----------+--------+-----+\n",
      "|       100|      2008|      10| 5000|\n",
      "|       200|      2011|      15| 9000|\n",
      "+----------+----------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('product_id').orderBy(col('year').asc())\n",
    "\n",
    "sales_df.join(product_df,sales_df.product_id == product_df.product_id,'right' )\\\n",
    ".select(sales_df.product_id,'year','quantity','price').withColumn('rnk',rank().over(window_spec))\\\n",
    ".filter( (~col('product_id').isNull()) & (col('rnk') == 1))\\\n",
    ".select('product_id',col('year').alias('first_year'),'quantity','price').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "5c4e3070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+-----+\n",
      "|product_id|first_year|quantity|price|\n",
      "+----------+----------+--------+-----+\n",
      "|       100|      2008|      10| 5000|\n",
      "|       200|      2011|      15| 9000|\n",
      "+----------+----------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.createOrReplaceTempView('salesdf')\n",
    "product_df.createOrReplaceTempView('productdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select salesdf.product_id as product_id,year,quantity,price from salesdf\n",
    "                    right join productdf on salesdf.product_id = productdf.product_id),\n",
    "       one as (select product_id,year as first_year, quantity,price, \n",
    "                    rank() over(partition by product_id order by year) as rnk from main)\n",
    "               select product_id,first_year,quantity,price from one where product_id is not null and rnk =1\n",
    "         \n",
    "\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2af8da",
   "metadata": {},
   "source": [
    "#### 20 58 Project Employees III M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4aaedc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/20m_product.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5db54844",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/20m_employee.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "26eddd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- project_id: integer (nullable = true)\n",
      " |-- employee_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "78bfef68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- experience_years: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "00fcc6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+------+----------------+\n",
      "|project_id|employee_id|employee_id|  name|experience_years|\n",
      "+----------+-----------+-----------+------+----------------+\n",
      "|         2|          1|          1|Khaled|               3|\n",
      "|         1|          1|          1|Khaled|               3|\n",
      "|         1|          2|          2|   Ali|               2|\n",
      "|         1|          3|          3|  John|               3|\n",
      "|         2|          4|          4|   Doe|               2|\n",
      "+----------+-----------+-----------+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.join(employee_df,products_df.employee_id == employee_df.employee_id,'inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "be3eb874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|project_id|employee_id|\n",
      "+----------+-----------+\n",
      "|         1|          1|\n",
      "|         1|          3|\n",
      "|         2|          1|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('project_id').orderBy(col(('experience_years')).desc())\n",
    "\n",
    "products_df.join(employee_df,products_df.employee_id == employee_df.employee_id,'inner')\\\n",
    ".select('project_id',products_df.employee_id,'name','experience_years')\\\n",
    ".withColumn('rnk',rank().over(window_spec)).filter(col('rnk') == 1).select('project_id','employee_id')\\\n",
    ".orderBy(col('project_id').asc(),col('experience_years').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "5c15a582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|project_id|employee_id|\n",
      "+----------+-----------+\n",
      "|         1|          1|\n",
      "|         1|          3|\n",
      "|         2|          1|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.createOrReplaceTempView('productsdf')\n",
    "employee_df.createOrReplaceTempView('employeedf')\n",
    "\n",
    "spark.sql('''\n",
    "  with main as (select project_id,productsdf.employee_id as employee_id,name,experience_years,\n",
    "                     rank() over(partition by project_id order by experience_years desc) as rnk from productsdf\n",
    "                     join employeedf on productsdf.employee_id = employeedf.employee_id)      \n",
    "               select project_id,employee_id from main where rnk = 1 order by project_id asc,experience_years desc\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ec931e",
   "metadata": {},
   "source": [
    "#### 21 63 Unpopular Books  M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fca037da",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/21m_books.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6964dcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/21m_orders.csv'))\n",
    "# Assume today is 2019-06-23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1add46fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- book_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- available_from: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1fe657f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- book_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- dispatch_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2a125518",
   "metadata": {},
   "outputs": [],
   "source": [
    "despatched_one_year_age = orders_df.select('order_id','book_id','quantity','dispatch_date',\n",
    "abs(datediff(to_date(col('dispatch_date'),'yyyy-MM-dd'),to_date(lit('2019-06-23'),'yyyy-MM-dd'))).alias('diff') )\\\n",
    ".filter(col(('diff')) >= lit('365'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1519f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_month_ago = '2019-05-23'\n",
    "available_one_month_ago = books_df.filter(to_date(col('available_from'), 'yyyy-MM-dd') <= lit(one_month_ago) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "64bb271c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+--------------+\n",
      "|book_id|            name|available_from|\n",
      "+-------+----------------+--------------+\n",
      "|      1|Kalila And Demna|    2010-01-01|\n",
      "|      2|      28 Letters|    2012-05-12|\n",
      "|      5|The Hunger Games|    2008-09-21|\n",
      "+-------+----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "available_one_month_ago.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d77d5e5",
   "metadata": {},
   "source": [
    "#### 22 64 New Users Daily Count M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "200d2597",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/22m_traffic.csv'))\n",
    "# Assume today is 2019-06-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "39c211b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- activity: string (nullable = true)\n",
      " |-- activity_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "traffic_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3bf9ead6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+\n",
      "|activity_date|user_count|\n",
      "+-------------+----------+\n",
      "|   2019-05-01|         1|\n",
      "|   2019-06-21|         2|\n",
      "+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_before_one_month = traffic_df.filter(to_date(col('activity_date'),'yyyy-MM-dd') <= date_add(to_date(lit('2019-06-30'),'yyyy-MM-dd'),-90))\\\n",
    ".select('user_id').distinct().collect()\n",
    "\n",
    "traffic_df.filter( (to_date(col('activity_date'),'yyyy-MM-dd') >= date_add(to_date(lit('2019-06-30'),'yyyy-MM-dd'),-90))\n",
    "                & (~col('user_id').isin([ user_before_one_month[i][0] for i in range(0,len(user_before_one_month))])))\\\n",
    ".groupBy('activity_date').agg(countDistinct(col('user_id')).alias('user_count')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "02e5480e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+\n",
      "|activity_date|user_count|\n",
      "+-------------+----------+\n",
      "|   2019-05-01|         1|\n",
      "|   2019-06-21|         2|\n",
      "+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "traffic_df.createOrReplaceTempView('trafficdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select activity_date,user_id from trafficdf where \n",
    "                (to_date(activity_date,'yyyy-MM-dd')  >= date_add(to_date('2019-06-30','yyyy-MM-dd'),-90)) and\n",
    "                user_id not in (select user_id from trafficdf where to_date(activity_date,'yyyy-MM-dd')  <= date_add(to_date('2019-06-30','yyyy-MM-dd'),-90)) )\n",
    "              select activity_date, count(distinct user_id) as user_count from main group by  activity_date  \n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d7ab96",
   "metadata": {},
   "source": [
    "#### 23 65 Highest Grade For Each Student M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8652611",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/23m_input.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3b28041a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- student_id: integer (nullable = true)\n",
      " |-- course_id: integer (nullable = true)\n",
      " |-- grade: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1166df7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----+\n",
      "|student_id|course_id|grade|\n",
      "+----------+---------+-----+\n",
      "|         2|        2|   95|\n",
      "|         2|        3|   95|\n",
      "|         1|        1|   90|\n",
      "|         1|        2|   99|\n",
      "|         3|        1|   80|\n",
      "|         3|        2|   75|\n",
      "|         3|        3|   82|\n",
      "+----------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ec6c16c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 344:============================>                        (107 + 4) / 200]\r",
      "\r",
      "[Stage 344:==========================================>          (159 + 5) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----+\n",
      "|student_id|course_id|grade|\n",
      "+----------+---------+-----+\n",
      "|         1|        2|   99|\n",
      "|         2|        2|   95|\n",
      "|         3|        3|   82|\n",
      "+----------+---------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('student_id').orderBy(col('grade').desc())\n",
    "min_spec = Window.partitionBy('student_id').orderBy(col('course_id').desc())\n",
    "\n",
    "input_df.withColumn('rnk',rank().over(window_spec)).filter(col('rnk') == 1)\\\n",
    ".withColumn('minx',max('course_id').over(min_spec))\\\n",
    ".withColumn('tie',when( (count('student_id').over(window_spec)) >=2,col('student_id') ))\\\n",
    ".withColumn('xx',when( ((col('tie') >= 2) & (col('minx') == col('course_id'))),col('student_id') ))\\\n",
    ".filter(col('xx').isNull()).select('student_id','course_id','grade').orderBy(col('student_id').asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9fb9dea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----+\n",
      "|student_id|course_id|grade|\n",
      "+----------+---------+-----+\n",
      "|         1|        2|   99|\n",
      "|         2|        2|   95|\n",
      "|         3|        3|   82|\n",
      "+----------+---------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 418:==========================================>          (161 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "input_df.createOrReplaceTempView('inputdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select student_id,course_id,grade,rank() over(partition by student_id order by grade desc) as rnk from inputdf),\n",
    "       one as (select student_id,course_id,grade,count(student_id) over(partition by student_id ) as cnt,\n",
    "                   min(course_id) over(partition by student_id order by course_id)  as minx from main where rnk = 1)\n",
    "              select student_id,course_id,grade from one where minx = course_id order by student_id asc    \n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761424f6",
   "metadata": {},
   "source": [
    "#### 24 67 Active Businesses M "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef4c49c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/24m_events.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "47cc8af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: integer (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- occurrences: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "428583ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+\n",
      "|business_id|event_type|occurrences|\n",
      "+-----------+----------+-----------+\n",
      "|          1|   reviews|          7|\n",
      "|          3|   reviews|          3|\n",
      "|          1|       ads|         11|\n",
      "|          2|       ads|          7|\n",
      "|          3|       ads|          6|\n",
      "|          1|page views|          3|\n",
      "|          2|page views|         12|\n",
      "+-----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0e1a8aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+---+---+\n",
      "|business_id|event_type|occurrences|cnt| sm|\n",
      "+-----------+----------+-----------+---+---+\n",
      "|          1|       ads|         11|  3| 24|\n",
      "|          2|       ads|          7|  3| 24|\n",
      "|          3|       ads|          6|  3| 24|\n",
      "|          1|   reviews|          7|  2| 10|\n",
      "|          3|   reviews|          3|  2| 10|\n",
      "|          1|page views|          3|  2| 15|\n",
      "|          2|page views|         12|  2| 15|\n",
      "+-----------+----------+-----------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('event_type')\n",
    "\n",
    "events_df.withColumn('cnt',count('occurrences').over(window_spec)).withColumn('sm',sum('occurrences').over(window_spec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e60655a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|business_id|\n",
      "+-----------+\n",
      "|          1|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events_df.withColumn('avg',sum('occurrences').over(window_spec)/count('occurrences').over(window_spec))\\\n",
    ".filter(col('occurrences') > col('avg')).groupBy('business_id').agg(count(col('business_id')).alias('buss_cnt'))\\\n",
    ".filter(col('buss_cnt') > 1).select('business_id').show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9d7634fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|business_id|\n",
      "+-----------+\n",
      "|          1|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events_df.createOrReplaceTempView('eventsdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select business_id,occurrences,sum(occurrences) over(partition by event_type)/count(occurrences) over(partition by event_type) as avg\n",
    "                    from eventsdf),\n",
    "       one as (select business_id,occurrences,avg,count(business_id) over(partition by business_id) bus_cnt from main where occurrences > avg)\n",
    "               select distinct business_id from one where bus_cnt > 1  \n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46373cbc",
   "metadata": {},
   "source": [
    "#### 25 69 Reported Posts II M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9dfa4826",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/25m_actions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b9b7dc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "removals_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/25m_removals.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8e49c02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- post_id: integer (nullable = true)\n",
      " |-- action_date: string (nullable = true)\n",
      " |-- action: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a67f0043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- post_id: integer (nullable = true)\n",
      " |-- remove_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "removals_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "99a8a592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|average_daily_percent|\n",
      "+---------------------+\n",
      "|                 0.75|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('action_date')\n",
    "\n",
    "\n",
    "actions_df.filter(col('extra') == lit('spam')).join(removals_df,actions_df.post_id == removals_df.post_id,'left')\\\n",
    ".withColumn('cnt',count('extra').over(window_spec))\\\n",
    ".withColumn('cnt_remove',count(removals_df.post_id).over(window_spec)).filter(~col('remove_date').isNull())\\\n",
    ".select(expr(\"avg(cnt_remove/cnt)\").alias('average_daily_percent')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "0e7ce5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|average_daily_percent|\n",
      "+---------------------+\n",
      "|                 0.75|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actions_df.createOrReplaceTempView('actionsdf')\n",
    "removals_df.createOrReplaceTempView('removalsdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with main as (select user_id,actionsdf.post_id as act_post_id,action_date,action,extra,removalsdf.post_id as rm_postid,\n",
    "                     removalsdf.remove_date as rm_remove_date from actionsdf left join removalsdf on actionsdf.post_id = removalsdf.post_id\n",
    "                     where extra = \"spam\"),\n",
    "        one as (select user_id,act_post_id,action_date,rm_remove_date,count(extra) over(partition by action_date) as cnt, \n",
    "                     count(rm_postid) over(partition by action_date) as rm_cnt from main)\n",
    "               select avg(rm_cnt/cnt) as average_daily_percent from one where rm_remove_date is not null\n",
    "\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf780ad2",
   "metadata": {},
   "source": [
    "#### 26 73 Article Views II M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c13be9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "views_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/26m_views.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "1e5ab189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- article_id: integer (nullable = true)\n",
      " |-- author_id: integer (nullable = true)\n",
      " |-- viewer_id: integer (nullable = true)\n",
      " |-- view_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "views_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "917720d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  5|\n",
      "|  6|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('view_date','viewer_id').orderBy(col('article_id').asc())\n",
    "\n",
    "views_df.withColumn('rnk',rank().over(window_spec)).filter(col('rnk') == 2).select(col('viewer_id').alias('id')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "f4f5e13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  5|\n",
      "|  6|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "views_df.createOrReplaceTempView('viewsdf')\n",
    "\n",
    "spark.sql('''\n",
    "        select viewer_id as id from\n",
    "        (select viewer_id, rank() over(partition by view_date,viewer_id order by article_id) as rnk from viewsdf)\n",
    "              where rnk = 2\n",
    "       ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e614d7",
   "metadata": {},
   "source": [
    "#### 27 76 Product Price at a Given Date M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f42d2a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/27m_products.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "cb085ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- new_price: integer (nullable = true)\n",
      " |-- change_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "76fd95f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|product_id|price|\n",
      "+----------+-----+\n",
      "|         1|   35|\n",
      "|         2|   50|\n",
      "|         3|   10|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('product_id')\n",
    "window_rnk = Window.partitionBy('product_id').orderBy(col('change_date').desc())\n",
    "\n",
    "products_df.withColumn('prd_cnt',count('product_id').over(window_spec))\\\n",
    ".withColumn('new_price',when(col('prd_cnt') == 1,10).otherwise(col('new_price')))\\\n",
    ".withColumn('change_date',when(col('prd_cnt') == 1,lit('2019-08-16') ).otherwise(col('change_date')))\\\n",
    ".filter((to_date(col('change_date'),'yyyy-MM-dd') <= lit('2019-08-16') ) )\\\n",
    ".withColumn('rnk',rank().over(window_rnk)).filter(col('rnk') == 1)\\\n",
    ".select('product_id',col('new_price').alias('price')).orderBy('product_id').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "d03ca443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|product_id|price|\n",
      "+----------+-----+\n",
      "|         1|   35|\n",
      "|         2|   50|\n",
      "|         3|   10|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.createOrReplaceTempView('productsdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with main as (select product_id,new_price,change_date,count(product_id) over(partition by product_id) as cnt from productsdf),\n",
    "        one as (select product_id,case when cnt = 1 then 10 else new_price end as new_price,\n",
    "                                  case when cnt = 1 then \"2019-08-16\" else change_date end as change_date from main),       \n",
    "        two as (select product_id,new_price,change_date from one where change_date <= \"2019-08-16\"),\n",
    "      three as (select product_id,new_price,rank() over(partition by product_id order by change_date desc) rnk from two)\n",
    "               select product_id,new_price as price from three where rnk = 1 order by product_id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb20d74",
   "metadata": {},
   "source": [
    "#### 28 78 Immediate Food Delivery II M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f252bcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "delivery_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/28m_delivery.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "60cc701f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- delivery_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_pref_delivery_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delivery_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "5fd86828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.0"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('customer_id').orderBy(col('order_date').asc())\n",
    "window_cnt = Window.partitionBy('rnk')\n",
    "\n",
    "total_cnt = delivery_df.withColumn('rnk',rank().over(window_spec)).filter(col('rnk') == 1)\\\n",
    ".withColumn('schedule_cnt',when( (col('order_date') != col('customer_pref_delivery_date')),col('rnk')) )\\\n",
    ".withColumn('immediate',when( (col('order_date') == col('customer_pref_delivery_date')),col('rnk')))\\\n",
    ".select(sum(col('rnk')).alias('total_cnt')).collect()\n",
    "\n",
    "scheddule_cnt = delivery_df.withColumn('rnk',rank().over(window_spec)).filter(col('rnk') == 1)\\\n",
    ".withColumn('schedule_cnt',when( (col('order_date') != col('customer_pref_delivery_date')),col('rnk')) )\\\n",
    ".withColumn('immediate',when( (col('order_date') == col('customer_pref_delivery_date')),col('rnk')))\\\n",
    ".filter(col('schedule_cnt') == 1 ).select(sum('schedule_cnt').alias('schedule_cnt')).collect()\n",
    "\n",
    "immediate_cnt = delivery_df.withColumn('rnk',rank().over(window_spec)).filter(col('rnk') == 1)\\\n",
    ".withColumn('schedule_cnt',when( (col('order_date') != col('customer_pref_delivery_date')),col('rnk')) )\\\n",
    ".withColumn('immediate',when( (col('order_date') == col('customer_pref_delivery_date')),col('rnk')))\\\n",
    ".filter(col('immediate') == 1 ).select(sum('immediate').alias('immediate')).collect()\n",
    "\n",
    "(scheddule_cnt[0][0]/total_cnt[0][0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "f5fe6f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|immediate_percentage|\n",
      "+--------------------+\n",
      "|                50.0|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delivery_df.createOrReplaceTempView('productsdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select delivery_id,customer_id,order_date,customer_pref_delivery_date,\n",
    "                 rank() over(partition by customer_id order by order_date) as rnk from productsdf),\n",
    "       one as (select delivery_id,customer_id,order_date,customer_pref_delivery_date,rnk from main where rnk = 1),\n",
    "       two as (select  distinct count(rnk) over(partition by rnk) as total_cnt from one),  \n",
    " immediate as (select count(*) as immediate from one where order_date = customer_pref_delivery_date),\n",
    " scheduled as (select count(*) as scheduled from one where order_date != customer_pref_delivery_date)\n",
    "               select (immediate/total_cnt * 100) as immediate_percentage  from two,immediate\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167bcaf9",
   "metadata": {},
   "source": [
    "#### 29 80 Monthly Transactions I M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ecdfa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/29m_transactions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "bab9eb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- trans_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "3df054dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----------+--------------+------------------+---------------------+\n",
      "|trans_date|country|trans_count|approved_count|trans_total_amount|approved_total_amount|\n",
      "+----------+-------+-----------+--------------+------------------+---------------------+\n",
      "|   2018-12|     US|          2|             1|              3000|                 1000|\n",
      "|   2019-01|     US|          1|             1|              2000|                 2000|\n",
      "|   2019-01|     DE|          1|             1|              2000|                 2000|\n",
      "+----------+-------+-----------+--------------+------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.withColumn('trans_date',date_format('trans_date','yyyy-MM'))\\\n",
    ".groupBy('trans_date','country').agg(count('*').alias('trans_count'),\n",
    "                                     sum(when(col('state') == lit('approved'),1).otherwise(0)).alias('approved_count'),\n",
    "                                     sum(col('amount')).alias('trans_total_amount'),\n",
    "                                     sum(when(col('state') == lit('approved'),col('amount')).otherwise(0)).alias('approved_total_amount')\n",
    "                                    ).orderBy('trans_date').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "d2e80c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----------+--------------+------------------+---------------------+\n",
      "|trans_date|country|trans_count|approved_count|trans_total_amount|approved_total_amount|\n",
      "+----------+-------+-----------+--------------+------------------+---------------------+\n",
      "|   2018-12|     US|          2|             1|              3000|                 1000|\n",
      "|   2019-01|     US|          1|             1|              2000|                 2000|\n",
      "|   2019-01|     DE|          1|             1|              2000|                 2000|\n",
      "+----------+-------+-----------+--------------+------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.createOrReplaceTempView('transactionsdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select date_format(trans_date,\"yyyy-MM\") as trans_date,country,state,amount from transactionsdf)\n",
    "               select trans_date,country,count(*) as trans_count,\n",
    "                        sum(case when state = \"approved\" then 1 else 0 end ) as approved_count,\n",
    "                        sum(amount) as trans_total_amount,\n",
    "                        sum(case when state = \"approved\" then amount else 0 end) as approved_total_amount \n",
    "                from main group by trans_date,country order by trans_date\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8badc22e",
   "metadata": {},
   "source": [
    "#### 30 82 Last Person to Fit in the Bus M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7bfd3182",
   "metadata": {},
   "outputs": [],
   "source": [
    "queue_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/30m_queue.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "5fc2f057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- person_id: integer (nullable = true)\n",
      " |-- person_name: string (nullable = true)\n",
      " |-- weight: integer (nullable = true)\n",
      " |-- turn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queue_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "e7fcfb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|person_name|\n",
      "+-----------+\n",
      "|  John Cena|\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/05 22:53:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.orderBy(\"turn\")\n",
    "\n",
    "queue_df.withColumn('row_total',sum('weight').over(window_spec)).filter(col('row_total') == 1000)\\\n",
    ".select('person_name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "471bc247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|person_name|\n",
      "+-----------+\n",
      "|  John Cena|\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/05 22:56:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "queue_df.createOrReplaceTempView('queue_df')\n",
    "\n",
    "spark.sql('''\n",
    "  with main as (select person_name,weight,turn,sum(weight) over(order by turn) as row_total from queue_df)\n",
    "               select person_name from main where row_total = 1000\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46039e1b",
   "metadata": {},
   "source": [
    "#### 31 83 Monthly Transactions II M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "10ebfa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/31m_transactions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "41313915",
   "metadata": {},
   "outputs": [],
   "source": [
    "chargebacks_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/31m_chargebacks.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80cc6221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- trans_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22c8720e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trans_id: integer (nullable = true)\n",
      " |-- trans_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chargebacks_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "d10c29af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------+---------------+----------------+-----------------+\n",
      "|trans_date|country|approved_count|approved_amount|chargeback_count|chargeback_amount|\n",
      "+----------+-------+--------------+---------------+----------------+-----------------+\n",
      "|   2019-05|     US|             1|           1000|               1|             2000|\n",
      "|   2019-06|     US|             2|           8000|               1|             1000|\n",
      "|   2019-09|     US|             0|              0|               1|             5000|\n",
      "+----------+-------+--------------+---------------+----------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2199:===========================================>        (166 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transactions_df = transactions_df.withColumn('trans_date',date_format('trans_date','yyyy-MM'))\n",
    "chargebacks_df = chargebacks_df.withColumn('trans_date',date_format('trans_date','yyyy-MM'))\n",
    "\n",
    "transactions = transactions_df.groupby('trans_date','country').agg(sum(when( col('state') == lit('approved'),1).otherwise(0)).alias('approved_count'),\n",
    "                                     sum(when( col('state') == lit('approved'),col('amount')).otherwise(0)).alias('approved_amount'))\\\n",
    "\n",
    "chargebacks = chargebacks_df.join(transactions_df,chargebacks_df.trans_id == transactions_df.id, 'left')\\\n",
    ".groupBy(chargebacks_df.trans_date,'country').agg(count('*').alias('chargeback_count'),\n",
    "                                                 sum('amount').alias('chargeback_amount'))\n",
    "\n",
    "transactions.join(chargebacks,['trans_date', 'country'],'full_outer' ).na.fill(0)\\\n",
    ".select('trans_date', 'country', 'approved_count', 'approved_amount', 'chargeback_count', 'chargeback_amount').orderBy('trans_date').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "165bf6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2203:==================================================> (385 + 4) / 400]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------+---------------+----------------+-----------------+\n",
      "|trans_date|country|approved_count|approved_amount|chargeback_count|chargeback_amount|\n",
      "+----------+-------+--------------+---------------+----------------+-----------------+\n",
      "|   2019-05|     US|             1|           1000|               1|             2000|\n",
      "|   2019-06|     US|             2|           8000|               1|             1000|\n",
      "|   2019-09|     US|             0|              0|               1|             5000|\n",
      "+----------+-------+--------------+---------------+----------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transactions_df = transactions_df.withColumn('trans_date',date_format('trans_date','yyyy-MM'))\n",
    "chargebacks_df = chargebacks_df.withColumn('trans_date',date_format('trans_date','yyyy-MM'))\n",
    "\n",
    "transactions = transactions_df.groupby('trans_date','country')\\\n",
    "                                .agg(sum(when( col('state') == lit('approved'),1).otherwise(0)).alias('approved_count'),\n",
    "                                     sum(when( col('state') == lit('approved'),col('amount')).otherwise(0)).alias('approved_amount'))\\\n",
    "                                .withColumn('chargeback_count',lit('0'))\\\n",
    "                                .withColumn('chargeback_amount',lit('0'))\n",
    "\n",
    "chargebacks = chargebacks_df.join(transactions_df,chargebacks_df.trans_id == transactions_df.id, 'left')\\\n",
    ".groupBy(chargebacks_df.trans_date,'country')\\\n",
    "                                .agg(count('*').alias('chargeback_count'),\n",
    "                                     sum('amount').alias('chargeback_amount'))\\\n",
    "                                .withColumn('approved_count',lit('0'))\\\n",
    "                                .withColumn('approved_amount',lit('0'))\\\n",
    "                                .select('trans_date','country','approved_count','approved_amount','chargeback_count','chargeback_amount')\n",
    "\n",
    "transactions.unionAll(chargebacks).groupBy('trans_date','country')\\\n",
    "              .agg(sum('approved_count').cast('int').alias('approved_count'),\n",
    "                   sum('approved_amount').cast('int').alias('approved_amount'),\n",
    "                   sum('chargeback_count').cast('int').alias('chargeback_count'),\n",
    "                   sum('chargeback_amount').cast('int').alias('chargeback_amount')).orderBy('trans_date').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d925515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 97:========================================>             (151 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------------+---------------+----------------+-----------------+\n",
      "|  month|country|approved_count|approved_amount|chargeback_count|chargeback_amount|\n",
      "+-------+-------+--------------+---------------+----------------+-----------------+\n",
      "|2019-05|     US|             1|           1000|               1|             2000|\n",
      "|2019-06|     US|             2|           8000|               1|             1000|\n",
      "|2019-09|     US|             0|              0|               1|             5000|\n",
      "+-------+-------+--------------+---------------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.createOrReplaceTempView('transactionsdf')\n",
    "chargebacks_df.createOrReplaceTempView('chargebacksdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with main as (select date_format(trans_date,\"yyyy-MM\") as month,country,count(*) as approved_count,sum(amount) as approved_amount\n",
    "                     from transactionsdf where  state = \"approved\" group by month,country),\n",
    "        one as (select date_format(chargebacksdf.trans_date,\"yyyy-MM\") as month,country,amount\n",
    "                     from chargebacksdf right join transactionsdf on chargebacksdf.trans_id = transactionsdf.id),\n",
    "        two as (select month,country,count(month) as chargeback_count,sum(amount) as chargeback_amount\n",
    "                     from one where month is not null group by month,country)\n",
    "                select two.month,two.country,coalesce(approved_count,0) as approved_count,coalesce(approved_amount,0) as approved_amount,chargeback_count,chargeback_amount \n",
    "                     from main full outer join two on main.month = two.month and main.country = two.country\n",
    "                     order by month\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef12cdf",
   "metadata": {},
   "source": [
    "#### 32 85 Team Scores in Football Tournament M "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5ff163c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/32m_teams.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b7f9d067",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/32m_matches.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4d02009c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- team_id: integer (nullable = true)\n",
      " |-- team_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teams_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f2a94232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- match_id: integer (nullable = true)\n",
      " |-- host_team: integer (nullable = true)\n",
      " |-- guest_team: integer (nullable = true)\n",
      " |-- host_goals: integer (nullable = true)\n",
      " |-- guest_goals: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matches_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0974553a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------+\n",
      "|team_id|  team_name|num_points|\n",
      "+-------+-----------+----------+\n",
      "|     10|Leetcode FC|         7|\n",
      "|     20| NewYork FC|         3|\n",
      "|     30| Atlanta FC|         1|\n",
      "|     40| Chicago FC|         0|\n",
      "|     50| Toronto FC|         3|\n",
      "+-------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "one = matches_df.withColumnRenamed('host_team','team')\\\n",
    "                        .withColumn('num_points',when((col('host_goals') > col('guest_goals')),3)\\\n",
    "                        .when((col('host_goals') == col('guest_goals')),1)) \\\n",
    "                        .select('team','num_points')\n",
    "\n",
    "two = matches_df.withColumnRenamed('guest_team','team') \\\n",
    "                        .withColumn('num_points',when((col('host_goals') < col('guest_goals')),3)\\\n",
    "                        .when((col('host_goals') == col('guest_goals')),1).otherwise(0)) \\\n",
    "                        .select('team','num_points')\n",
    "\n",
    "one.union(two).groupBy(col('team')).agg(sum(col('num_points')).alias('num_points'))\\\n",
    "  .join(teams_df,one.team == teams_df.team_id,'right')\\\n",
    "  .select('team_id','team_name',coalesce(col('num_points'),lit('0')).alias('num_points')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "300310b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------+\n",
      "|team_id|  team_name|num_points|\n",
      "+-------+-----------+----------+\n",
      "|     10|Leetcode FC|         7|\n",
      "|     20| NewYork FC|         3|\n",
      "|     30| Atlanta FC|         1|\n",
      "|     40| Chicago FC|         0|\n",
      "|     50| Toronto FC|         3|\n",
      "+-------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matches_df.createOrReplaceTempView('matchesdf')\n",
    "teams_df.createOrReplaceTempView('teamsdf')\n",
    "\n",
    "spark.sql('''\n",
    "    with one as (select host_team as team, case when host_goals > guest_goals then 3 \n",
    "                                             when host_goals == guest_goals then 1 end as num_points from matchesdf),\n",
    "         two as (select guest_team as team,case when host_goals < guest_goals then 3 \n",
    "                                              when host_goals == guest_goals then 1 end as num_points from matchesdf),\n",
    "       three as (select team, sum(num_points) as num_points from (select * from one union all select * from two rigt) \n",
    "                                        group by team)\n",
    "                 select team_id,team_name,coalesce(num_points,'0') as num_points from teamsdf left join three  on team_id == team\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c32cb7",
   "metadata": {},
   "source": [
    "#### 33 89 Page Recommendations M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "545c4227",
   "metadata": {},
   "outputs": [],
   "source": [
    "friendship_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/33m_friendship.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c77dc515",
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/33m_likes.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "05a45553",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user1_id: integer (nullable = true)\n",
      " |-- user2_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "friendship_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "be6a5e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- page_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "likes_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2b942470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|recommended_page|\n",
      "+----------------+\n",
      "|              23|\n",
      "|              24|\n",
      "|              33|\n",
      "|              56|\n",
      "|              77|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "friendship_df.filter(col('user1_id') == 1).select(col('user2_id').alias('like'))\\\n",
    ".unionAll(friendship_df.filter(col('user2_id') == 1).select(col('user1_id').alias('like')))\\\n",
    ".join(likes_df,col('like') == likes_df.user_id,'inner' )\\\n",
    ".filter(~col('page_id').isin(likes_df.filter(col('user_id') == 1).select('page_id').collect()[0][0]))\\\n",
    ".select(col('page_id').alias('recommended_page')).distinct().orderBy('recommended_page').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "23377b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|page_id|\n",
      "+-------+\n",
      "|     23|\n",
      "|     24|\n",
      "|     33|\n",
      "|     56|\n",
      "|     77|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "friendship_df.createOrReplaceTempView('friendshipdf')\n",
    "likes_df.createOrReplaceTempView('likesdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as ( (select user2_id as like from friendshipdf where user1_id == 1)\n",
    "                     union(select user1_id as like from friendshipdf where user2_id == 1) )\n",
    "            select distinct page_id from likesdf join main on likesdf.user_id = main.like \n",
    "                 where page_id not in (select page_id from likesdf where user_id = 1)\n",
    "                 order by page_id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2877586d",
   "metadata": {},
   "source": [
    "#### 34 90 All People Report to the Given Manager M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "df6b52e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/34m_employees.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "84941c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- manager_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "98a7d90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|employee_id|\n",
      "+-----------+\n",
      "|          2|\n",
      "|          4|\n",
      "|          7|\n",
      "|         77|\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(employee_id=2), Row(employee_id=77)]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one  = employees_df.filter( (col('manager_id') == 1) & (col('employee_id') != 1) ).select('employee_id').collect()\n",
    "first = [one[i][0] for i in range(0,len(one))]\n",
    "\n",
    "two  = employees_df.filter( (col('manager_id')).isin(first)).select('employee_id').collect()\n",
    "second = [two[i][0] for i in range(0,len(two))]\n",
    "\n",
    "three = employees_df.filter( (col('manager_id')).isin(second)).select('employee_id').collect()\n",
    "third = [three[i][0] for i in range(0,len(three))]\n",
    "\n",
    "spark.createDataFrame(one).union(spark.createDataFrame(two)).union(spark.createDataFrame(three))\\\n",
    ".orderBy('employee_id').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1ddc13d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------+\n",
      "|employee_id|employee_name|manager_id|\n",
      "+-----------+-------------+----------+\n",
      "|          1|         Boss|         1|\n",
      "|          3|        Alice|         3|\n",
      "|          2|          Bob|         1|\n",
      "|          4|       Daniel|         2|\n",
      "|          7|         Luis|         4|\n",
      "|          8|         Jhon|         3|\n",
      "|          9|       Angela|         8|\n",
      "|         77|       Robert|         1|\n",
      "+-----------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "813877b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|employee_id|\n",
      "+-----------+\n",
      "|          2|\n",
      "|          4|\n",
      "|          7|\n",
      "|         77|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.createOrReplaceTempView('employeesdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with one as (select employee_id from employeesdf where manager_id = 1 and employee_id != 1),\n",
    "       two as (select employee_id from employeesdf where manager_id in (select * from one)),\n",
    "     three as (select employee_id from employeesdf where manager_id in (select * from two))\n",
    "               (select * from one)union(select * from two)union(select * from three) order by employee_id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ad6c17",
   "metadata": {},
   "source": [
    "#### 35 92  Find the Start and End Number of Continuous Ranges M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fc16d0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/35m_logs.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "15072660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- log_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "212e4da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:52:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/06 14:52:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/06 14:52:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/06 14:52:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/06 14:52:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/06 14:52:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|start|end|\n",
      "+-----+---+\n",
      "|    1|  3|\n",
      "|    7|  8|\n",
      "|   10| 10|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.orderBy(\"log_id\")\n",
    "\n",
    "df_with_lag = logs_df.withColumn(\"prev_log_id\", lag(\"log_id\").over(window_spec))\n",
    "\n",
    "df_with_breaks = df_with_lag.withColumn(\"is_new_group\", when((col(\"log_id\") - col(\"prev_log_id\")) > 1, 1).otherwise(0))\n",
    "\n",
    "df_with_groups = df_with_breaks.withColumn(\"group_id\", sum(\"is_new_group\").over(Window.orderBy(\"log_id\")))\n",
    "\n",
    "logic_df = df_with_groups.withColumn('strend',rank().over(Window.partitionBy('group_id').orderBy('log_id')))\\\n",
    ".withColumn('start',when(col('strend') == min('strend').over(Window.partitionBy('group_id')),col('log_id')).otherwise(0))\\\n",
    ".withColumn('end',when(col('strend')   == max('strend').over(Window.partitionBy('group_id')),col('log_id')).otherwise(0))\n",
    "\n",
    "start_df = logic_df.select('start').withColumn('rnk',dense_rank().over(Window.orderBy(col('start')))).filter(col('rnk') != 1)\n",
    "end_df = logic_df.select('end').withColumn('rnk',dense_rank().over(Window.orderBy(col('end')))).filter(col('rnk') != 1)\n",
    "\n",
    "start_df.join(end_df,start_df.rnk == end_df.rnk,'inner').select('start','end').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "44805e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 15:00:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/06 15:00:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/06 15:00:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/06 15:00:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/06 15:00:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/06 15:00:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|start|end|\n",
      "+-----+---+\n",
      "|    1|  3|\n",
      "|    7|  8|\n",
      "|   10| 10|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_df.createOrReplaceTempView('logsdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with main as (select log_id, sum(is_new_group) OVER(ORDER BY log_id) as group_id  from \n",
    "                    (select log_id,case when (log_id - prev_log_id) > 1 then 1 else 0  end is_new_group from \n",
    "                        (select log_id,lag(log_id) OVER(ORDER BY log_id) as prev_log_id from logsdf)) group by log_id,is_new_group),    \n",
    "      start as (select distinct min(log_id) OVER(PARTITION BY group_id ) as start from main),\n",
    "        end as (select distinct max(log_id) OVER(PARTITION BY group_id ) as end from main),\n",
    "     end_id as (select end,row_number() OVER(ORDER BY end) as row_id from end), \n",
    "   start_id as (select start,row_number() OVER(ORDER BY start) as row_id from start)\n",
    "                select start,end from start_id join end_id on start_id.row_id == end_id.row_id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca19ebc",
   "metadata": {},
   "source": [
    "#### 36 95 Running Total for Different Genders M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3feb6d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/36m_scores.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a51bdc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- player_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- score_points: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "3686d285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----+\n",
      "|gender|       day|total|\n",
      "+------+----------+-----+\n",
      "|     F|2019-12-30|   17|\n",
      "|     F|2019-12-31|   40|\n",
      "|     F|2020-01-01|   57|\n",
      "|     F|2020-01-07|   80|\n",
      "|     M|2019-12-18|    2|\n",
      "|     M|2019-12-25|   13|\n",
      "|     M|2019-12-30|   26|\n",
      "|     M|2019-12-31|   29|\n",
      "|     M|2020-01-07|   36|\n",
      "+------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('gender').orderBy(col('day'))\n",
    "window_rnk  = Window.partitionBy('gender').orderBy(col('rnk').asc())\n",
    "\n",
    "scores_df.withColumn('rnk',row_number().over(window_spec))\\\n",
    ".withColumn('total',sum('score_points').over(window_rnk)).select('gender','day','total').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "cd6962b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----+\n",
      "|gender|       day|total|\n",
      "+------+----------+-----+\n",
      "|     F|2019-12-30|   17|\n",
      "|     F|2019-12-31|   40|\n",
      "|     F|2020-01-01|   57|\n",
      "|     F|2020-01-07|   80|\n",
      "|     M|2019-12-18|    2|\n",
      "|     M|2019-12-25|   13|\n",
      "|     M|2019-12-30|   26|\n",
      "|     M|2019-12-31|   29|\n",
      "|     M|2020-01-07|   36|\n",
      "+------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores_df.createOrReplaceTempView('scoresdf')\n",
    "\n",
    "spark.sql('''\n",
    "          select gender,day,sum(score_points) over(partition by gender order by rnk) as total from\n",
    "          (select gender,day,score_points,row_number() over(partition by gender order by day asc) as rnk from scoresdf)\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b314aa68",
   "metadata": {},
   "source": [
    "#### 37 96 Restaurant Growth M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dc7dae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/37m_customer.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "3d435f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- visited_on: string (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "51a9f11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 18:49:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/06 18:49:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------+\n",
      "|visited_on|amount|average_amount|\n",
      "+----------+------+--------------+\n",
      "|2019-01-07|   860|        122.86|\n",
      "|2019-01-08|   840|         120.0|\n",
      "|2019-01-09|   840|         120.0|\n",
      "|2019-01-10|  1000|        142.86|\n",
      "+----------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.orderBy(col('visited_on').asc()).rowsBetween(0,6)\n",
    "\n",
    "customer_df.groupBy('visited_on').agg(sum(col('amount')).alias('amount')).orderBy('visited_on')\\\n",
    ".withColumn('xxx',sum('amount').over(window_spec))\\\n",
    ".withColumn('yyy',lead('visited_on',6,0).over(Window.orderBy('visited_on')))\\\n",
    ".withColumn('average_amount',round(col('xxx')/7,2)  ).filter(col('yyy') != lit('0') )\\\n",
    ".select(col('yyy').alias('visited_on'),col('xxx').alias('amount'),'average_amount').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "b81a033b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 19:05:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------+\n",
      "|visited_on|amount|average_amount|\n",
      "+----------+------+--------------+\n",
      "|2019-01-07|   860|        122.86|\n",
      "|2019-01-08|   840|         120.0|\n",
      "|2019-01-09|   840|         120.0|\n",
      "|2019-01-10|  1000|        142.86|\n",
      "+----------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.createOrReplaceTempView('customerdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select visited_on,sum(amount) as amount from customerdf group by visited_on order by visited_on asc),\n",
    "       one as (select lead(visited_on,6,0) over(order by visited_on) as visited_on,\n",
    "                    sum(amount) over(order by visited_on rows between 0 PRECEDING and 6 FOLLOWING) as amount from main)\n",
    "              select visited_on,amount,round((amount/7),2) as average_amount from one where visited_on != \"0\"\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aa5c70",
   "metadata": {},
   "source": [
    "#### 38 100 Movie Rating M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4f8ff0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/38m_movies.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a45df341",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/38m_users.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c4255884",
   "metadata": {},
   "outputs": [],
   "source": [
    "movierating_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/38m_movierating.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "6952ecb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movie_id: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "3e707968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "94309d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movie_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movierating_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "78a3687b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 21:03:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/06 21:03:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daniel Frozen 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = users_df.join(movierating_df.select('user_id'),on = 'user_id')\\\n",
    ".groupBy(users_df.name).agg(count(users_df.name).alias('cnt'))\\\n",
    ".withColumn('rnk',rank().over(Window.orderBy(col('cnt').desc()))).filter(col('rnk') == 1)\\\n",
    ".select('name').orderBy('name').first()[0]\n",
    "\n",
    "window_spec = Window.partitionBy('movie_id')\n",
    "\n",
    "movierating = movierating_df.withColumn('created_at',date_format('created_at','yyyy-MM')).filter(col('created_at') ==lit('2020-02') )\\\n",
    "           .select('movie_id','rating','created_at')\n",
    "\n",
    "moviename = movies_df.join(movierating,on = 'movie_id').withColumn('cnt',count('movie_id').over(window_spec))\\\n",
    ".withColumn('sm',sum('rating').over(window_spec)).select('title',expr(\"sm/cnt\").alias('average'))\\\n",
    ".withColumn('rnk',rank().over(Window.orderBy(col('average').desc()))).filter(col('rnk') == 1)\\\n",
    ".select('title').orderBy('title').first()[0]\n",
    "\n",
    "print(result,moviename)\n",
    "                                                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "271e944d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 22:21:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/06 22:21:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 992:======================================>              (144 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|  result|\n",
      "+--------+\n",
      "|Frozen 2|\n",
      "|  Daniel|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "movies_df.createOrReplaceTempView('moviesdf')\n",
    "users_df.createOrReplaceTempView('usersdf')\n",
    "movierating_df.createOrReplaceTempView('movieratingdf')\n",
    "\n",
    "\n",
    "spark.sql('''\n",
    "  with name as (select name,cnt ,rank() over(order by cnt desc) as rnk from \n",
    "                    (select name,count(name) as cnt from usersdf join movieratingdf on usersdf.user_id = movieratingdf.user_id\n",
    "                         group by name) ),\n",
    "  user_name as (select name as result from name where rnk = 1 order by name limit 1),\n",
    "      movie as (select moviesdf.movie_id,title,x.rating from moviesdf join \n",
    "                     (select movieratingdf.movie_id,rating,date_format(created_at,'yyyy-MM') as created_at from movieratingdf) x\n",
    "                           on moviesdf.movie_id = x.movie_id where x.created_at = \"2020-02\"),\n",
    "  movie_avg as (select title,sum(rating) over(partition by movie_id)/count(movie_id) over(partition by movie_id) as average from movie),\n",
    "  movie_rng as (select title as result, rank() over(order by average desc) as rnk from movie_avg),\n",
    "  res_movie as (select result from movie_rng where rnk = 1 order by result limit 1)\n",
    "               (select * from user_name)union(select * from res_movie)\n",
    "\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63039dfb",
   "metadata": {},
   "source": [
    "#### 39 102 Activity Participants M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "08505fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "friends_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/39m_friends.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fece0ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/39m_activities.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "7f11f034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- activity: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "friends_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "295d88fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activities_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "0b9d3104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 22:42:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|activity|\n",
      "+--------+\n",
      "| Singing|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('activity')\n",
    "window_rnk = Window.orderBy(col('cnt').desc())\n",
    "\n",
    "friends_df.withColumn('cnt',count('activity').over(window_spec))\\\n",
    ".withColumn('rnk',dense_rank().over(window_rnk)).filter(col('rnk') == 2).select('activity').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "ae8fd7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 22:48:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|activity|\n",
      "+--------+\n",
      "| Singing|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "friends_df.createOrReplaceTempView('friendsdf')\n",
    "activities_df.createOrReplaceTempView('activitiesdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with main as (select activity, dense_rank() over(order by cnt desc) as rnk from \n",
    "         (select activity, count(activity) over(partition by activity) as cnt from friendsdf))\n",
    "         select distinct activity from main where rnk = 2\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dd42e5",
   "metadata": {},
   "source": [
    "#### 40 103  Number of Trusted Contacts of a Customer M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b02e1656",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/40m_customers.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "50efbfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "contacts_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/40m_contacts.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1f53940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "invoices_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/40m_invoices.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "1fe69ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "31a0f89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- contact_name: string (nullable = true)\n",
      " |-- contact_email: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contacts_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "c0ee9327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- invoice_id: integer (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "invoices_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "c208e2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-----+------------+--------------------+\n",
      "|invoice_id|customer_name|price|contacts_cnt|trusted_contacts_cnt|\n",
      "+----------+-------------+-----+------------+--------------------+\n",
      "|        44|         Alex|   60|           1|                   1|\n",
      "|        55|         John|  500|           0|                   0|\n",
      "|        66|          Bob|  400|           2|                   0|\n",
      "|        77|        Alice|  100|           3|                   2|\n",
      "|        88|        Alice|  200|           3|                   2|\n",
      "|        99|          Bob|  300|           2|                   0|\n",
      "+----------+-------------+-----+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "invoice_info = customers_df.join(invoices_df,customers_df.customer_id == invoices_df.user_id,'inner')\\\n",
    ".select('invoice_id','customer_name','price','customer_id')\n",
    "\n",
    "contact_cnt = customers_df.join(contacts_df,customers_df.customer_id == contacts_df.user_id,'left')\\\n",
    ".select('customer_id','customer_name','user_id','contact_name').withColumn('contacts_cnt',count('user_id').over(Window.partitionBy('user_id')))\\\n",
    ".select('customer_id','customer_name','contacts_cnt').distinct()\n",
    "\n",
    "anti_info = contacts_df.join(customers_df,customers_df.customer_name == contacts_df.contact_name,'leftanti')\\\n",
    ".withColumn('anticnt',count('user_id').over(Window.partitionBy('user_id'))).select('user_id','anticnt').distinct()\n",
    "\n",
    "invoice_info.join(contact_cnt,invoice_info.customer_id == contact_cnt.customer_id,'inner')\\\n",
    ".select('invoice_id',invoice_info.customer_name,'price','contacts_cnt',invoice_info.customer_id)\\\n",
    ".join(anti_info,invoice_info.customer_id == anti_info.user_id,'left')\\\n",
    ".withColumn('trusted_contacts_cnt',expr(\"contacts_cnt - coalesce(anticnt,0) \"))\\\n",
    ".select('invoice_id','customer_name','price','contacts_cnt','trusted_contacts_cnt').orderBy('invoice_id').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "196cc5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-----+------------+--------------------+\n",
      "|invoice_id|customer_name|price|contacts_cnt|trusted_contacts_cnt|\n",
      "+----------+-------------+-----+------------+--------------------+\n",
      "|        44|         Alex|   60|           1|                   1|\n",
      "|        55|         John|  500|           0|                   0|\n",
      "|        66|          Bob|  400|           2|                   0|\n",
      "|        77|        Alice|  100|           3|                   2|\n",
      "|        88|        Alice|  200|           3|                   2|\n",
      "|        99|          Bob|  300|           2|                   0|\n",
      "+----------+-------------+-----+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df.createOrReplaceTempView('customersdf')\n",
    "contacts_df.createOrReplaceTempView('contactsdf')\n",
    "invoices_df.createOrReplaceTempView('invoicesdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with invoice as (select invoice_id,customer_name,price,customer_id from customersdf join invoicesdf\n",
    "                        on customersdf.customer_id = invoicesdf.user_id),\n",
    "  contacts_cnt as (select distinct customer_id,count(user_id) over(partition by user_id) as contacts_cnt from customersdf \n",
    "                        left join contactsdf on customersdf.customer_id = contactsdf.user_id),\n",
    "      anti_cnt as (select distinct user_id,count(user_id) over(partition by user_id) as cnt_ant from contactsdf  left anti join customersdf \n",
    "                        on contactsdf.contact_name = customersdf.customer_name) \n",
    "                  select invoice_id,customer_name,price,contacts_cnt, (contacts_cnt - coalesce(cnt_ant,0)) as trusted_contacts_cnt from \n",
    "                       invoice  join contacts_cnt on invoice.customer_id = contacts_cnt.customer_id\n",
    "                                left join anti_cnt on anti_cnt.cnt_ant = invoice.customer_id\n",
    "                       order by invoice_id                             \n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "e77b0f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-----------------+\n",
      "|user_id|contact_name|    contact_email|\n",
      "+-------+------------+-----------------+\n",
      "|      1|         Jal| jal@leetcode.com|\n",
      "|      2|        Omar|omar@leetcode.com|\n",
      "|      2|        Meir|meir@leetcode.com|\n",
      "+-------+------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contacts_df.join(customers_df,customers_df.customer_name == contacts_df.contact_name,'leftanti').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4a8029",
   "metadata": {},
   "source": [
    "#### 41 107 Capital Gain/Loss M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "584703b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/41m_stocks.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f366a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- stock_name: string (nullable = true)\n",
      " |-- operation: string (nullable = true)\n",
      " |-- operation_day: integer (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stocks_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77bdfdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 139:====================================================>(199 + 1) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+\n",
      "|  stock_name|capital_gain_loss|\n",
      "+------------+-----------------+\n",
      "|    Handbags|           -23000|\n",
      "|Corona Masks|             9500|\n",
      "|    Leetcode|             8000|\n",
      "+------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('stock_name','operation')\n",
    "window_rnk =  Window.partitionBy('stock_name','operation').orderBy(col('operation_day').desc())\n",
    "\n",
    "buy = stocks_df.withColumn('sumprice',sum('price').over(window_spec))\\\n",
    ".withColumn('rnk',rank().over(window_rnk)).filter( (col('rnk') == 1) & (col('operation') == lit('Buy'))  ).select('stock_name','operation',col('sumprice').alias('buy_price'),'rnk')\n",
    "\n",
    "sell = stocks_df.withColumn('sumprice',sum('price').over(window_spec))\\\n",
    ".withColumn('rnk',rank().over(window_rnk)).filter( (col('rnk') == 1) & (col('operation') == lit('Sell'))  ).select('stock_name','operation',col('sumprice').alias('sell_price'),'rnk')\n",
    "\n",
    "sell.join(buy,['stock_name','rnk'],'inner').select('stock_name',expr(\"sell_price - buy_price\").alias('capital_gain_loss')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "be5acb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+\n",
      "|  stock_name|capital_gain_loss|\n",
      "+------------+-----------------+\n",
      "|    Handbags|           -23000|\n",
      "|Corona Masks|             9500|\n",
      "|    Leetcode|             8000|\n",
      "+------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stocks_df.createOrReplaceTempView('stocksdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select stock_name,operation,sumprice, rank() over(partition by stock_name,operation order by sumprice desc) as rnk from \n",
    "                   (select stock_name,operation,sum(price) as sumprice from stocksdf group by stock_name,operation)),\n",
    "      sell as (select stock_name,operation,sumprice as sell_price,rnk from main where operation = \"Sell\" ),\n",
    "      buy  as (select stock_name,operation,sumprice as buy_price,rnk from main where operation = \"Buy\") \n",
    "              select sell.stock_name, (sell_price - buy_price) as capital_gain_loss from sell join buy on \n",
    "                   sell.stock_name = buy.stock_name and sell.rnk = buy.rnk\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4040575",
   "metadata": {},
   "source": [
    "#### 42 108 Customers Who Bought Products A and B but Not C M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c6b9c46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/42m_customers.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "02db92a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/42m_orders.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9b08194b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "217464de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "185c4643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|customer_name|\n",
      "+-------------+\n",
      "|    Elizabeth|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('customer_id')\n",
    "\n",
    "orders_df.filter(col('product_name').isin(['A','B','C'])).select('customer_id','product_name')\\\n",
    ".withColumn('cnt',count('customer_id').over(window_spec)).filter(col('cnt') == 2).select('customer_id').distinct()\\\n",
    ".join(customers_df,on = 'customer_id').select('customer_name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a15c8391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|customer_name|\n",
      "+-------------+\n",
      "|    Elizabeth|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df.createOrReplaceTempView('customersdf')\n",
    "orders_df.createOrReplaceTempView('ordersdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with main as (select distinct customer_id from \n",
    "                     (select customer_id,product_name,count(customer_id) over(partition by customer_id) as cnt\n",
    "                          from ordersdf where product_name in ('A','B','C'))\n",
    "                     where cnt = 2)\n",
    "               select customer_name from customersdf join main on customersdf.customer_id = main.customer_id\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166e7edc",
   "metadata": {},
   "source": [
    "#### 43 113 Evaluate Boolean Expression M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5f79aa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/43m_variables.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b9f6c20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "expressions_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/43m_expressions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3fc1c6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- value: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "variables_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "22abc94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- left_operand: string (nullable = true)\n",
      " |-- operator: string (nullable = true)\n",
      " |-- right_operand: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expressions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3188462f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-------------+-----+\n",
      "|left_operand|operator|right_operand|value|\n",
      "+------------+--------+-------------+-----+\n",
      "|           x|       >|            y|false|\n",
      "|           x|       <|            y| true|\n",
      "|           x|       =|            y|false|\n",
      "|           y|       >|            x| true|\n",
      "|           y|       <|            x|false|\n",
      "|           x|       =|            x| true|\n",
      "+------------+--------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = 66\n",
    "y = 77\n",
    "expressions_df.withColumn('xx',when(col('left_operand') == lit('x'),66) \\\n",
    "                              .when(col('left_operand') == lit('y'),77)) \\\n",
    "              .withColumn('yy',when(col('right_operand') == lit('x'),66) \\\n",
    "                              .when(col('right_operand') == lit('y'),77)) \\\n",
    "              .withColumn('value',when(col('operator') == '>', col('xx') > col('yy')) \\\n",
    "                                 .when(col('operator') == '<', col('xx') < col('yy')) \\\n",
    "                                 .when(col('operator') == '=', col('xx') == col('yy'))) \\\n",
    "                                 .select('left_operand','operator','right_operand','value').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "810099d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-------------+-----+\n",
      "|left_operand|operator|right_operand|value|\n",
      "+------------+--------+-------------+-----+\n",
      "|           x|       >|            y|false|\n",
      "|           x|       <|            y| true|\n",
      "|           x|       =|            y|false|\n",
      "|           y|       >|            x|false|\n",
      "|           y|       <|            x| true|\n",
      "|           x|       =|            x|false|\n",
      "+------------+--------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expressions_df.createOrReplaceTempView('expressionsdf')\n",
    "\n",
    "spark.sql(''' \n",
    "   with base as (select left_operand,operator,right_operand,case when left_operand == \"x\" then 66 else 77 end as xx,  \n",
    "                        case when right_operand == \"y\" then 77 else 66 end as yy from expressionsdf) \n",
    "            select left_operand,operator,right_operand, case when operator = \">\" then \"xx\" > \"yy\"\n",
    "                                                            when operator = \"<\" then \"xx\" < \"yy\"\n",
    "                                                            when operator = \"=\" then \"xx\" = \"yy\" end as value \n",
    "                                                             from base\n",
    "  ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22cd2ed",
   "metadata": {},
   "source": [
    "#### 44 114 Apples & Oranges M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4172d6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/44m_sales.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fc0bc7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sale_date: string (nullable = true)\n",
      " |-- fruit: string (nullable = true)\n",
      " |-- sold_num: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b76e9c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "| sale_date|diff|\n",
      "+----------+----+\n",
      "|2020-05-01|   2|\n",
      "|2020-05-02|   0|\n",
      "|2020-05-03|  20|\n",
      "|2020-05-04|  -1|\n",
      "+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "apple = sales_df.filter(col('fruit') == lit('apples')).select('sale_date',col('sold_num').alias('apple_sold'))\n",
    "orange = sales_df.filter(col('fruit') == lit('oranges')).select('sale_date',col('sold_num').alias('orange_sold'))\n",
    "\n",
    "apple.join(orange, on = 'sale_date').select('sale_date',expr(\"apple_sold - orange_sold\").alias('diff')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "799096cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "| sale_date|diff|\n",
      "+----------+----+\n",
      "|2020-05-01|   2|\n",
      "|2020-05-02|   0|\n",
      "|2020-05-03|  20|\n",
      "|2020-05-04|  -1|\n",
      "+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.createOrReplaceTempView('salesdf')\n",
    "\n",
    "spark.sql('''\n",
    "           select apple.sale_date as sale_date, (apple_sold - orange_sold) as diff from \n",
    "                     (select sale_date,sold_num as apple_sold from salesdf where fruit = \"apples\") as apple join\n",
    "                     (select sale_date,sold_num as orange_sold from salesdf where fruit = \"oranges\") as orange\n",
    "                on apple.sale_date =orange.sale_date\n",
    "\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1d8855",
   "metadata": {},
   "source": [
    "#### 45 115 Active Users M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8433ed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "logins_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/45m_logins.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "075056f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/45m_accounts.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90cc44ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- login_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logins_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3b44be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accounts_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd3f2cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 12:=====================================>                 (68 + 4) / 100]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|    name|\n",
      "+---+--------+\n",
      "|  7|Jonathan|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('id').orderBy(col('login_date').asc())\n",
    "\n",
    "logins_df.withColumn('rnk',rank().over(window_spec)).filter(col('rnk') ==lit('6'))\\\n",
    ".join(accounts_df,on = 'id').select('id','name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "537c7b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 23:=====================================>                 (69 + 4) / 100]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|    name|\n",
      "+---+--------+\n",
      "|  7|Jonathan|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accounts_df.createOrReplaceTempView('accountsdf')\n",
    "logins_df.createOrReplaceTempView('loginsdf')\n",
    "\n",
    "spark.sql('''\n",
    "          select x.id,name from accountsdf join\n",
    "          (select id,login_date,rank() over(partition by id order by login_date asc) as rnk from loginsdf) as x\n",
    "          on accountsdf.id = x.id where rnk = 6\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b08301",
   "metadata": {},
   "source": [
    "#### 46 116 Rectangles Area M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9d2963a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/46m_points.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24a83870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- x_value: integer (nullable = true)\n",
      " |-- y_value: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "points_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d55cfd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+\n",
      "| p1| p2|area|\n",
      "+---+---+----+\n",
      "|  1|  2|   2|\n",
      "|  2|  3|   4|\n",
      "+---+---+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/08 00:17:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "x = points_df.select('x_value')\n",
    "y = points_df.select('y_value')\n",
    "\n",
    "window_spec = Window.orderBy(col('id').asc())\n",
    "\n",
    "points_df.withColumn('1xst',lead('x_value',1,0).over(window_spec))\\\n",
    ".withColumn('1yxt',lead('y_value',1,0).over(window_spec))\\\n",
    ".withColumn('1idxt',lead('id',1,0).over(window_spec))\\\n",
    ".withColumn('2xst',lead('x_value',2,0).over(window_spec))\\\n",
    ".withColumn('2yxt',lead('y_value',2,0).over(window_spec))\\\n",
    ".withColumn('2idxt',lead('id',2,0).over(window_spec))\\\n",
    ".select('id','1idxt','2idxt','x_value','1xst','2xst','y_value','1yxt','2yxt',\n",
    "        expr(\"abs((x_value - 1xst) * (y_value - 1yxt ))\").alias('area1'),\n",
    "        expr(\"abs((x_value - 2xst) * (y_value - 2yxt ))\").alias('area2'),\n",
    "       ).filter(col('1idxt') != 0).select(col('id').alias('p1'),col('1idxt').alias('p2'),col('area1').alias('area')  ).show()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1186e10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+\n",
      "| p1| p2|area|\n",
      "+---+---+----+\n",
      "|  1|  2|   2|\n",
      "|  2|  3|   4|\n",
      "+---+---+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/08 00:45:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "points_df.createOrReplaceTempView('pointsdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (   select id,lead(id, 1, 0) over(order by id) as id1,\n",
    "                           lead(id, 2, 0) over(order by id) as id2,\n",
    "                           x_value, \n",
    "                           lead(x_value, 1, 0) over(order by id) as x1,\n",
    "                           lead(x_value, 2, 0) over(order by id) as x2, \n",
    "                           y_value, \n",
    "                           lead(y_value, 1, 0) over(order by id) as y1, \n",
    "                           lead(y_value, 2, 0) over(order by id) as y2\n",
    "                        from pointsdf),\n",
    "       one as (select id,id1,id2, abs((x_value - x1) * (y_value - y1)) as area,\n",
    "                    abs((x_value - x2) * (y_value - y2)) as area2 from main)\n",
    "              select id as p1,id1 as p2,area from one where id != 3\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2a57ff",
   "metadata": {},
   "source": [
    "#### 47 117 Calculate Salaries M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "57893691",
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/47m_salaries.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1499356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- company_id: integer (nullable = true)\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salaries_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "198093b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 93:=================================>                    (125 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------------+------+\n",
      "|company_id|employee_id|employee_name|salary|\n",
      "+----------+-----------+-------------+------+\n",
      "|         1|          1|         Tony|  1020|\n",
      "|         1|          2|       Pronub| 10863|\n",
      "|         1|          3|       Tyrrox|  5508|\n",
      "|         2|          1|          Pam|   300|\n",
      "|         2|          7|       Bassem|   450|\n",
      "|         2|          9|     Hermione|   700|\n",
      "|         3|          2|       Ognjen|  1672|\n",
      "|         3|          7|      Bocaben|    76|\n",
      "|         3|         13|      Nyancat|  2508|\n",
      "|         3|         15|  Morninngcat|  5910|\n",
      "+----------+-----------+-------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 93:==============================================>       (174 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('company_id').orderBy(col('salary').desc())\n",
    "\n",
    "salaries_df.withColumn('rnk',max('salary').over(window_spec))\\\n",
    ".withColumn('per',when((col('rnk') > lit('10000')),col('salary') - (49/100 * col('salary')) )\n",
    "                 .when((col('rnk') < lit('1000')),col('salary'))\n",
    "                 .when(((col('rnk') <= lit('10000')) & (col('rnk') > lit('1000'))),col('salary') - (24/100 * col('salary'))))\\\n",
    ".select('company_id','employee_id','employee_name',col('per').cast('int').alias('salary')).orderBy('company_id','employee_id').show()\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88f05ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 135:======================================>              (147 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------------+----------+\n",
      "|company_id|employee_id|employee_name|percentage|\n",
      "+----------+-----------+-------------+----------+\n",
      "|         1|          1|         Tony|    1020.0|\n",
      "|         1|          2|       Pronub|   10863.0|\n",
      "|         1|          3|       Tyrrox|    5508.0|\n",
      "|         2|          1|          Pam|     300.0|\n",
      "|         2|          7|       Bassem|     450.0|\n",
      "|         2|          9|     Hermione|     700.0|\n",
      "|         3|          2|       Ognjen|    1672.0|\n",
      "|         3|          7|      Bocaben|      76.0|\n",
      "|         3|         13|      Nyancat|    2508.0|\n",
      "|         3|         15|  Morninngcat|   5910.52|\n",
      "+----------+-----------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salaries_df.createOrReplaceTempView('salariesdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select company_id,employee_id,employee_name,salary,max(salary) over(partition by company_id order by salary desc) as max_sal from salariesdf)\n",
    "           select company_id,employee_id,employee_name,case when max_sal > 10000 then salary - (49/100 * salary) \n",
    "                                                            when max_sal < 1000 then salary\n",
    "                                                            when max_sal <= 10000 and max_sal > 1000 then salary - (24/100 * salary) \n",
    "                                                            end as percentage from main\n",
    "                 order by company_id,employee_id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c317f149",
   "metadata": {},
   "source": [
    "#### 48 121 Countries You Can Safely Invest In M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a38eebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/48m_person.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b745a738",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/48m_country.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "58054dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "calls_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/48m_calls.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "be1e945b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- phone_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95a37d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- country_code: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9291f9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- caller_id: integer (nullable = true)\n",
      " |-- callee_id: integer (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calls_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fde7e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Peru'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df =calls_df.withColumn('callarrays',array_sort(array(col('caller_id'),col('callee_id')))) \\\n",
    "  .agg((sum(col('duration'))/count(col('duration'))).alias('total_avg'))\n",
    "\n",
    "final_df= calls_df.withColumn('callarrays',array_sort(array(col('caller_id'),col('callee_id')))).groupBy(col('callarrays'))\\\n",
    "  .agg(count(col('duration')).alias('cnt'),sum(col('duration')).alias('sumduration'))\n",
    "\n",
    "israel_df = final_df.withColumn(\"exploded_callarrays\", explode(final_df[\"callarrays\"])).filter(col('exploded_callarrays').isin([7,9]))\\\n",
    ".select( (sum(col('sumduration'))/sum(col('cnt'))).alias('israel')).select(lit('israel').alias('country'),col('israel').alias('avg'))\n",
    "\n",
    "Morocco_df = final_df.withColumn(\"exploded_callarrays\", explode(final_df[\"callarrays\"])).filter(col('exploded_callarrays').isin([1,2]))\\\n",
    ".select( (sum(col('sumduration'))/sum(col('cnt'))).alias('Morocco')).select(lit('Morocco').alias('country'),col('Morocco').alias('avg'))\n",
    "\n",
    "peru_df = final_df.withColumn(\"exploded_callarrays\", explode(final_df[\"callarrays\"])).filter(col('exploded_callarrays').isin([3,12]))\\\n",
    ".select( (sum(col('sumduration'))/sum(col('cnt'))).alias('Peru')).select(lit('Peru').alias('country'),col('Peru').alias('avg'))\n",
    "\n",
    "israel_df.union(Morocco_df).union(peru_df).orderBy(desc('avg')).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "602d8ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|country|   avg|\n",
      "+-------+------+\n",
      "|   Peru|145.67|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person_df.createOrReplaceTempView('persondf')\n",
    "country_df.createOrReplaceTempView('countrydf')\n",
    "calls_df.createOrReplaceTempView('callsdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with code_list as (select collect_list(id) as coldelst,countrydf.name from person_df join countrydf on \n",
    "                       substring(person_df.phone_number,1,3) == countrydf.country_code group by countrydf.name),\n",
    "      code_array as (select array_sort(array(caller_id,callee_id)) as code, count(duration) as count_call, \n",
    "                       sum(duration) as total_duration from callsdf group by code),\n",
    "    explode_code as (select code,count_call,total_duration,explode(code) as ex_code from code_array),\n",
    "           logic as (select country,sum(avg)/count(country) as avg from (select case when ex_code in (3,12) then \"Peru\"\n",
    "                                 when ex_code in (1,2)  then \"Morocco\"\n",
    "                                 when ex_code in (7,9)  then \"Israel\" end as country,\n",
    "                            case when ex_code in (3,12) then round(sum(total_duration)/sum(count_call),2) \n",
    "                                 when ex_code in (1,2) then round(sum(total_duration)/sum(count_call),2) \n",
    "                                 when ex_code in (7,9) then round(sum(total_duration)/sum(count_call),2) end as avg \n",
    "                            from explode_code  group by ex_code) group by country),\n",
    "      global_avg as (select sum(duration)/count(duration) as global_avg from callsdf )\n",
    "                    select * from logic where avg > (select global_avg from  global_avg)\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef5fc68",
   "metadata": {},
   "source": [
    "#### 49 125 The Most Recent Three Orders M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "12060d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/49m_customers.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "62f519e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/49m_orders.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b1fe186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d995add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- cost: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "25b96962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 163:=============================================>       (172 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+--------+----------+\n",
      "|customer_name|customer_id|order_id|order_date|\n",
      "+-------------+-----------+--------+----------+\n",
      "|    Annabelle|          3|       7|2020-08-01|\n",
      "|    Annabelle|          3|       3|2020-07-31|\n",
      "|     Jonathan|          2|       6|2020-08-01|\n",
      "|     Jonathan|          2|       2|2020-07-30|\n",
      "|     Jonathan|          2|       9|2020-08-07|\n",
      "|       Marwan|          4|       4|2020-07-29|\n",
      "|      Winston|          1|       8|2020-08-03|\n",
      "|      Winston|          1|       1|2020-07-31|\n",
      "|      Winston|          1|      10|2020-07-15|\n",
      "+-------------+-----------+--------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 163:==============================================>      (175 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('customer_id').orderBy(col('order_date').desc())\n",
    "\n",
    "orders_df.withColumn('rnk',rank().over(window_spec)).filter(col('rnk') < 4).join(customers_df,on = 'customer_id')\\\n",
    ".select(col('name').alias('customer_name'),'customer_id','order_id','order_date')\\\n",
    ".orderBy(col('customer_name').asc(),col('customer_id').asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "58baf811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+--------+----------+\n",
      "|customer_name|customer_id|order_id|order_date|\n",
      "+-------------+-----------+--------+----------+\n",
      "|    Annabelle|          3|       7|2020-08-01|\n",
      "|    Annabelle|          3|       3|2020-07-31|\n",
      "|     Jonathan|          2|       6|2020-08-01|\n",
      "|     Jonathan|          2|       2|2020-07-30|\n",
      "|     Jonathan|          2|       9|2020-08-07|\n",
      "|       Marwan|          4|       4|2020-07-29|\n",
      "|      Winston|          1|       8|2020-08-03|\n",
      "|      Winston|          1|       1|2020-07-31|\n",
      "|      Winston|          1|      10|2020-07-15|\n",
      "+-------------+-----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.createOrReplaceTempView('ordersdf')\n",
    "customers_df.createOrReplaceTempView('customersdf')\n",
    "\n",
    "spark.sql('''\n",
    "         select name as customer_name,x.customer_id,x.order_id,x.order_date from \n",
    "         (select order_id,order_date,customer_id, rank() over(partition by customer_id order by order_date desc) as rnk from ordersdf) x\n",
    "              join customersdf on x.customer_id  = customersdf.customer_id\n",
    "              where x.rnk < 4\n",
    "              order by customer_name asc,customer_id asc\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07613bd2",
   "metadata": {},
   "source": [
    "#### 50 127 he Most Recent Orders for Each Product  M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d95f2956",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/50m_customers.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0a4a7178",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/50m_orders.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8db886e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/50m_products.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "17e6d3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1ed5dcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "98e12adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ba1fd701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+--------+----------+\n",
      "|product_name|product_id|order_id|order_date|\n",
      "+------------+----------+--------+----------+\n",
      "|    keyboard|         1|       6|2020-08-01|\n",
      "|    keyboard|         1|       7|2020-08-01|\n",
      "|       mouse|         2|       8|2020-08-03|\n",
      "|      screen|         3|       3|2020-08-29|\n",
      "+------------+----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('product_id').orderBy(col('order_date').desc())\n",
    "\n",
    "orders_df.withColumn('rnk',rank().over(window_spec)).filter(col('rnk') == 1).join(products_df,on = 'product_id')\\\n",
    ".select('product_name','product_id','order_id','order_date').orderBy(col('product_id').asc(),col('product_id').asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e56eee81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+--------+----------+\n",
      "|product_name|product_id|order_id|order_date|\n",
      "+------------+----------+--------+----------+\n",
      "|    keyboard|         1|       6|2020-08-01|\n",
      "|    keyboard|         1|       7|2020-08-01|\n",
      "|       mouse|         2|       8|2020-08-03|\n",
      "|      screen|         3|       3|2020-08-29|\n",
      "+------------+----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df.createOrReplaceTempView('customersdf')\n",
    "orders_df.createOrReplaceTempView('ordersdf')\n",
    "products_df.createOrReplaceTempView('productsdf')\n",
    "\n",
    "spark.sql('''\n",
    "           select product_name,x.product_id,x.order_id,x.order_date from \n",
    "                      (select product_id,order_id,order_date, rank() over(partition by product_id order by order_date desc) as rnk from ordersdf) x\n",
    "                 join productsdf on productsdf.product_id = x.product_id \n",
    "                 where x.rnk = 1\n",
    "                 order by product_name,x.product_id\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e97485",
   "metadata": {},
   "source": [
    "#### 51 128 Bank Account Summary M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "95aea4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/51m_users.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "88460719",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/51m_transactions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37cb8234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- credit: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "784a520d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trans_id: integer (nullable = true)\n",
      " |-- paid_by: integer (nullable = true)\n",
      " |-- paid_to: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- transacted_on: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c06f0a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------+------+-------------+\n",
      "|trans_id|paid_by|paid_to|amount|transacted_on|\n",
      "+--------+-------+-------+------+-------------+\n",
      "|       1|      1|      3|   400|   2020-08-01|\n",
      "|       2|      3|      2|   500|   2020-08-02|\n",
      "|       3|      2|      1|   200|   2020-08-03|\n",
      "+--------+-------+-------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "36629c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 281:===============================================>     (358 + 4) / 400]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+---------------------+\n",
      "|user_id|user_name|credit|credit_limit_breached|\n",
      "+-------+---------+------+---------------------+\n",
      "|      1| Moustafa|  -100|                  Yes|\n",
      "|      2| Jonathan|   500|                   No|\n",
      "|      3|  Winston|  9900|                   No|\n",
      "|      4|     Luis|   800|                   No|\n",
      "+-------+---------+------+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "paid_by = transactions_df.groupBy('paid_by').agg((- sum('amount')).alias('chage_amt')).withColumnRenamed('paid_by','user_id')\n",
    "paid_to = transactions_df.groupBy('paid_to').agg((sum('amount')).alias('chage_amt')).withColumnRenamed('paid_by','user_id')\n",
    "\n",
    "paid_by.union(paid_to).groupby('user_id').agg(sum('chage_amt').alias('change_rate'))\\\n",
    ".join(users_df,'user_id','right').withColumn('credit',(coalesce(col('change_rate'),lit('0')) + col('credit')))\\\n",
    ".withColumn('credit_limit_breached',when(col('credit') < 0,lit('Yes')).otherwise(lit('No')))\\\n",
    ".select('user_id','user_name',col('credit').cast('int').alias('credit'),'credit_limit_breached').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ce66f3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+---------------------+\n",
      "|user_id|user_name|credit|credit_limit_breached|\n",
      "+-------+---------+------+---------------------+\n",
      "|      1| Moustafa|  -100|                  Yes|\n",
      "|      2| Jonathan|   500|                   No|\n",
      "|      3|  Winston|  9900|                   No|\n",
      "|      4|     Luis|   800|                   No|\n",
      "+-------+---------+------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.createOrReplaceTempView('usersdf')\n",
    "transactions_df.createOrReplaceTempView('transactionsdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select user_id,sum(change_amt) as change_amt from (\n",
    "                   (select paid_by as user_id,sum(amount) * -1 as change_amt from transactionsdf group by paid_by)\n",
    "                   union\n",
    "                   (select paid_to as user_id,sum(amount) as change_amt from transactionsdf group by paid_to))\n",
    "                   group by user_id),\n",
    "       one as (select usersdf.user_id,user_name,credit,(coalesce(change_amt,0)) as change_amt from main\n",
    "                   right join usersdf on main.user_id == usersdf.user_id),\n",
    "       two as (select  user_id,user_name, (credit + change_amt) as credit from one)\n",
    "               select user_id,user_name,credit,case when credit < 0 then \"Yes\" else \"No\" end credit_limit_breached from two\n",
    "         ''').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054ee73e",
   "metadata": {},
   "source": [
    "#### 52 133 The Most Frequently Ordered Products for Each Customer M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f7e1b04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/52m_customers.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "855bb028",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/52m_orders.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8d0adf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/52m_products.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4d93beda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ded77ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7f7dc7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e6f91387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------------+\n",
      "|customer_id|product_id|product_name|\n",
      "+-----------+----------+------------+\n",
      "|          1|         2|       mouse|\n",
      "|          2|         1|    keyboard|\n",
      "|          2|         2|       mouse|\n",
      "|          2|         3|      screen|\n",
      "|          3|         3|      screen|\n",
      "|          4|         1|    keyboard|\n",
      "+-----------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('customer_id').orderBy(col('cnt').desc())\n",
    "orders_df.groupby('customer_id','product_id').agg(count('product_id').alias('cnt'))\\\n",
    ".withColumn('rnk',rank().over(window_spec)).filter(col('rnk') == 1).join(products_df,'product_id','inner')\\\n",
    ".select('customer_id','product_id','product_name').orderBy('customer_id','product_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6881cd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------------+\n",
      "|customer_id|product_id|product_name|\n",
      "+-----------+----------+------------+\n",
      "|          1|         2|       mouse|\n",
      "|          2|         1|    keyboard|\n",
      "|          2|         2|       mouse|\n",
      "|          2|         3|      screen|\n",
      "|          3|         3|      screen|\n",
      "|          4|         1|    keyboard|\n",
      "+-----------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df.createOrReplaceTempView('customersdf')\n",
    "orders_df.createOrReplaceTempView('ordersdf')\n",
    "products_df.createOrReplaceTempView('productsdf')\n",
    "\n",
    "\n",
    "range_df = spark.range()\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select customer_id,product_id,count(product_id) as cnt from ordersdf group by customer_id,product_id),\n",
    "       rnk as (select customer_id,product_id, rank() over(partition by customer_id order by cnt desc) as rnk from main)\n",
    "               select customer_id,rnk.product_id,product_name from rnk join productsdf on rnk.product_id = productsdf.product_id\n",
    "                    where rnk = 1 order by customer_id,product_id\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9986bce3",
   "metadata": {},
   "source": [
    "#### 53 135 Find the Missing IDs M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "98605d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/53m_customers.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bd82d7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "3d322dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|rng|\n",
      "+---+\n",
      "|  2|\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_num = customers_df.select(min(col('customer_id')).alias('min_num')).collect()[0][0]\n",
    "max_num = customers_df.select(max(col('customer_id')).alias('max_num')).collect()[0][0]\n",
    "\n",
    "df = spark.createDataFrame([(spark.range(min_num,max_num+1).collect()[i][0],)  for i in range(0,len(spark.range(min_num,max_num+1).collect()))],['rng'])\n",
    "\n",
    "cus_id  = [customers_df.select('customer_id').collect()[i][0] for i in range(0,len(customers_df.select('customer_id').collect()))]\n",
    "                                                                             \n",
    "df.filter(~col('rng').isin(cus_id)  ).show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "f2ebbaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  2|\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df.createOrReplaceTempView('customersdf')\n",
    "range_df.createOrReplaceTempView('rangedf')\n",
    "range_df = spark.range(min_num,max_num + 1)\n",
    "\n",
    "spark.sql('''\n",
    "          select id from rangedf where id not in (select customer_id  from customersdf)\n",
    "           ''').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2141e8",
   "metadata": {},
   "source": [
    "#### 54 143 Number of Calls Between Two Persons M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e9cd2752",
   "metadata": {},
   "outputs": [],
   "source": [
    "calls_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/54m_calls.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "901d92db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- from_id: integer (nullable = true)\n",
      " |-- to_id: integer (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calls_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "7edf70d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 999:===========================================>         (165 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+--------------+\n",
      "|person1|person2|call_count|total_duration|\n",
      "+-------+-------+----------+--------------+\n",
      "|      1|      2|         2|            70|\n",
      "|      1|      3|         1|            20|\n",
      "|      3|      4|         4|           999|\n",
      "+-------+-------+----------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('arr_id')\n",
    "window_rnk = Window.partitionBy('arr_id').orderBy(col('from_id').asc())\n",
    "\n",
    "calls_df.withColumn('arr_id',array_sort(array('from_id','to_id')))\\\n",
    ".withColumn('gp_id',sum('duration').over(window_spec)).withColumn('cnt_id',count('duration').over(window_spec))\\\n",
    ".withColumn('rnk',rank().over(window_rnk)).filter(col('rnk') == 1)\\\n",
    ".select(col('from_id').alias('person1'),col('to_id').alias('person2'),col('cnt_id').alias('call_count'),col('gp_id').alias('total_duration'))\\\n",
    ".distinct().orderBy('person1').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "46ad1d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1069:=============================================>      (176 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+--------------+\n",
      "|person1|person2|call_count|total_duration|\n",
      "+-------+-------+----------+--------------+\n",
      "|      1|      3|         1|            20|\n",
      "|      1|      2|         2|            70|\n",
      "|      3|      4|         4|           999|\n",
      "+-------+-------+----------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "calls_df.createOrReplaceTempView('callsdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select arr_id,from_id,to_id,duration, count(duration) over(partition by arr_id) as call_count,sum(duration) over(partition by arr_id) as total_duration from  \n",
    "                    (select from_id,to_id,duration,array_sort(array(from_id,to_id)) as arr_id from callsdf)),\n",
    "      one  as (select  from_id as person1,to_id as person2,duration,call_count,total_duration,arr_id,rank() over(partition by arr_id order by from_id) as rnk  from main)\n",
    "               select distinct person1,person2,call_count,total_duration from one where rnk = 1\n",
    "                     order by person1 \n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ba677a",
   "metadata": {},
   "source": [
    "#### 55 144 Biggest Window Between Visits M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1d8f77b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "usevisits_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/55m_useviisits.csv'))\n",
    "\n",
    "#today's date is '2021-1-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "f430ccf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- visit_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usevisits_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "d9e4358f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|user_id|biggest_window|\n",
      "+-------+--------------+\n",
      "|      1|            39|\n",
      "|      2|            65|\n",
      "|      3|            51|\n",
      "+-------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1156:================================================>   (185 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('user_id').orderBy(col('visit_date').asc())\n",
    "window_rnk = Window.partitionBy('user_id').orderBy(col('biggest_window').desc())\n",
    "\n",
    "usevisits_df.withColumn('lead_date',lead('visit_date',1,0).over(window_spec))\\\n",
    ".withColumn('start_date',when(col('lead_date') == 0,lit(\"2021-1-1\")).otherwise(col('lead_date')))\\\n",
    ".withColumn('biggest_window',datediff(col('start_date'),col('visit_date')))\\\n",
    ".withColumn('rnk',rank().over(window_rnk)).filter(col('rnk') == 1).select('user_id','biggest_window').orderBy('user_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "7aa79705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|user_id|biggest_window|\n",
      "+-------+--------------+\n",
      "|      1|            39|\n",
      "|      2|            65|\n",
      "|      3|            51|\n",
      "+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usevisits_df.createOrReplaceTempView('usevisitsdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select user_id,visit_date,lead(visit_date,1,0) over(partition by user_id order by visit_date asc) start_date from usevisitsdf),\n",
    "       one as (select user_id,visit_date,start_date,case when start_date = 0 then \"2021-1-1\" else start_date end as date from main),\n",
    "       two as (select user_id, datediff(date,visit_date) as biggest_window,rank() over(partition by user_id order by datediff(date,visit_date) desc) as rnk from one)\n",
    "               select user_id,biggest_window from two where rnk = 1 order by user_id asc\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2ee331",
   "metadata": {},
   "source": [
    "#### 56 145 Count Apples and Oranges M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1afb10da",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/56m_boxes.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "01aa5139",
   "metadata": {},
   "outputs": [],
   "source": [
    "chests_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/56m_chests.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "67f70655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- box_id: integer (nullable = true)\n",
      " |-- chest_id: integer (nullable = true)\n",
      " |-- apple_count: integer (nullable = true)\n",
      " |-- orange_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "boxes_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "81d910ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- chest_id: integer (nullable = true)\n",
      " |-- apple_count: integer (nullable = true)\n",
      " |-- orange_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chests_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "eb6136f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|apple_count|orange_count|\n",
      "+-----------+------------+\n",
      "|        151|         123|\n",
      "+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main_df = boxes_df.join(chests_df,boxes_df.chest_id == chests_df.chest_id,'left')\\\n",
    ".select('box_id',boxes_df.chest_id,boxes_df.apple_count.alias('apples'),boxes_df.orange_count.alias('oranges'),\n",
    "        coalesce(chests_df.apple_count,lit('0')).alias('chest_apple'),\n",
    "        coalesce(chests_df.orange_count,lit('0')).alias('chest_orange'))\n",
    "        \n",
    "one_df = main_df.withColumn('applecnt',(col('apples')+col('chest_apple') ))\\\n",
    ".withColumn('orangecnt',(col('oranges')+ col('chest_orange')))\\\n",
    "\n",
    "one_df.select(expr(\"sum(applecnt)\").cast('int').alias('apple_count'),lit('0').alias('orange_count'))\\\n",
    ".union(one_df.select(lit('0').alias('apple_count'),expr(\"sum(orangecnt)\").cast('int').alias('orange_count')))\\\n",
    ".select(expr(\"sum(apple_count)\").cast('int').alias('apple_count'),expr(\"sum(orange_count)\").cast('int').alias('orange_count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "a76dbe1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|apple_count|orange_count|\n",
      "+-----------+------------+\n",
      "|        151|         123|\n",
      "+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "boxes_df.createOrReplaceTempView('boxesdf')\n",
    "chests_df.createOrReplaceTempView('chestsdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select boxesdf.chest_id,boxesdf.apple_count,coalesce(chestsdf.apple_count,0) as chest_apple,\n",
    "                     boxesdf.orange_count,coalesce(chestsdf.orange_count,0) as chest_orange \n",
    "                     from boxesdf left join chestsdf on boxesdf.chest_id = chestsdf.chest_id),\n",
    "       one as ((select sum((apple_count+chest_apple)) as apple_count,'0' as orange_count from main) union(select '0' as apple_count ,\n",
    "                     sum((orange_count+chest_orange)) as orange_count from main))\n",
    "                     select cast(sum(apple_count) as int) as apple_count, cast(sum(orange_count) as int) as orange_count from one\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0a2c2a",
   "metadata": {},
   "source": [
    "#### 57 149 Leetflex Banned Accounts M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cd6903a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loginfo_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/57m_loginfo.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "faac5b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- account_id: integer (nullable = true)\n",
      " |-- ip_address: integer (nullable = true)\n",
      " |-- login: string (nullable = true)\n",
      " |-- logout: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loginfo_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "611c6e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|account_id|\n",
      "+----------+\n",
      "|         1|\n",
      "|         4|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('account_id').orderBy(col('ip_address').asc())\n",
    "\n",
    "odd_df = loginfo_df.withColumn('rnk',rank().over(window_spec)).filter(col('rnk') == 1)\n",
    "even_df = loginfo_df.withColumn('rnk',rank().over(window_spec)).filter(col('rnk') == 2)\n",
    "\n",
    "evendf = even_df.withColumnRenamed('account_id','even_account_id').withColumnRenamed('ip_address','even_ip_address')\\\n",
    ".withColumnRenamed('login','even_login').withColumnRenamed('logout','even_logout')\n",
    "\n",
    "evendf.join(odd_df,evendf.even_account_id == odd_df.account_id,'inner')\\\n",
    ".select(odd_df.account_id,odd_df.ip_address,evendf.even_ip_address\n",
    "       ,unix_timestamp(odd_df.login).alias('odd_login')\n",
    "       ,unix_timestamp(odd_df.logout).alias('odd_logout')\n",
    "       ,unix_timestamp(evendf.even_login).alias('even_login')\n",
    "       ,unix_timestamp(evendf.even_logout).alias('even_logout')\n",
    "    ,(unix_timestamp(odd_df.logout) - unix_timestamp(odd_df.login)).alias('odd_diff')\n",
    "        ,(unix_timestamp(evendf.even_logout) - unix_timestamp(evendf.even_login) ).alias('even_diff')\n",
    "      ).filter( (col('ip_address') != col('even_ip_address')) & (col('odd_diff') != col('even_diff')))\\\n",
    ".select('account_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "35938c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|account_id|\n",
      "+----------+\n",
      "|         1|\n",
      "|         1|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loginfo_df.alias('d1').join(loginfo_df.alias('d2'),(col('d1.account_id') == col('d2.account_id') )\n",
    "                            & (unix_timestamp(col('d1.login')) < unix_timestamp(col('d2.logout')) )\n",
    "                            & (unix_timestamp(col('d1.logout')) > unix_timestamp(col('d2.login')) )\n",
    "                            & (col('d1.ip_address') != col('d2.ip_address')), 'inner')\\\n",
    "          .select(col('d1.account_id')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed867829",
   "metadata": {},
   "source": [
    "#### 58 152 Grand Slam Titles M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7514b763",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/58m_players.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "89faf5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "championships_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/58m_championships.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd4d9798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- player_id: integer (nullable = true)\n",
      " |-- player_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "players_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eeac6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- wimbledon: integer (nullable = true)\n",
      " |-- Fr_open: integer (nullable = true)\n",
      " |-- US_open: integer (nullable = true)\n",
      " |-- Au_open: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "championships_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "e7d6dfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------------+\n",
      "|player_id|player_name|grand_slams_count|\n",
      "+---------+-----------+-----------------+\n",
      "|        1|      Nadal|                7|\n",
      "|        2|    Federer|                5|\n",
      "+---------+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "championships_df.withColumn('arr',array(col('wimbledon'),col('Fr_open'),col('US_open'),col('Au_open')))\\\n",
    ".groupBy().agg(explode(sort_array(flatten(collect_list(col('arr'))))).alias('lst')).groupby ('lst').agg(count('lst').alias('grand_slams_count'))\\\n",
    ".join(players_df,col('lst') == players_df.player_id,'inner').select('player_id','player_name','grand_slams_count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "42b56182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------------+\n",
      "|player_id|player_name|grand_slams_count|\n",
      "+---------+-----------+-----------------+\n",
      "|        1|      Nadal|                7|\n",
      "|        2|    Federer|                5|\n",
      "+---------+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "players_df.createOrReplaceTempView('playersdf')\n",
    "championships_df.createOrReplaceTempView('championshipsdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select explode(arr) as arrcol from \n",
    "                    (select array(wimbledon,Fr_open,US_open,Au_open) as arr from championshipsdf)),\n",
    "       one as (select arrcol,count(arrcol) as grand_slams_count from main group by arrcol)\n",
    "              select player_id,player_name,grand_slams_count from playersdf join one on one.arrcol = playersdf.player_id\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4334c1",
   "metadata": {},
   "source": [
    "#### 59 156 Find Interview Candidates M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a5a8f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "contests_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/59m_contests.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "881e33af",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/59m_users.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "347d5b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contest_id: integer (nullable = true)\n",
      " |-- gold_medal: integer (nullable = true)\n",
      " |-- silver_medal: integer (nullable = true)\n",
      " |-- bronze_medal: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contests_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf7ff62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- mail: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a71ce11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/12 10:39:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/12 10:39:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|user_id|              mail|\n",
      "+-------+------------------+\n",
      "|      1|sarah@leetcode.com|\n",
      "|      2|  bob@leetcode.com|\n",
      "|      3|alice@leetcode.com|\n",
      "|      5|quarz@leetcode.com|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.orderBy(col('contest_id').asc()).rowsBetween(0,2)\n",
    "window_one = Window.orderBy(col('contest_id').asc()).rowsBetween(0,1)\n",
    "\n",
    "first_df = contests_df.withColumn('arr',array(col('gold_medal'),col('silver_medal'),col(('bronze_medal'))))\n",
    "\n",
    "twodf = first_df.withColumn('arrcollect',collect_list('arr').over(window_spec)).select('contest_id','arrcollect')\n",
    "\n",
    "threedf = twodf.withColumn('ln',size('arrcollect')).filter(col('ln') >= 3).select('contest_id','arrcollect')\\\n",
    ".groupby('contest_id','arrcollect').agg(explode(flatten('arrcollect')).alias('flt'))\\\n",
    ".groupby('contest_id','flt').agg(count('flt').alias('cnt')).filter(col('cnt') >= 3).select(col('flt').alias('userid'))\n",
    "\n",
    "threemedaldf = threedf.join(users_df,threedf.userid == users_df.user_id,'inner').select('user_id','mail').distinct()\n",
    "\n",
    "window_spec_gold = Window.orderBy(col('user_id').desc())\n",
    "\n",
    "golddf = contests_df.groupBy('gold_medal').agg(count('gold_medal').alias('user_id')).withColumn('rnk',rank().over(window_spec_gold))\\\n",
    ".filter(col('rnk') == 1).select(col('gold_medal').alias('user_id')).join(users_df,'user_id','inner')\\\n",
    ".select('user_id','mail')\n",
    "\n",
    "threemedaldf.union(golddf).orderBy('user_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "966130d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/12 11:14:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/12 11:14:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|user_id|              mail|\n",
      "+-------+------------------+\n",
      "|      1|sarah@leetcode.com|\n",
      "|      2|  bob@leetcode.com|\n",
      "|      3|alice@leetcode.com|\n",
      "|      5|quarz@leetcode.com|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contests_df.createOrReplaceTempView('contestsdf')\n",
    "users_df.createOrReplaceTempView('usersdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with arr as (select contest_id,collect_list(array(gold_medal,silver_medal,bronze_medal)) over(order by contest_id asc rows between \n",
    "                0 PRECEDING and 2 FOLLOWING) as arrcollect from contestsdf),\n",
    "       one as (select contest_id,arrcollect from arr where size(arrcollect) >=3),\n",
    "       two as (select contest_id,explode(flatten(arrcollect)) as ids from one),\n",
    "     three as (select ids,count(ids) as id from two group by contest_id,ids),\n",
    "      four as (select distinct ids as user_id,mail from three join usersdf on usersdf.user_id = three.ids where id >= 3),\n",
    "   goldcnt as (select gold_medal,count(gold_medal) as cnt from contestsdf group by gold_medal),\n",
    "   goldrnk as (select gold_medal,cnt,rank() over(order by cnt desc) as rnk from goldcnt),\n",
    "      gold as (select gold_medal,mail as user_id from goldrnk join usersdf on usersdf.user_id = goldrnk.gold_medal where rnk = 1)\n",
    "               (select * from four) union(select * from gold) order by user_id\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e735ebf",
   "metadata": {},
   "source": [
    "#### 60 158 Maximum Transaction Each Day M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4aadc2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/60m_transactions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "e9d87cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "1457cace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|transaction_id|\n",
      "+--------------+\n",
      "|             1|\n",
      "|             6|\n",
      "|             5|\n",
      "|             8|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('datecol').orderBy(col('amount').desc())\n",
    "\n",
    "transactions_df.withColumn('datecol',substring(col('day'),1,9)).withColumn('mx',rank().over(window_spec))\\\n",
    ".filter(col('mx') == 1).select('transaction_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "7da33edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|transaction_id|\n",
      "+--------------+\n",
      "|             1|\n",
      "|             6|\n",
      "|             5|\n",
      "|             8|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.createOrReplaceTempView('transactionsdf')\n",
    "\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select transaction_id,substring(day,1,9) as day, amount from transactionsdf),\n",
    "       one as (select transaction_id,amount, rank() over(partition by day order by amount desc) as rnk from main)\n",
    "              select transaction_id from one where rnk = 1\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f840dc",
   "metadata": {},
   "source": [
    "#### 61 159 League Statistics M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "83c78c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/61m_teams.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7707f0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/61m_matches.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7af26827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- team_id: integer (nullable = true)\n",
      " |-- team_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teams_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b97ff7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- home_team_id: integer (nullable = true)\n",
      " |-- away_team_id: integer (nullable = true)\n",
      " |-- home_team_goals: integer (nullable = true)\n",
      " |-- away_team_goals: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matches_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "50ec5434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------------+---------------+\n",
      "|home_team_id|away_team_id|home_team_goals|away_team_goals|\n",
      "+------------+------------+---------------+---------------+\n",
      "|           1|           4|              0|              1|\n",
      "|           1|           6|              3|              3|\n",
      "|           4|           1|              5|              2|\n",
      "|           6|           1|              0|              0|\n",
      "+------------+------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matches_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "854eebb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1248:============================================>       (516 + 4) / 600]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+------+--------+------------+---------+\n",
      "|team_name|matches_played|points|goal_for|goal_against|goal_diff|\n",
      "+---------+--------------+------+--------+------------+---------+\n",
      "| Dortmund|             2|     6|       6|           2|        4|\n",
      "|  Arsenal|             2|     2|       3|           3|        0|\n",
      "|     Ajax|             4|     2|       5|           9|       -4|\n",
      "+---------+--------------+------+--------+------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "name = matches_df.withColumn('match_played',array('home_team_id','away_team_id'))\\\n",
    ".groupby().agg(explode(flatten(collect_list(col('match_played')))).alias('team_id'))\\\n",
    ".groupby('team_id').agg(count(col('team_id')).alias('matches_played')).join(teams_df,'team_id','inner')\\\n",
    ".select('team_id','team_name','matches_played')\n",
    "\n",
    "#####\n",
    "home_point = matches_df.withColumnRenamed('home_team_id','team').withColumn('point',when( (col('home_team_goals') - col('away_team_goals') ) < 0,0 )\n",
    "                                  .when( ( col('home_team_goals') > col('away_team_goals') ),3 )\n",
    "                                  .when(( col('home_team_goals') == col('away_team_goals') ),1))\\\n",
    "                                  .select('team','point')\n",
    "\n",
    "away_point = matches_df.withColumnRenamed('away_team_id','team').withColumn('point',when( (col('away_team_goals') - col('home_team_goals') ) < 0,0 )\n",
    "                                  .when( ( col('away_team_goals') > col('home_team_goals') ),3 )\n",
    "                                  .when(( col('home_team_goals') == col('away_team_goals') ),1))\\\n",
    "                                  .select('team','point')\n",
    "\n",
    "one = home_point.union(away_point).groupby('team').agg(sum(col('point')).alias('points'))\\\n",
    ".select('team','points',lit('0').alias('goal_for'),lit('0').alias('goal_against'))\n",
    "####\n",
    "home_goal = matches_df.groupby('home_team_id').agg(sum(col('home_team_goals')).alias('goal_for'))\\\n",
    ".select(col('home_team_id').alias('team'),'goal_for')\n",
    "\n",
    "away_goal = matches_df.groupby('away_team_id').agg(sum(col('away_team_goals')).alias('goal_for'))\\\n",
    ".select(col('away_team_id').alias('team'),'goal_for')\n",
    "\n",
    "two = home_goal.union(away_goal).groupby('team').agg(sum('goal_for').alias('goal_for'))\\\n",
    ".select('team',lit('0').alias('points'),'goal_for',lit('0').alias('goal_against'))\n",
    "####\n",
    "home_agnt = matches_df.groupby('home_team_id').agg(sum(col('away_team_goals')).alias('goal_against'))\\\n",
    ".select(col('home_team_id').alias('team'),'goal_against')\n",
    "\n",
    "away_agnt = matches_df.groupby('away_team_id').agg(sum(col('home_team_goals')).alias('goal_against'))\\\n",
    ".select(col('away_team_id').alias('team'),'goal_against')\n",
    "\n",
    "three = home_agnt.union(away_agnt).groupby('team').agg(sum('goal_against').alias('goal_against'))\\\n",
    ".select('team',lit('0').alias('points'),lit('0').alias('goal_for'),'goal_against')\n",
    "####\n",
    "logic = one.union(two).union(three).groupby('team').agg(sum('points').alias('points'),sum('goal_for').alias('goal_for'),sum('goal_against').alias('goal_against'))\n",
    "\n",
    "logic.join(name,name.team_id == logic.team,'inner').withColumn('goal_diff',expr(\"goal_for - goal_against\"))\\\n",
    ".select('team_name','matches_played',col('points').cast('int').alias('points'),col('goal_for').cast('int').alias('goal_for')\n",
    ",col('goal_against').cast('int').alias('goal_against'),col('goal_diff').cast('int').alias('goal_diff')).orderBy(col('goal_diff').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a08df3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b49a65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fb414a7",
   "metadata": {},
   "source": [
    "#### 62 160 Suspicious Bank Accounts M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8fc60b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/62m_accounts.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "585eff1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/62m_transactions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "961af72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- account_id: integer (nullable = true)\n",
      " |-- max_income: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accounts_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3d5834ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- account_id: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "151765f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|account_id|\n",
      "+----------+\n",
      "|         3|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('account_id','month')\n",
    "window_row = Window.partitionBy('account_id').orderBy(col('month').asc())\n",
    "\n",
    "transactions_df.filter(col('type') == lit('Creditor')).withColumn('month',substring('day',1,7))\\\n",
    ".select('account_id','amount','month').withColumn('excedamt',sum('amount').over(window_spec))\\\n",
    ".select('account_id','month','excedamt').distinct()\\\n",
    ".withColumn('comp',when( ((col('account_id') == lit('3')) & (col('excedamt') > 21000 )),lit('0'))\n",
    "                   .when( ((col('account_id') == lit('4')) & (col('excedamt') > 10400 )),lit('0')).otherwise(lit('1')))\\\n",
    ".withColumn('ls',lead('comp',1).over(window_row)).filter(col('comp') == col('ls')).select('account_id').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a0bbbcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|account_id|\n",
      "+----------+\n",
      "|         3|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.createOrReplaceTempView('transactionsdf')\n",
    "accounts_df.createOrReplaceTempView('accountsdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select account_id,amount,substring(day,1,7) as month from transactionsdf where type = \"Creditor\" ),\n",
    "       one as (select account_id,month,sum(amount) as amt from main group by account_id,month \n",
    "                    order by account_id,month),\n",
    "       two as (select account_id,month, case when (account_id = 3) and (amt > 21000) then \"0\"\n",
    "                                           when (account_id = 4) and (amt > 10400) then \"0\"\n",
    "                                           else \"1\" end as flag from one),\n",
    "     three as (select account_id,month,flag,lead(flag) over(partition by account_id order by month asc) as led from two)\n",
    "               select account_id from three where flag = led\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ef1f49",
   "metadata": {},
   "source": [
    "#### 63 162 Orders With Maximum Quantity Above Average M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f1a65eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "orderdetail_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/63m_orderdetail.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a35143d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderdetail_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "aca1fcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+-------+\n",
      "|order_id|               avg|max_qty|\n",
      "+--------+------------------+-------+\n",
      "|       1|12.333333333333334|     15|\n",
      "|       2|               5.5|      8|\n",
      "|       3|14.333333333333334|     20|\n",
      "|       4|               5.0|      8|\n",
      "+--------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderdetail_df.groupby('order_id').agg((sum(col('quantity'))/count(col('product_id'))).alias('avg'),max('quantity').alias('max_qty'))\\\n",
    ".filter(col('max_qty') > col('avg')).orderBy('order_id').show()\n",
    "                                                                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e7ca7356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+-----+\n",
      "|order_id|               avg|mxqty|\n",
      "+--------+------------------+-----+\n",
      "|       1|12.333333333333334|   15|\n",
      "|       3|14.333333333333334|   20|\n",
      "|       4|               5.0|    8|\n",
      "|       2|               5.5|    8|\n",
      "+--------+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderdetail_df.createOrReplaceTempView('orderdetaildf')\n",
    "\n",
    "spark.sql('''\n",
    "          select order_id,sum(quantity)/count(product_id) as avg, max(quantity) as mxqty \n",
    "               from orderdetaildf group by order_id having avg < mxqty\n",
    "       ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0e053d",
   "metadata": {},
   "source": [
    "#### 64 164 Group Employees of the Same Salary M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2f0eb7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/64m_employees.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8eb96815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "157a00f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/12 15:00:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------+----+\n",
      "|employee_id|   name|salary|rank|\n",
      "+-----------+-------+------+----+\n",
      "|          2|   Meir|  3000|   1|\n",
      "|          3|Michael|  3000|   1|\n",
      "|          7|Addilyn|  7400|   2|\n",
      "|          9| Kannon|  7400|   2|\n",
      "+-----------+-------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('salary')\n",
    "window_rnk = Window.orderBy(col('salary').asc())\n",
    "\n",
    "employees_df.withColumn('gp',count('salary').over(window_spec)).filter(col('gp') > 1)\\\n",
    ".withColumn('rank',dense_rank().over(window_rnk)).select('employee_id','name','salary','rank').orderBy('rank').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "3561466a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/12 15:07:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------+----+\n",
      "|employee_id|   name|salary|rank|\n",
      "+-----------+-------+------+----+\n",
      "|          2|   Meir|  3000|   1|\n",
      "|          3|Michael|  3000|   1|\n",
      "|          7|Addilyn|  7400|   2|\n",
      "|          9| Kannon|  7400|   2|\n",
      "+-----------+-------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.createOrReplaceTempView('employeesdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select employee_id,name,salary,count(salary) over(partition by salary) as gp from employeesdf)\n",
    "               select employee_id,name,salary,dense_rank() over(order by salary asc) as rank from main where gp > 1\n",
    "                    order by rank\n",
    "\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41378b8",
   "metadata": {},
   "source": [
    "#### 65 166 Count Salary Categories M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4bf56014",
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/65m_accounts.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d0c130c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- account_id: integer (nullable = true)\n",
      " |-- income: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accounts_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "a34160da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----------+\n",
      "|account_id|income|   category|\n",
      "+----------+------+-----------+\n",
      "|         3|108939|High Salary|\n",
      "|         2| 12747| Low Salary|\n",
      "|         8| 87709|High Salary|\n",
      "|         6| 91796|High Salary|\n",
      "+----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accounts_df.withColumn('category',when( ((col('income') > lit(20000)) & (col('income') < lit(50000))),lit('Average Salary'))\n",
    "                                  .when( (col('income') < lit(20000)),lit('Low Salary'))\n",
    "                                 .when( (col('income') > lit(50000)),lit('High Salary'))     \n",
    "                      ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "a7582464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|      category|accounts_count|\n",
      "+--------------+--------------+\n",
      "|    Low Salary|             1|\n",
      "|Average Salary|             0|\n",
      "|   High Salary|             3|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "catagory_df = spark.createDataFrame([('Low Salary',),('Average Salary',),('High Salary',)],[\"category\"] )\n",
    "\n",
    "accounts_df.withColumn('category',when( col('income').between(20000,50000),lit('Average Salary'))\n",
    "                                  .when( (col('income') < lit(20000)),lit('Low Salary'))\n",
    "                                 .when( (col('income') > lit(50000)),lit('High Salary')).otherwise(0)    \n",
    "                                ).groupby('category').agg(count('category').alias('accounts_count'))\\\n",
    ".join(catagory_df,'category','right').select('category',coalesce('accounts_count',lit('0')).alias('accounts_count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "8cd07f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|      category|accounts_count|\n",
      "+--------------+--------------+\n",
      "|    Low Salary|             1|\n",
      "|Average Salary|             0|\n",
      "|   High Salary|             3|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accounts_df.createOrReplaceTempView('accountsdf')\n",
    "catagory_df.createOrReplaceTempView('catagorydf')\n",
    "\n",
    "spark.sql('''\n",
    "  with main as (select case when ((income > 20000) and (income < 50000)) then \"Average Salary\"\n",
    "                  when income < 20000 then \"Low Salary\"\n",
    "                  when income > 50000 then \"High Salary\"\n",
    "                  else \"0\" end as category from accountsdf),\n",
    "        one as (select category,count(category) as accounts_count from main group by category)\n",
    "                select catagorydf.category,coalesce(accounts_count,'0') as accounts_count from one right join catagorydf on catagorydf.category = one.category\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85c38f2",
   "metadata": {},
   "source": [
    "#### 66 167  Confirmation Rate M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8f8a618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "signups_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/66m_signups.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b8c3a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmations_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/66m_confirmations.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "4ed8df8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- time_stamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "signups_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "c59879bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- time_stamp: string (nullable = true)\n",
      " |-- action: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confirmations_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "6d56fe2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|user_id|confirmation_rate|\n",
      "+-------+-----------------+\n",
      "|      2|              0.5|\n",
      "|      7|              1.0|\n",
      "|      6|              0.0|\n",
      "|      3|              0.0|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('user_id')\n",
    "window_cnt = Window.partitionBy('user_id','action')\n",
    "\n",
    "confirmations_df.withColumn('cnt',count('user_id').over(window_spec))\\\n",
    ".withColumn('concnt',when( (col('action') == lit('confirmed')),count('action').over(window_cnt)).otherwise(lit('0')))\\\n",
    ".withColumn('timcnt',when( (col('action') == lit('timeout')),count('action').over(window_cnt)).otherwise(lit('0')))\\\n",
    ".withColumn('preagg',when(col('concnt') != 0,col('concnt')/col('cnt')).otherwise(0)).filter(col('preagg') > 0)\\\n",
    ".select('user_id','preagg').join(signups_df,'user_id','right')\\\n",
    ".select(signups_df.user_id,round(coalesce(col('preagg'),lit('0')),2).alias('confirmation_rate')).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "0a2a148e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|user_id|confirmation_rate|\n",
      "+-------+-----------------+\n",
      "|      3|              0.0|\n",
      "|      7|              1.0|\n",
      "|      2|              0.5|\n",
      "|      6|              0.0|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "signups_df.createOrReplaceTempView('signupsdf')\n",
    "confirmations_df.createOrReplaceTempView('confirmationsdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select user_id,count(user_id) over(partition by user_id) as cnt,\n",
    "                    case when action = \"confirmed\" then count(action) over(partition by user_id,action) else 0 end  as concnt,\n",
    "                    case when action = \"timeout\" then count(action) over(partition by user_id,action) else 0 end  as timcnt from confirmationsdf),\n",
    "       one as (select distinct user_id,concnt/cnt as aggper from main where concnt > 0 )\n",
    "               select signupsdf.user_id,round(coalesce(aggper,0),2) as confirmation_rate  from one right join signupsdf on signupsdf.user_id = one.user_id   \n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9084fa64",
   "metadata": {},
   "source": [
    "#### 67 169 Strong Friendship M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ffef8dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "friendship_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/67m_friendship.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "3357af5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user1_id: integer (nullable = true)\n",
      " |-- user2_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "friendship_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "326bbb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/13 00:04:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/13 00:04:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------------+\n",
      "|user1_id|user2_id|common_friend|\n",
      "+--------+--------+-------------+\n",
      "|       1|       2|            4|\n",
      "|       1|       3|            3|\n",
      "+--------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_arrcomp = Window.orderBy(col('user1_id').asc())\n",
    "\n",
    "right = friendship_df.withColumn('rightarr',collect_list(col('user2_id')).over(window_arr)).select('user1_id','rightarr').distinct()\n",
    "left = friendship_df.withColumn('leftarr',collect_list(col('user1_id')).over(window_arr)).select('user2_id','leftarr').distinct()\n",
    "\n",
    "one = right.join(left,right.user1_id == left.user2_id,'left')\\\n",
    ".withColumn('distleftarr',coalesce(element_at(array_distinct('leftarr'),1),lit(0)))\\\n",
    ".withColumn('arr',array('distleftarr'))\\\n",
    ".withColumn('arryunion',array_union('rightarr','arr'))\\\n",
    ".withColumn('arrremove',array_remove('arryunion',1))\\\n",
    ".select('user1_id','user2_id','arrremove')\n",
    "\n",
    "main = one.withColumn('arrremove',array_remove('arrremove',0))\\\n",
    ".withColumn('sizearr',size('arrremove')).filter(col('sizearr') > 2).select('user1_id','arrremove')\n",
    "\n",
    "c_one = main.withColumn('user2_id',lead('user1_id',1).over(window_arrcomp))\\\n",
    ".withColumn('arrtwo',lead('arrremove',1).over(window_arrcomp))\\\n",
    ".withColumn('common_friend',size(array_intersect('arrremove','arrtwo'))).filter(col('common_friend') >= 3)\\\n",
    ".select('user1_id','user2_id','common_friend')\n",
    "\n",
    "c_two = main.withColumn('user2_id',lead('user1_id',2).over(window_arrcomp))\\\n",
    ".withColumn('arrtwo',lead('arrremove',2).over(window_arrcomp))\\\n",
    ".withColumn('common_friend',size(array_intersect('arrremove','arrtwo'))).filter(col('common_friend') >= 3)\\\n",
    ".select('user1_id','user2_id','common_friend')\n",
    "\n",
    "c_one.union(c_two).show()                                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e35ca3d",
   "metadata": {},
   "source": [
    "#### 68 170 All the Pairs With the Maximum Number of Common Followers M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ffa83667",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/68m_relatiions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "6f16aa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- follower_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "relations_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "ecdeecab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/13 00:41:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/13 00:41:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/13 00:41:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------+\n",
      "|user_id|user2_id|common_friend|\n",
      "+-------+--------+-------------+\n",
      "|      1|       7|            3|\n",
      "+-------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('user_id').orderBy(col('user_id').asc())\n",
    "window_arr = Window.partitionBy('user_id')\n",
    "window_ord = Window.orderBy(col('user_id').asc())\n",
    "\n",
    "main = relations_df.withColumn('gp_lead',lead('follower_id',1,0).over(window_spec))\\\n",
    ".withColumn('arraylist',collect_list('follower_id').over(window_arr)).select('user_id','arraylist').distinct()\n",
    "\n",
    "one = main.withColumn('user2_id',lead(col('user_id'),1).over(window_ord))\\\n",
    ".withColumn('commarr',lead('arraylist',1).over(window_ord))\\\n",
    ".withColumn('common_friend',size(array_intersect('arraylist','commarr'))).filter(col('common_friend') > 0)\\\n",
    ".select('user_id','user2_id','common_friend')\n",
    "\n",
    "two = main.withColumn('user2_id',lead(col('user_id'),2).over(window_ord))\\\n",
    ".withColumn('commarr',lead('arraylist',2).over(window_ord))\\\n",
    ".withColumn('common_friend',size(array_intersect('arraylist','commarr'))).filter(col('common_friend') > 0)\\\n",
    ".select('user_id','user2_id','common_friend')\n",
    "\n",
    "one.union(two).withColumn('rnk',rank().over(Window.orderBy(col('common_friend').desc()))).filter(col('rnk') == 1)\\\n",
    ".select('user_id','user2_id','common_friend').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "id": "239ff4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|user1_id|user2_id|\n",
      "+--------+--------+\n",
      "|       1|       7|\n",
      "|       7|       1|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main.alias('d1').crossJoin(main.alias('d2'))\\\n",
    ".select(col('d1.user_id').alias('d1userid'),col('d2.user_id').alias('d2userid'),\n",
    "        col('d1.arraylist').alias('d1arrlst'),col('d2.arraylist').alias('d2arrlst'))\\\n",
    ".withColumn('arr',size(array_intersect('d1arrlst','d2arrlst'))).filter(col('arr') > 2).filter(col('d1userid') != col('d2userid'))\\\n",
    ".select(col('d1userid').alias('user1_id'),col('d2userid').alias('user2_id')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "c184acf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|d1user|d2user|\n",
      "+------+------+\n",
      "|     1|     7|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "relations_df.createOrReplaceTempView('relationsdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with main as (select user_id,collect_list(follower_id) over(partition by user_id) as arr from relationsdf),\n",
    "        one as (select d1.user_id as d1user,d2.user_id as d2user,d1.arr as d1arr,d2.arr as d2arr from main d1 cross join main d2),\n",
    "        two as (select d1user,d2user,size(array_intersect(d1arr,d2arr)) as size from one)\n",
    "                select distinct d1user,d2user from two where size > 2 and d1user != d2user limit 1\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502ee8ff",
   "metadata": {},
   "source": [
    "#### 69 173 Find Cutoff Score for Each School M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "39b9f992",
   "metadata": {},
   "outputs": [],
   "source": [
    "schools_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/69m_schools.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "890c602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exam_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/69m_exam.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c6f3858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- school_id: integer (nullable = true)\n",
      " |-- capacity: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schools_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43528d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- score: integer (nullable = true)\n",
      " |-- student_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exam_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ea3aad0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|school_id|score|\n",
      "+---------+-----+\n",
      "|       11|  744|\n",
      "|        5|  975|\n",
      "|        9|   -1|\n",
      "|       10|  749|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy(\"school_id\").orderBy(col(\"score\").asc())\n",
    "\n",
    "schools_df.crossJoin(exam_df).filter(col('student_count') <=col('capacity') )\\\n",
    ".withColumn('divs',floor(expr(\"score/capacity\")))\\\n",
    ".withColumn('rnk',row_number().over(window_spec)).filter(col('rnk') == 1).select('school_id','score')\\\n",
    ".join(schools_df,'school_id','right').select('school_id',coalesce(col('score'),lit('-1')).alias('score')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a2433b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|school_id|score|\n",
      "+---------+-----+\n",
      "|       11|  744|\n",
      "|        5|  975|\n",
      "|        9|   -1|\n",
      "|       10|  749|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schools_df.createOrReplaceTempView('schools_df')\n",
    "exam_df.createOrReplaceTempView('exam_df')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select * from schools_df cross join exam_df where student_count <= capacity),\n",
    "       one as (select school_id,capacity,score,student_count, row_number() over(partition by school_id order by score asc) as rnk from main),\n",
    "       two as (select school_id,score from one  where rnk = 1)\n",
    "               select schools_df.school_id,coalesce(score,-1) as score from two right join schools_df on two.school_id = schools_df.school_id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990101bc",
   "metadata": {},
   "source": [
    "#### 70 174 Count the Number of Experiments M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8328e2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/70m_experiments.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "257ed913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- experiment_id: integer (nullable = true)\n",
      " |-- platform: string (nullable = true)\n",
      " |-- experiment_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiments_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "35dff5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+---------------+\n",
      "|platform|experiment_name|num_experiments|\n",
      "+--------+---------------+---------------+\n",
      "| Android|    Programming|              0|\n",
      "| Android|        Reading|              1|\n",
      "| Android|         Sports|              0|\n",
      "|     IOS|    Programming|              1|\n",
      "|     IOS|        Reading|              0|\n",
      "|     IOS|         Sports|              1|\n",
      "|     Web|    Programming|              1|\n",
      "|     Web|        Reading|              2|\n",
      "|     Web|         Sports|              0|\n",
      "+--------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('platform','exname').orderBy(col('platform').asc(),col('exname').asc())\n",
    "\n",
    "exp = experiments_df.select(col('platform').alias('explatform'),col('experiment_name').alias('exname'))\n",
    "main = experiment_name.crossJoin(platform).select('platform','experiment_name')\n",
    "\n",
    "main.join(exp, (main.platform == exp.explatform) & \n",
    "          (main.experiment_name == exp.exname) ,'left')\\\n",
    ".withColumn('num_experiments',count('exname').over(window_spec))\\\n",
    ".withColumn('experiment_name',when( (col('exname').isNull() ),col('experiment_name')).otherwise(col('exname')))\\\n",
    ".select('platform','experiment_name','num_experiments').distinct().orderBy(col('platform').asc(),col('experiment_name').asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "54b509c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+---------------+\n",
      "|platform|experimentname|num_experiments|\n",
      "+--------+--------------+---------------+\n",
      "| Android|   Programming|              0|\n",
      "| Android|       Reading|              1|\n",
      "| Android|        Sports|              0|\n",
      "|     IOS|   Programming|              1|\n",
      "|     IOS|       Reading|              0|\n",
      "|     IOS|        Sports|              1|\n",
      "|     Web|   Programming|              1|\n",
      "|     Web|       Reading|              2|\n",
      "|     Web|        Sports|              0|\n",
      "+--------+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiments_df.createOrReplaceTempView('experimentsdf')\n",
    "\n",
    "spark.sql('''\n",
    " with one as (select distinct platform from experimentsdf),\n",
    "      two as (select distinct experiment_name from experimentsdf),\n",
    "    three as (select * from one cross join two),\n",
    "  expname as (select platform as explatform,experiment_name as exname from experimentsdf ),\n",
    "    logic as (select platform,experiment_name,explatform,exname,\n",
    "                   case when exname is null then experiment_name else exname end experimentname\n",
    "                   from three left join expname on platform = explatform and experiment_name = exname)\n",
    "              select platform,experimentname,count(exname) as num_experiments from logic \n",
    "                   group by platform,experimentname order by platform asc,experimentname asc\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674a2bea",
   "metadata": {},
   "source": [
    "#### 71 175 Number of Accounts That Did Not Stream M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1fdb671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subscriptions_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/71m_subscriptions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7817f650",
   "metadata": {},
   "outputs": [],
   "source": [
    "streams_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/71m_streams.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "13b3a742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- account_id: integer (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      " |-- end_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subscriptions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "20d95fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: integer (nullable = true)\n",
      " |-- account_id: integer (nullable = true)\n",
      " |-- stream_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streams_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6969c233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|accounts_count|\n",
      "+--------------+\n",
      "|             2|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_year = subscriptions_df.filter(year('end_date') == lit('2021')).select('account_id')\n",
    "instream_year = streams_df.filter(year('stream_date') == lit('2021'))\n",
    "instream_year.join(in_year,'account_id','right').select(coalesce(col('session_id'),lit(-1)).alias('session_id'))\\\n",
    ".groupby('session_id').agg(count(col('session_id')).alias('accounts_count')).filter(col('session_id') == -1).select('accounts_count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "93f20758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|accounts_count|\n",
      "+--------------+\n",
      "|             2|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subscriptions_df.createOrReplaceTempView('subscriptionsdf')\n",
    "streams_df.createOrReplaceTempView('streamsdf')\n",
    "\n",
    "spark.sql('''\n",
    " with one as (select account_id from subscriptionsdf where year(end_date) = \"2021\"),\n",
    "      two as (select account_id,session_id from streamsdf where year(stream_date) = \"2021\"),\n",
    "    three as (select one.account_id,coalesce(session_id,-1) as session_id from one left join two on one.account_id = two.account_id)\n",
    "              select count(session_id) as accounts_count from three where session_id = -1\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2455e073",
   "metadata": {},
   "source": [
    "#### 72 177 Accepted Candidates From the Interviews M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8092ac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/72m_candidates.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b90f1a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/72m_rounds.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "7ac121aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- candidate_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- years_of_exp: integer (nullable = true)\n",
      " |-- interview_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "candidates_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "62e163e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- interview_id: integer (nullable = true)\n",
      " |-- round_id: integer (nullable = true)\n",
      " |-- score: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rounds_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e3249c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|candidate_id|\n",
      "+------------+\n",
      "|           9|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "one = candidates_df.filter(col('years_of_exp') >= 2)\n",
    "two = rounds_df.groupby('interview_id').agg(sum(col('score')).alias('score'))\n",
    "one.join(two,one.interview_id == two.interview_id,'inner').filter(col('score') > 15).select('candidate_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "242a76e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|candidate_id|\n",
      "+------------+\n",
      "|           9|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "candidates_df.createOrReplaceTempView('candidatesdf')\n",
    "rounds_df.createOrReplaceTempView('roundsdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with one as (select candidate_id,interview_id from candidatesdf where years_of_exp >= 2),\n",
    "       two as (select interview_id,sum(score) as score from roundsdf group by interview_id having score > 15)\n",
    "              select candidate_id from one join two on one.interview_id = two.interview_id\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d8f86a",
   "metadata": {},
   "source": [
    "#### 73 178 The Category of Each Member in the Store M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "eba04254",
   "metadata": {},
   "outputs": [],
   "source": [
    "members_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/73m_members.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "161efb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/73m_visits.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "03ee4011",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/73m_purchases.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b073e52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- member_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "members_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "172ab10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- visit_id: integer (nullable = true)\n",
      " |-- member_id: integer (nullable = true)\n",
      " |-- visit_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visits_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "00ba6c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- visit_id: integer (nullable = true)\n",
      " |-- charged_amount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purchases_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "388345fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1236:================================================>   (187 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+--------+\n",
      "|member_id|   name|category|\n",
      "+---------+-------+--------+\n",
      "|        1|Narihan|  Bronze|\n",
      "|        3|Winston|  Silver|\n",
      "|        8|  Hercy| Diamond|\n",
      "|        9|  Alice|    Gold|\n",
      "|       11|    Bob|  Silver|\n",
      "+---------+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('member_id')\n",
    "window_visit = Window.partitionBy('visit_id')\n",
    "\n",
    "one = members_df.join(visits_df,members_df.member_id == visits_df.member_id,'left')\\\n",
    ".select(members_df.member_id,members_df.name,coalesce(col('visit_id'),lit(0)).alias('visit_id'))\\\n",
    ".withColumn('cntmem',count('member_id').over(window_spec))\\\n",
    ".withColumnRenamed('visit_id','visitid')\n",
    "\n",
    "purchases_df.join(one,one.visitid == purchases_df.visit_id,'right')\\\n",
    ".withColumn('cntvisit',count('visit_id').over(window_visit))\\\n",
    ".withColumn('pre', round((100 * col('cntvisit'))/ col('cntmem'),2))\\\n",
    ".filter(~( (col('cntmem') >= 2) & (col('pre') == 0) )) \\\n",
    ".withColumn('category',when( (col('visitid') == 0 ),lit('Bronze'))\n",
    "                      .when( (col('pre') >= 80),lit('Diamond'))\n",
    "                      .when( (col('pre').between(50,80)),lit('Gold'))\n",
    "                      .when( (col('pre') < 50 ),lit('Silver')) )\\\n",
    ".select('member_id','name','category').distinct().orderBy(col('member_id').asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "9427635d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+--------+\n",
      "|member_id|   name|category|\n",
      "+---------+-------+--------+\n",
      "|        1|Narihan|  Bronze|\n",
      "|        3|Winston|  Silver|\n",
      "|        8|  Hercy| Diamond|\n",
      "|        9|  Alice|    Gold|\n",
      "|       11|    Bob|  Silver|\n",
      "+---------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "members_df.createOrReplaceTempView('membersdf')\n",
    "visits_df.createOrReplaceTempView('visitsdf')\n",
    "purchases_df.createOrReplaceTempView('purchasesdf')\n",
    "\n",
    "spark.sql('''\n",
    " with one as (select membersdf.member_id as member_id,name,coalesce(visit_id,0) as visitid \n",
    "                   from membersdf left join visitsdf on membersdf.member_id = visitsdf.member_id),\n",
    "   cntmem as (select member_id,name,visitid,count(member_id) over(partition by member_id) as cntmem from one),\n",
    " prelogic as (select member_id,name,visitid,cntmem,charged_amount,count(visit_id) over(partition by visit_id) as cntamt\n",
    "                   from cntmem left join purchasesdf on cntmem.visitid = purchasesdf.visit_id),\n",
    "  conrate as (select member_id,name,cntmem,cntamt,visitid,round((100 * cntamt/cntmem),2) as conrate  from prelogic),\n",
    "   result as (select member_id,name,cntmem,conrate,visitid,case when visitid  == 0  then \"Bronze\"\n",
    "                                         when conrate >= 80 then \"Diamond\"\n",
    "                                         when conrate between 50 and 80 then \"Gold\"\n",
    "                                         when conrate < 50 then \"Silver\" else 0 end as category \n",
    "                    from conrate where not (cntmem >= 2 and conrate = 0))\n",
    "              select member_id,name,category from result order by member_id\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d218fb74",
   "metadata": {},
   "source": [
    "#### 74 179 Account Balance M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "37f9fbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/74m_transactions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "556c8263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- account_id: integer (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "7f33e757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+\n",
      "|account_id|       day|balance|\n",
      "+----------+----------+-------+\n",
      "|         1|2021-11-07|   2000|\n",
      "|         1|2021-11-09|   1000|\n",
      "|         1|2021-11-11|   4000|\n",
      "|         2|2021-12-07|   7000|\n",
      "|         2|2021-12-12|      0|\n",
      "+----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_lead = Window.partitionBy('account_id').orderBy(col('day').asc())\n",
    "\n",
    "transactions_df.withColumn('ntvamt',when(col('type') == lit('Withdraw'),col('amount') * -1).otherwise(col('amount')))\\\n",
    ".withColumn('balance',sum('ntvamt').over(window_lead)).select('account_id','day','balance').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "d7d911a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+\n",
      "|account_id|       day|balance|\n",
      "+----------+----------+-------+\n",
      "|         1|2021-11-07|   2000|\n",
      "|         1|2021-11-09|   1000|\n",
      "|         1|2021-11-11|   4000|\n",
      "|         2|2021-12-07|   7000|\n",
      "|         2|2021-12-12|      0|\n",
      "+----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.createOrReplaceTempView('transactionsdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with main as (select account_id,day,case when type == \"Withdraw\" then amount * -1 else amount end amt \n",
    "                     from transactionsdf)\n",
    "                select account_id,day, sum(amt) over(partition by account_id order by day asc) as balance from main\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27ae498",
   "metadata": {},
   "source": [
    "#### 75 182 Drop Type 1 Orders for Customers With Type 0 Orders M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0e7ed3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/75m_orders.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "c16bd202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- order_type: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "86d88312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+\n",
      "|order_id|customer_id|order_type|\n",
      "+--------+-----------+----------+\n",
      "|       1|          1|         0|\n",
      "|       2|          1|         0|\n",
      "|      11|          2|         0|\n",
      "|      22|          3|         0|\n",
      "|      31|          4|         1|\n",
      "|      32|          4|         1|\n",
      "+--------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('customer_id','order_type').orderBy(col('customer_id').asc())\n",
    "\n",
    "orders_df.withColumn('cnt',count('order_type').over(window_spec))\\\n",
    ".withColumn('flag',when( col('cnt') == 2,lit('1'))\n",
    "                  .when( ((col('order_type') == lit('0')) & (col('cnt') == lit('1'))),lit('1')))\\\n",
    ".filter(col('flag') == 1).select('order_id','customer_id','order_type').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "af9f4093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+\n",
      "|order_id|customer_id|order_type|\n",
      "+--------+-----------+----------+\n",
      "|       1|          1|         0|\n",
      "|       2|          1|         0|\n",
      "|      11|          2|         0|\n",
      "|      22|          3|         0|\n",
      "|      31|          4|         1|\n",
      "|      32|          4|         1|\n",
      "+--------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.createOrReplaceTempView('ordersdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select order_id,customer_id,order_type,count(order_type) over(partition by customer_id,order_type) as cnt \n",
    "                    from ordersdf),\n",
    "       one as (select order_id,customer_id,order_type,case when cnt = 2 then 1\n",
    "                                                          when (order_type = 0) and (cnt = 1) then 1 \n",
    "                                                          end as flag from main)\n",
    "               select order_id,customer_id,order_type from one where flag = 1\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a0e58e",
   "metadata": {},
   "source": [
    "#### 76 183 The Airport With the Most Traffic M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a61ed87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/76m_flights.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "579371fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- departure_airport: integer (nullable = true)\n",
      " |-- arrival_airport: integer (nullable = true)\n",
      " |-- flights_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "8f0448f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/14 00:07:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 1836:================================================>   (376 + 4) / 400]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|airport|\n",
      "+-------+\n",
      "|      2|\n",
      "+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dep = flights_df.groupby('departure_airport').agg(sum(col('flights_count')).alias('flight_cnt')).withColumnRenamed('departure_airport','airport')\n",
    "arr = flights_df.groupby('arrival_airport').agg(sum(col('flights_count')).alias('flight_cnt')).withColumnRenamed('arrival_airport','airport')\n",
    "\n",
    "dep.union(arr).groupby('airport').agg(sum((col('flight_cnt'))).alias('flight_cnt'))\\\n",
    ".withColumn('rnk',rank().over(Window.orderBy(col('flight_cnt').desc()))).filter(col('rnk') == 1).select('airport').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "88575dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/14 00:21:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 1949:==========================================>         (329 + 5) / 400]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|airport|\n",
      "+-------+\n",
      "|      2|\n",
      "+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "flights_df.createOrReplaceTempView('flightsdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with main as ((select departure_airport as airport,sum(flights_count) as flights_count \n",
    "                          from flightsdf group by departure_airport)\n",
    "                      union(select arrival_airport as airport,sum(flights_count) as flights_count \n",
    "                          from flightsdf group by arrival_airport)),\n",
    "        one as (select airport,sum(flights_count) as flights_count from main group by airport),\n",
    "        two as (select airport,rank() over(order by flights_count desc) as rnk from one)\n",
    "                select airport from two where rnk = 1\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9cf116",
   "metadata": {},
   "source": [
    "#### 77 184 The Number of Passengers in Each Bus I M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a8fe2c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "buses_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/77m_buses.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6f5ae478",
   "metadata": {},
   "outputs": [],
   "source": [
    "passengers_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/77m_passengers.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "516adab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bus_id: integer (nullable = true)\n",
      " |-- arrival_time: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "buses_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "3daeb4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passenger_id: integer (nullable = true)\n",
      " |-- arrival_time: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "passengers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "a7c64b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+\n",
      "|bus_id|passengers_cnt|\n",
      "+------+--------------+\n",
      "|     1|             1|\n",
      "|     3|             3|\n",
      "|     2|             0|\n",
      "+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "one = buses_df.join(passengers_df,passengers_df.arrival_time <= buses_df.arrival_time,'left' )\\\n",
    ".groupBy(\"passenger_id\").agg(min(\"bus_id\").alias(\"earliest_bus_id\"))\n",
    "one.join(buses_df,one.earliest_bus_id == buses_df.bus_id,'right')\\\n",
    ".groupby('bus_id').agg(count('earliest_bus_id').alias('passengers_cnt')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "af81fb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+\n",
      "|bus_id|passengers_cnt|\n",
      "+------+--------------+\n",
      "|     1|             1|\n",
      "|     3|             3|\n",
      "|     2|             0|\n",
      "+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "buses_df.createOrReplaceTempView('busesdf')\n",
    "passengers_df.createOrReplaceTempView('passengersdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with main as (select * from busesdf left join \n",
    "                     passengersdf where passengersdf.arrival_time  <= busesdf.arrival_time),\n",
    "        one as (select passenger_id, min(bus_id) as earlier_bus from main group by passenger_id)\n",
    "                select distinct bus_id,coalesce(earlier_bus,0) as passengers_cnt from one right join busesdf on one.earlier_bus = busesdf.bus_id    \n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3944a3",
   "metadata": {},
   "source": [
    "#### 78 185 Order Two Columns Independently M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fb8a5540",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/78m_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1ac38cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- first_col: integer (nullable = true)\n",
      " |-- second_col: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d141fdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|first_col|second_col|\n",
      "+---------+----------+\n",
      "|        4|         2|\n",
      "|        2|         3|\n",
      "|        3|         1|\n",
      "|        1|         4|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf702ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/15 15:24:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/15 15:24:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|first_col|second_col|\n",
      "+---------+----------+\n",
      "|        1|         4|\n",
      "|        2|         3|\n",
      "|        3|         2|\n",
      "|        4|         1|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_rnk1  = Window.orderBy(col('first_col').asc())\n",
    "window_rnk2  = Window.orderBy(col('second_col').desc())\n",
    "\n",
    "one = data_df.select('first_col').orderBy(col('first_col').asc()).withColumn('rnk',rank().over(window_rnk1))\n",
    "two = data_df.select('second_col').orderBy(col('second_col').desc()).withColumn('rnk',rank().over(window_rnk2))\n",
    "\n",
    "one.join(two,'rnk','inner').select('first_col','second_col').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fbc5654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/15 15:31:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/15 15:31:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|first_col|second_col|\n",
      "+---------+----------+\n",
      "|        1|         4|\n",
      "|        2|         3|\n",
      "|        3|         2|\n",
      "|        4|         1|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.createOrReplaceTempView('datadf')\n",
    "\n",
    "spark.sql('''\n",
    "  with one as (select first_col,rank() over(order by first_col asc) as rnk from datadf),\n",
    "       two as (select second_col, rank() over (order by second_col desc) as rnk from datadf)\n",
    "               select one.first_col,two.second_col from one,two where one.rnk=two.rnk\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee202c9f",
   "metadata": {},
   "source": [
    "#### 79 186 The Change in Global Rankings M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5250bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "teampoints_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/79m_teampoints.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a0f51909",
   "metadata": {},
   "outputs": [],
   "source": [
    "pointschange_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/79m_pointschange.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "644da737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- team_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- points: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teampoints_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0416444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- team_id: integer (nullable = true)\n",
      " |-- points_change: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pointschange_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ce2d075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------+\n",
      "|team_id|       name|points|\n",
      "+-------+-----------+------+\n",
      "|      3|    Algeria|  1431|\n",
      "|      1|    Senegal|  2132|\n",
      "|      2|New Zealand|  1402|\n",
      "|      4|    Croatia|  1817|\n",
      "+-------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teampoints_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "85135eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------+---------+\n",
      "|team_id|       name|points|rank_diff|\n",
      "+-------+-----------+------+---------+\n",
      "|      1|    Senegal|  2110|        0|\n",
      "|      3|    Algeria|  1830|        1|\n",
      "|      4|    Croatia|  1830|       -1|\n",
      "|      2|New Zealand|  1402|        0|\n",
      "+-------+-----------+------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/15 15:47:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/15 15:47:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.orderBy(col('points').desc())\n",
    "window_rnk = Window.orderBy(col('diff').desc(),col('name').asc())\n",
    "\n",
    "teampoints_df.withColumn('rnkglobal',rank().over(window_spec)).join(pointschange_df,'team_id','inner')\\\n",
    ".withColumn('diff',expr(\"points + points_change\")).withColumn('rnkdiff',rank().over(window_rnk))\\\n",
    ".withColumn('rank_diff',col('rnkglobal') - col('rnkdiff')).select('team_id','name',col('diff').alias('points'),'rank_diff').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "753229c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----+---------+\n",
      "|team_id|       name|point|rank_diff|\n",
      "+-------+-----------+-----+---------+\n",
      "|      1|    Senegal| 2110|        0|\n",
      "|      3|    Algeria| 1830|        1|\n",
      "|      4|    Croatia| 1830|       -1|\n",
      "|      2|New Zealand| 1402|        0|\n",
      "+-------+-----------+-----+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/15 15:59:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/15 15:59:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "teampoints_df.createOrReplaceTempView('teampointsdf')\n",
    "pointschange_df.createOrReplaceTempView('pointschangedf')\n",
    "\n",
    "spark.sql('''\n",
    " with one as (select teampointsdf.team_id,name,points, rank() over(order by points desc) as rnk,points_change, (points + points_change) as pntchg\n",
    "                   from teampointsdf,pointschangedf where teampointsdf.team_id = pointschangedf.team_id),\n",
    "      two as (select team_id,name,pntchg as point,rnk,rank() over(order by pntchg desc, name asc) as rnktwo from one)\n",
    "              select team_id,name,point, (rnk - rnktwo) as rank_diff from two\n",
    "\n",
    "\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8b2fba",
   "metadata": {},
   "source": [
    "#### 80 188 Users With Two Purchases Within Seven Days M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "55ae68d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/80m_purchases.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e3a0a6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- purchase_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- purchase_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purchases_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "432a86b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|user_id|\n",
      "+-------+\n",
      "|      7|\n",
      "|      2|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('user_id').orderBy(col('purchase_date').asc())\n",
    "\n",
    "purchases_df.withColumn('leaddate',lead('purchase_date',1).over(window_spec))\\\n",
    ".withColumn('dffdate',datediff(col('purchase_date'),col('leaddate'))).filter(col('dffdate') <= 7)\\\n",
    ".select('user_id').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "49fd17a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|user_id|\n",
      "+-------+\n",
      "|      7|\n",
      "|      2|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purchases_df.createOrReplaceTempView('purchasesdf')\n",
    "\n",
    "spark.sql('''\n",
    " with one as (select user_id,purchase_date,lead(purchase_date,1) over(partition by user_id order by purchase_date asc) as leaddate \n",
    "               from purchasesdf),\n",
    "      two as (select user_id,datediff(purchase_date,leaddate) as diff from one)\n",
    "              select distinct user_id from two where diff <= 7\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a6b52f",
   "metadata": {},
   "source": [
    "#### 81 190 Number of Times a Driver Was a Passenger M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "216ca5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/81m_rides.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9c889a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ride_id: integer (nullable = true)\n",
      " |-- driver_id: integer (nullable = true)\n",
      " |-- passenger_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rides_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b5d25ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|driver_id|cnt|\n",
      "+---------+---+\n",
      "|        7|  2|\n",
      "|       11|  0|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "one = rides_df.select('driver_id').distinct()\n",
    "two = rides_df.groupby('passenger_id').agg(count('passenger_id').alias('cnt'))\n",
    "\n",
    "one.join(two,one.driver_id == two.passenger_id,'left')\\\n",
    ".select('driver_id',coalesce(col('cnt'),lit('0')).alias('cnt')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5d186bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|driver_id|cnt|\n",
      "+---------+---+\n",
      "|        7|  2|\n",
      "|       11|  0|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rides_df.createOrReplaceTempView('ridesdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with one as (select distinct driver_id from ridesdf),\n",
    "       two as (select passenger_id,count('passenger_id') as cnt from ridesdf group by passenger_id)\n",
    "               select driver_id,coalesce(cnt,0) as cnt from one left join two on one.driver_id = two.passenger_id \n",
    "\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42b71cd",
   "metadata": {},
   "source": [
    "#### 82 191 Products With Three or More Orders in Two Consecutive Years M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1aa4d128",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/82m_orders.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3cc34fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- purchase_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "499c9ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|product_id|\n",
      "+----------+\n",
      "|         1|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('product_id').orderBy(col('purchase_date').asc())\n",
    "\n",
    "orders_df.withColumn('rownum',row_number().over(window_spec)).filter(col('rownum') >=3)\\\n",
    ".select('product_id').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7708decf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|product_id|\n",
      "+----------+\n",
      "|         1|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.createOrReplaceTempView('ordersdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with one as (select product_id,row_number() over(partition by product_id order by purchase_date asc) as rownum\n",
    "                    from ordersdf)\n",
    "               select distinct product_id from one where rownum >=3\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91146d1d",
   "metadata": {},
   "source": [
    "#### 83 192 Tasks Count in the Weekend M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "782fb9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/83m_task.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "973dcbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- task_id: integer (nullable = true)\n",
      " |-- assignee_id: integer (nullable = true)\n",
      " |-- submit_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tasks_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "45e3cddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|weekend_cnt|working_cnt|\n",
      "+-----------+-----------+\n",
      "|          3|          3|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('weekend')\n",
    "\n",
    "tasks_df.withColumn('dayweek',dayofweek('submit_date'))\\\n",
    ".withColumn('weekend',when ( ((col('dayweek') >=2) & (col('dayweek') <= 6)),lit('0') ).otherwise(lit('1')))\\\n",
    ".withColumn('weekend_cnt',when(col('weekend') == lit(1),count('weekend').over(window_spec)))\\\n",
    ".withColumn('working_cnt',when(col('weekend') == lit(0),count('weekend').over(window_spec)))\\\n",
    ".select(coalesce('weekend_cnt',lit(0)).alias('weekend_cnt'),coalesce('working_cnt',lit(0)).alias('working_cnt'))\\\n",
    ".distinct().select(sum(col('weekend_cnt')).alias('weekend_cnt'),sum(col('working_cnt')).alias('working_cnt')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "52f7f2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|weekend_cnt|working_cnt|\n",
      "+-----------+-----------+\n",
      "|          3|          3|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tasks_df.createOrReplaceTempView('tasksdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select submit_date,dayofweek(submit_date) as day from tasksdf),\n",
    "       one as (select case when day between 2 and 6 then 0 else 1 end range from main),\n",
    "       two as (select case when range = 0 then count(range) end as working_cnt,\n",
    "                      case when range = 1 then count(range) end as weekend_cnt from one group by range)\n",
    "               select sum(coalesce(weekend_cnt,0)) as weekend_cnt,sum(coalesce(working_cnt,0)) as working_cnt from two\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92b2c71",
   "metadata": {},
   "source": [
    "#### 84 193 Arrange Table by Gender M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ce3882c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "genders_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/84m_genders.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b8b4d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "genders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "813a4065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 73:=========================>                             (93 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|user_id|gender|\n",
      "+-------+------+\n",
      "|      3|female|\n",
      "|      1| other|\n",
      "|      4|  male|\n",
      "|      7|female|\n",
      "|      2| other|\n",
      "|      5|  male|\n",
      "|      9|female|\n",
      "|      6| other|\n",
      "|      8|  male|\n",
      "+-------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 73:==============================================>       (171 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "winddow_spec = Window.partitionBy('flag').orderBy(col('flag').desc(),col('user_id').asc())\n",
    "\n",
    "main = genders_df.withColumn('flag',when(col('gender') == lit('female'),lit('1'))\n",
    "                            .when(col('gender') == lit('other'), lit('2'))\n",
    "                            .when(col('gender') == lit('male'),  lit('3')))\n",
    "one = main.withColumn('rnk',rank().over(winddow_spec)).filter(col('rnk') == 1).orderBy('flag').select('user_id','gender')\n",
    "two = main.withColumn('rnk',rank().over(winddow_spec)).filter(col('rnk') == 2).orderBy('flag').select('user_id','gender')\n",
    "three = main.withColumn('rnk',rank().over(winddow_spec)).filter(col('rnk') == 3).orderBy('flag').select('user_id','gender')\n",
    "\n",
    "one.union(two).union(three).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "808c1cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 540:==============================>                      (114 + 5) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|user_id|gender|\n",
      "+-------+------+\n",
      "|      3|female|\n",
      "|      1| other|\n",
      "|      4|  male|\n",
      "|      7|female|\n",
      "|      2| other|\n",
      "|      5|  male|\n",
      "|      9|female|\n",
      "|      6| other|\n",
      "|      8|  male|\n",
      "+-------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "genders_df.createOrReplaceTempView('gendersdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select user_id,gender,case when gender = \"female\" then 1\n",
    "                                      when gender = \"other\"  then 2\n",
    "                                      when gender =  \"male\"  then 3\n",
    "                                      end as flag from gendersdf),\n",
    "       one as (select user_id,gender,flag,rank() over(partition by flag order by flag desc,user_id asc) as rnk from main),\n",
    "       two as ((select user_id,gender,flag,1 as ord from one where rnk = 1 order by flag)\n",
    "               union\n",
    "               (select user_id,gender,flag,2 as ord from one where rnk = 2 order by flag)\n",
    "               union\n",
    "               (select user_id,gender,flag,3 as ord from one where rnk = 3 order by flag)\n",
    "               order by ord asc,flag asc)\n",
    "               select user_id,gender from two  \n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fbe793",
   "metadata": {},
   "source": [
    "#### 85 194 The First Day of the Maximum Recorded Degree in Each City M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6d24fcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/85m_weather.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "21c6a7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city_id: integer (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- degree: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c1a97b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+\n",
      "|city_id|       day|degree|\n",
      "+-------+----------+------+\n",
      "|      1|2022-01-07|    24|\n",
      "|      2|2022-08-07|    37|\n",
      "|      3|2022-02-07|    -6|\n",
      "+-------+----------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 535:===============================================>     (178 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('city_id')\n",
    "window_rnk = Window.partitionBy('city_id').orderBy(col('day').asc())\n",
    "\n",
    "weather_df.withColumn('maxtemp',max('degree').over(window_spec))\\\n",
    ".withColumn('rnk',rank().over(window_rnk)).filter(col('rnk') == 1)\\\n",
    ".select('city_id','day',col('maxtemp').alias('degree')).orderBy('city_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "cdc2a9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+\n",
      "|city_id|       day|degree|\n",
      "+-------+----------+------+\n",
      "|      1|2022-01-07|    24|\n",
      "|      2|2022-08-07|    37|\n",
      "|      3|2022-02-07|    -6|\n",
      "+-------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_df.createOrReplaceTempView('weatherdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select city_id,day,max(degree) over(partition by city_id) as degree,\n",
    "                     rank() over(partition by city_id order by day asc)  as rnk from weatherdf)\n",
    "            select city_id,day,degree from main where rnk = 1 order by city_id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8c4866",
   "metadata": {},
   "source": [
    "#### 86 195 Product Sales Analysis IV M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7f21cb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/86m_product.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "cd4978ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/86m_sales.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "900e02ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "83370b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sale_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "d7bfce3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|user_id|product_id|\n",
      "+-------+----------+\n",
      "|    101|         3|\n",
      "|    102|         3|\n",
      "|    102|         2|\n",
      "|    102|         1|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('user_id').orderBy(col('saleprice').desc())\n",
    "window_sumpid = Window.partitionBy('user_id','product_id')\n",
    "\n",
    "sales_df.join(product_df,'product_id','inner').withColumn('sumqty',sum('quantity').over(window_sumpid))\\\n",
    ".withColumn('saleprice',expr(\"sumqty * price\")).withColumn('mstspent',rank().over(window_spec)).filter(col('mstspent') == 1)\\\n",
    ".select('user_id','product_id').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "6c2f1d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|user_id|product_id|\n",
      "+-------+----------+\n",
      "|    101|         3|\n",
      "|    102|         3|\n",
      "|    102|         2|\n",
      "|    102|         1|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_df.createOrReplaceTempView('productdf')\n",
    "sales_df.createOrReplaceTempView('salesdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with main as (select productdf.product_id,user_id,quantity,price,sum(quantity) over(partition by user_id,productdf.product_id) as qty \n",
    "                     from productdf,salesdf where productdf.product_id = salesdf.product_id),\n",
    "                     \n",
    "        one as (select product_id,user_id, (qty * price) as salesprice from main),\n",
    "        two as (select product_id,user_id,rank() over(partition by user_id order by salesprice desc) as rnk from one)\n",
    "                select distinct user_id,product_id from two where rnk = 1\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81476658",
   "metadata": {},
   "source": [
    "#### 87 198 Compute the Rank as a Percentage M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "03c41482",
   "metadata": {},
   "outputs": [],
   "source": [
    "students_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/87m_students.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "e5193d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- student_id: integer (nullable = true)\n",
      " |-- department_id: integer (nullable = true)\n",
      " |-- mark: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "328c2a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------+\n",
      "|student_id|department_id|percentage|\n",
      "+----------+-------------+----------+\n",
      "|         7|            1|       0.0|\n",
      "|         1|            1|      50.0|\n",
      "|         3|            1|     100.0|\n",
      "|         2|            2|       0.0|\n",
      "|         8|            2|       0.0|\n",
      "+----------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('department_id')\n",
    "window_rnk = Window.partitionBy('department_id').orderBy(col('mark').desc())\n",
    "\n",
    "students_df.withColumn('cnt',count('student_id').over(window_spec))\\\n",
    ".withColumn('rnk',rank().over(window_rnk))\\\n",
    ".withColumn('percentage',round(expr(\" (rnk - 1) * 100 /(cnt - 1)\"),2))\\\n",
    ".select('student_id','department_id','percentage').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "b4e33712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------+\n",
      "|student_id|department_id|percentage|\n",
      "+----------+-------------+----------+\n",
      "|         7|            1|       0.0|\n",
      "|         1|            1|      50.0|\n",
      "|         3|            1|     100.0|\n",
      "|         2|            2|       0.0|\n",
      "|         8|            2|       0.0|\n",
      "+----------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students_df.createOrReplaceTempView('studentsdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with main as (select student_id,department_id,mark,count(student_id) over(partition by department_id) cnt,\n",
    "                     rank() over(partition by department_id order by mark desc) rnk from studentsdf)\n",
    "                select student_id,department_id,((rnk - 1) * 100 /(cnt - 1)) as percentage from main\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e550ed4",
   "metadata": {},
   "source": [
    "#### 88 200 Calculate the Influence of Each Salesperson M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0cd1fcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "salesperson_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/88m_salesperson.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a2b183bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/88m_customer.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1af4f491",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/88m_sales.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "707dc3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- salesperson_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salesperson_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "1c617c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- salesperson_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "d9699d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sale_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "bdd8ebdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+-----+\n",
      "|salesperson_id| name|total|\n",
      "+--------------+-----+-----+\n",
      "|             1|Alice| 1246|\n",
      "|             2|  Bob| 1844|\n",
      "|             3|Jerry|    0|\n",
      "+--------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.join(sales_df,'customer_id','inner').groupby('salesperson_id').agg(sum(col('price')).alias('total'))\\\n",
    ".join(salesperson_df,'salesperson_id','right').select('salesperson_id','name',coalesce('total',lit('0')).alias('total')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "b0f00fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+-----+\n",
      "|salesperson_id| name|total|\n",
      "+--------------+-----+-----+\n",
      "|             1|Alice| 1246|\n",
      "|             2|  Bob| 1844|\n",
      "|             3|Jerry|    0|\n",
      "+--------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salesperson_df.createOrReplaceTempView('salespersondf')\n",
    "customer_df.createOrReplaceTempView('customerdf')\n",
    "sales_df.createOrReplaceTempView('salesdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with main as (select salesperson_id,sum(price) as total from customerdf,salesdf \n",
    "                     where customerdf.customer_id = salesdf.customer_id group by salesperson_id)\n",
    "                select salespersondf.salesperson_id,name,coalesce(total,0) as total from salespersondf left join main on\n",
    "                      salespersondf.salesperson_id = main.salesperson_id\n",
    "\n",
    "\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9272c32a",
   "metadata": {},
   "source": [
    "#### 89 202 hange Null Values in a Table to the Previous Value M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a67c7859",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffeeshop_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/89m_coffeeshop.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7d738a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- drink: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coffeeshop_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "03c5e740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+-----+\n",
      "| id|            drink|index|\n",
      "+---+-----------------+-----+\n",
      "|  9|     Rum and Coke|    0|\n",
      "|  6|     Rum and Coke|    1|\n",
      "|  7|     Rum and Coke|    2|\n",
      "|  3|St Germain Spritz|    3|\n",
      "|  1| Orange Margarita|    4|\n",
      "|  2| Orange Margarita|    5|\n",
      "+---+-----------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/15 22:22:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.orderBy(col('index').asc()).rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "coffeeshop_df.withColumn('index',monotonically_increasing_id())\\\n",
    ".withColumn('drink',last('drink',ignorenulls=True).over(window_spec)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "25b93ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+-----+\n",
      "| id|            drink|index|\n",
      "+---+-----------------+-----+\n",
      "|  9|     Rum and Coke|    0|\n",
      "|  6|     Rum and Coke|    1|\n",
      "|  7|     Rum and Coke|    2|\n",
      "|  3|St Germain Spritz|    3|\n",
      "|  1| Orange Margarita|    4|\n",
      "|  2| Orange Margarita|    5|\n",
      "+---+-----------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/15 22:28:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.rowsBetween(-2,0)\n",
    "\n",
    "coffeeshop_df.withColumn('index',monotonically_increasing_id())\\\n",
    ".withColumn('drink',last('drink',ignorenulls=True).over(window_spec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "251f1acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+\n",
      "| id|            drink|\n",
      "+---+-----------------+\n",
      "|  9|     Rum and Coke|\n",
      "|  6|     Rum and Coke|\n",
      "|  7|     Rum and Coke|\n",
      "|  3|St Germain Spritz|\n",
      "|  1| Orange Margarita|\n",
      "|  2| Orange Margarita|\n",
      "+---+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/16 00:34:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "coffeeshop_df.createOrReplaceTempView('coffeeshopdf')\n",
    "\n",
    "spark.sql('''\n",
    "           select id,last(drink,True) over(ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as drink from coffeeshopdf\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b329e209",
   "metadata": {},
   "source": [
    "#### 90 203 Employees With Deductions M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b4d0eccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/90m_employees.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c2edb9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/90m_logs.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "92b00adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- needed_hours: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "d57297a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- in_time: string (nullable = true)\n",
      " |-- out_time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_df.printSchema() # unix_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "0c215784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|employee_id|\n",
      "+-----------+\n",
      "|          2|\n",
      "|          3|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('employee_id')\n",
    "\n",
    "logs_df.withColumn('uintime',expr(\"unix_timestamp(in_time)\")).withColumn('uouttime',expr(\"unix_timestamp(out_time)\"))\\\n",
    ".withColumn('diff',round( (expr(\"uouttime - uintime \")/3600),4))\\\n",
    ".withColumn('to',sum('diff').over(window_spec)).join(employees_df,'employee_id','right')\\\n",
    ".select('employee_id',coalesce('to',lit('0')).alias('timeworked'),'needed_hours')\\\n",
    ".filter(col('timeworked') < col('needed_hours')).select('employee_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "ff0b7e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|employee_id|\n",
      "+-----------+\n",
      "|          2|\n",
      "|          3|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.createOrReplaceTempView('employeesdf')\n",
    "logs_df.createOrReplaceTempView('logsdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select employee_id,\n",
    "                     sum(round((unix_timestamp(out_time) - unix_timestamp(in_time))/3600,4)) as workedhours from logsdf\n",
    "                     group by employee_id),\n",
    "       one as (select employeesdf.employee_id,coalesce(workedhours,0) as workedhours,needed_hours from main right join employeesdf\n",
    "                     on main.employee_id = employeesdf.employee_id)\n",
    "               select employee_id from one where workedhours < needed_hours\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea816d2d",
   "metadata": {},
   "source": [
    "#### 91 208 Immediate Food Delivery III M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0e1dc487",
   "metadata": {},
   "outputs": [],
   "source": [
    "delivery_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/91m_delivery.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2924b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- delivery_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_pref_delivery_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delivery_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a7a410d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:==================================>                    (127 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|order_date|immediate_percentage|\n",
      "+----------+--------------------+\n",
      "|2019-08-01|               66.67|\n",
      "|2019-08-02|               66.67|\n",
      "|2019-08-03|               100.0|\n",
      "|2019-08-04|                0.00|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:=================================================>     (179 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('order_date')\n",
    "window_cnt = Window.partitionBy('diff','order_date')\n",
    "\n",
    "one  = delivery_df.withColumn('cnt',count('order_date').over(window_spec))\\\n",
    ".withColumn('diff',datediff('customer_pref_delivery_date','order_date'))\\\n",
    ".withColumn('diffcnt',count('diff').over(window_cnt))\\\n",
    ".withColumn('immediate_percentage',round(expr(\"(diffcnt/cnt)*100\"),2) )\\\n",
    ".filter(col('diff') == 0)\\\n",
    ".select('order_date','immediate_percentage').distinct()\n",
    "\n",
    "delivery_df.select('order_date').distinct().join(one,'order_date','left')\\\n",
    ".select('order_date',coalesce('immediate_percentage',lit('0.00')).alias('immediate_percentage')).orderBy('order_date').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b95645b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 91:================================================>     (181 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|order_date|immediate_percentage|\n",
      "+----------+--------------------+\n",
      "|2019-08-01|               66.67|\n",
      "|2019-08-02|               66.67|\n",
      "|2019-08-03|               100.0|\n",
      "|2019-08-04|                 0.0|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delivery_df.createOrReplaceTempView('deliverydf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select order_date,count(order_date) over(partition by order_date) cnt,\n",
    "                    datediff(customer_pref_delivery_date,order_date) as diff from deliverydf),\n",
    "       one as (select cnt,diff,count(diff) over(partition by diff,order_date) diffcnt,order_date from main),\n",
    "       two as (select order_date,cnt,diff,diffcnt,round((diffcnt/cnt)*100,2) immediate_percentage from one),\n",
    "     three as (select distinct order_date,immediate_percentage from two where diff = 0 )\n",
    "               select distinct deliverydf.order_date,coalesce(immediate_percentage,0) as immediate_percentage from deliverydf \n",
    "                    left join three on deliverydf.order_date = three.order_date order by order_date\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51c9082",
   "metadata": {},
   "source": [
    "#### 92 210 Find Active Users M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2b64d607",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/92m_users.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ab440ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.printSchema() #greatest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "befc87df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|user_id|\n",
      "+-------+\n",
      "|      6|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('user_id').orderBy(col('created_at').asc())\n",
    "\n",
    "users_df.withColumn('rnk',lead('created_at',1).over(window_spec))\\\n",
    ".withColumn('diff',datediff('rnk','created_at')).filter(col('diff') <= 7).select('user_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9bde0a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|user_id|\n",
      "+-------+\n",
      "|      6|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.createOrReplaceTempView('usersdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with main as (select user_id,created_at,lead(created_at,1) over(partition by user_id order by created_at asc) diff from usersdf)\n",
    "                select user_id from main where datediff(diff,created_at) <= 7\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78f8fa1",
   "metadata": {},
   "source": [
    "#### 93 211 Count Occurrences in Text M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1cb6425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/93m_files.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bac9ab06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- file_name: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3249c7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| one|count|\n",
      "+----+-----+\n",
      "|bear|    2|\n",
      "|bull|    3|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_df.withColumn(\"one\",regexp_extract(col(\"content\"), r\"(bull)\", 0)).select('one')\\\n",
    ".union(file_df.withColumn(\"one\",regexp_extract(col(\"content\"), r\"(bear)\", 0)).select('one'))\\\n",
    ".groupby('one').agg(count('one').alias('count')).filter((col(\"one\") != \"\")).show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "790a040d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| one|count|\n",
      "+----+-----+\n",
      "|bear|    2|\n",
      "|bull|    3|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_df.createOrReplaceTempView('filedf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as ((select file_name,regexp_extract(content,\"(bull)\",0) as one  from filedf)\n",
    "                     union\n",
    "                         (select file_name, regexp_extract(content,\"(bear)\",0) as one  from filedf))\n",
    "             select one,count(one) as count from main group by one having one !=\"\"\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ded5d4",
   "metadata": {},
   "source": [
    "#### 94 212  Flight Occupancy and Waitlist Analysis M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e0744aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/94m_flights.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a36560a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "passengers_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/94m_passengers.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "952bf95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- flight_id: integer (nullable = true)\n",
      " |-- capacity: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "51395ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passenger_id: integer (nullable = true)\n",
      " |-- flight_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "passengers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0038f6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------------+\n",
      "|flight_id|booked_cnt|waitlist_cnt|\n",
      "+---------+----------+------------+\n",
      "|        1|         2|           1|\n",
      "|        2|         2|           0|\n",
      "|        3|         1|           1|\n",
      "+---------+----------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 377:====================================================>(198 + 2) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('flight_id')\n",
    "\n",
    "flights_df.join(passengers_df,'flight_id','inner').withColumn('cnt',count('passenger_id').over(window_spec))\\\n",
    ".withColumn('waitlist_cnt',expr(\"cnt - capacity\")).select('flight_id',col('capacity').alias('booked_cnt'),'waitlist_cnt')\\\n",
    ".distinct().orderBy('flight_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6653614e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n",
      "|flight_id|waitlist_cnt|\n",
      "+---------+------------+\n",
      "|        1|           1|\n",
      "|        2|           0|\n",
      "|        3|           1|\n",
      "+---------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 407:==============================================>      (175 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "passengers_df.createOrReplaceTempView('passengersdf')\n",
    "flights_df.createOrReplaceTempView('flightsdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select flightsdf.flight_id,capacity,passenger_id,count(passenger_id) over(partition by flightsdf.flight_id) as cnt \n",
    "                    from passengersdf join flightsdf on  passengersdf.flight_id = flightsdf.flight_id)\n",
    "                select distinct flight_id, (cnt-capacity) as waitlist_cnt from main order by flight_id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b256ad",
   "metadata": {},
   "source": [
    "#### 95 213 Election Results M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b0ac1dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "votes_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/95m_votes.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0d4f0577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- voter: string (nullable = true)\n",
      " |-- candidate: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "votes_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e2a5fd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/16 15:24:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|candidate|\n",
      "+---------+\n",
      "|Christine|\n",
      "|     Ryan|\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 518:============================================>        (168 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('voter')\n",
    "window_rnk = Window.orderBy(col('cndidate').desc())\n",
    "\n",
    "votes_df.withColumn('cnt',count('candidate').over(window_spec)).withColumn('cntg',count('voter').over(window_spec))\\\n",
    ".withColumn('logic',when(col('cntg') > 1,1/col('cntg')).otherwise(col('cnt') ))\\\n",
    ".withColumn('cndidate',round(sum('logic').over(Window.partitionBy('candidate')),2))\\\n",
    ".withColumn('rnk',rank().over(window_rnk)).filter(col('rnk') == 1).select('candidate').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "57b38f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 622:========================================>            (153 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|candidate|\n",
      "+---------+\n",
      "|Christine|\n",
      "|     Ryan|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "votes_df.createOrReplaceTempView('votesdf')\n",
    "\n",
    "spark.sql('''\n",
    " with one as (select voter,candidate,count(voter) over(partition by voter) as cnt,\n",
    "                   count(candidate) over(partition by candidate) as cancnt from votesdf),\n",
    "      two as (select voter,candidate,cnt,cancnt,case when cnt > 1 then round(1/cnt,2) else cnt end as ratiop from one)\n",
    "              select candidate from two group by candidate having sum(ratiop) > 2 \n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597ca3a2",
   "metadata": {},
   "source": [
    "#### 96 216 Rolling Average Steps M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "45d1e5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/96m_steps.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "9dbc931e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- steps_count: integer (nullable = true)\n",
      " |-- steps_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "steps_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "b6415989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------------+\n",
      "|user_id|steps_date|rolling_average|\n",
      "+-------+----------+---------------+\n",
      "|      1|2021-09-04|         535.33|\n",
      "|      1|2021-09-05|         595.67|\n",
      "|      2|2021-09-06|         284.67|\n",
      "|      3|2021-09-07|         505.67|\n",
      "|      3|2021-09-08|         674.67|\n",
      "+-------+----------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 930:==============================================>      (177 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('user_id').orderBy(col('steps_date').asc()).rowsBetween(0,2)\n",
    "window_led = Window.partitionBy('user_id').orderBy(col('steps_date'))\n",
    "window_cnt = Window.partitionBy('user_id')\n",
    "\n",
    "steps_df.withColumn('rng',avg(col('steps_count')).over(window_spec))\\\n",
    ".withColumn('lead',lead('steps_date',1).over(window_led))\\\n",
    ".withColumn('diff',datediff('lead','steps_date')).filter(col('diff') == 1)\\\n",
    ".withColumn('cnt',count('user_id').over(window_cnt)).filter(col('cnt') > 1)\\\n",
    ".withColumn('lst',last('steps_count').over(window_cnt)).filter(col('lst') != col('steps_count'))\\\n",
    ".select('user_id','steps_date',round(col('rng'),2).alias('rolling_average')).orderBy('user_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "45e5b896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------------+\n",
      "|user_id|steps_date|rolling_average|\n",
      "+-------+----------+---------------+\n",
      "|      1|2021-09-04|         535.33|\n",
      "|      1|2021-09-05|         595.67|\n",
      "|      2|2021-09-06|         284.67|\n",
      "|      3|2021-09-07|         505.67|\n",
      "|      3|2021-09-08|         674.67|\n",
      "+-------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "steps_df.createOrReplaceTempView('stepsdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with main as (select user_id,steps_count,steps_date,avg(steps_count) \n",
    "                     over(partition by user_id order by steps_date asc rows between 0 preceding and 2 Following) as avgrng,\n",
    "                     lead(steps_date,1) over(partition by user_id order by steps_date asc) leaddt from stepsdf),\n",
    "        one as (select user_id,steps_count,steps_date,avgrng,count(user_id) over(partition by user_id) as usrcnt,\n",
    "                      last(steps_count) over(partition by user_id) as lst from main where datediff(leaddt,steps_date)  = 1)\n",
    "                select distinct user_id,steps_date,round(avgrng,2) as rolling_average \n",
    "                      from one where lst != steps_count order by user_id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d81de5",
   "metadata": {},
   "source": [
    "#### 97 217 Calculate Orders Within Each Interval M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0b91b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/97m_orders.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "bb730fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- minute: integer (nullable = true)\n",
      " |-- order_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "3ebd2fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|interval_no|total_orders|\n",
      "+-----------+------------+\n",
      "|          1|          17|\n",
      "|          2|          18|\n",
      "+-----------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/16 20:12:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/16 20:12:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.orderBy(col('minute').asc()).rowsBetween(0,5)\n",
    "window_lead = Window.orderBy(col('minute').asc())\n",
    "\n",
    "orders_df.withColumn('total_orders',sum('order_count').over(window_spec))\\\n",
    ".withColumn('xx',lead('minute',6).over(window_lead))\\\n",
    ".withColumn('diff',expr(\"xx - minute\"))\\\n",
    ".withColumn('rnk',expr(\"xx-diff\")).filter(col('rnk') <= 2).select(col('rnk').alias('interval_no'),'total_orders').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "cf93aaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|interval_no|total_orders|\n",
      "+-----------+------------+\n",
      "|          1|          17|\n",
      "|          2|          18|\n",
      "+-----------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/16 20:20:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "orders_df.createOrReplaceTempView('ordersdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select minute,order_count,sum(order_count) \n",
    "                     over(order by minute asc rows between 0 preceding and 5 Following) as total_orders,\n",
    "                     lead(minute,6) over(order by minute asc) as le from ordersdf),\n",
    "       one as (select minute,order_count,total_orders,(le - minute) as diff,le - (le - minute) as rnk from main)\n",
    "               select rnk as interval_no,total_orders from one where rnk <= 2\n",
    "       ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3139922c",
   "metadata": {},
   "source": [
    "#### 98 218 Market Analysis III M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7b740111",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/98m_users.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "75e55198",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/98m_orders.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "37c294c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/98m_items.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "c08fb3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- seller_id: integer (nullable = true)\n",
      " |-- join_date: string (nullable = true)\n",
      " |-- favorite_brand: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "2d8f495d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- seller_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "1621a27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- item_brand: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "0e33bbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|seller_id|num_items|\n",
      "+---------+---------+\n",
      "|        2|        1|\n",
      "|        3|        1|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.join(items_df,'item_id','inner').join(users_df,'seller_id','right')\\\n",
    ".filter(col('favorite_brand') == col('item_brand')).groupby('seller_id').agg(count(col('favorite_brand')).alias('num_items'))\\\n",
    ".orderBy('seller_id').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "99866bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|seller_id|num_items|\n",
      "+---------+---------+\n",
      "|        2|        1|\n",
      "|        3|        1|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.createOrReplaceTempView('usersdf')\n",
    "orders_df.createOrReplaceTempView('ordersdf')\n",
    "items_df.createOrReplaceTempView('itemsdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with one as (select ordersdf.seller_id,item_brand,favorite_brand from itemsdf \n",
    "                    join ordersdf on itemsdf.item_id = ordersdf.item_id\n",
    "                    right join usersdf on usersdf.seller_id = ordersdf.seller_id)             \n",
    "               select seller_id,count(item_brand) as num_items from one where item_brand = favorite_brand \n",
    "                    group by seller_id order by seller_id                                        \n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55a6fd6",
   "metadata": {},
   "source": [
    "#### 99 219 Symmetric Coordinates M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f60066da",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/99m_coordinates.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "1ec829c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: integer (nullable = true)\n",
      " |-- y: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coordinates_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "56c6ccc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "| 20| 20|\n",
      "| 20| 21|\n",
      "| 22| 23|\n",
      "+---+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/16 22:34:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/16 22:34:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/16 22:34:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "window_y = Window.orderBy(col('x').asc())\n",
    "window_x = Window.orderBy(col('y').asc())\n",
    "winddow_spec = Window.orderBy('x')\n",
    "winddow_x = Window.orderBy('x2')\n",
    "\n",
    "first = coordinates_df.orderBy('x')\\\n",
    ".withColumn('y2',lead('y',1).over(window_y))\\\n",
    ".withColumn('ro',row_number().over(winddow_spec))\n",
    "\n",
    "second = coordinates_df.orderBy('x')\\\n",
    ".withColumn('x2',lead('x',1).over(window_x))\\\n",
    ".withColumn('ro',row_number().over(winddow_x)).select('x2','ro').orderBy(col('x2').asc_nulls_last())\\\n",
    ".select('x2',expr(\"ro - 1\").alias('ro'))\n",
    "\n",
    "first.join(second,'ro','inner').select(col('x').alias('x1'),'y2','x2',col('y').alias('y1'))\\\n",
    ".withColumn('flag',when( ((col('x1') == col('y2') ) & ( col('x2') == col('y1') )) ,lit('1')).otherwise(lit('0')))\\\n",
    ".filter(col('flag') == 1).select(col('x1').alias('x'),col('y1').alias('y')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a40029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af7df371",
   "metadata": {},
   "source": [
    "#### 100 220 Find Peak Calling Hours for Each City M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1a386e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "calls_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/100m_calls.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "0187b938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- caller_id: integer (nullable = true)\n",
      " |-- recipient_id: integer (nullable = true)\n",
      " |-- call_time: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calls_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "2c5cbc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+---------------+\n",
      "|    city|peak_calling_hour|number_of_calls|\n",
      "+--------+-----------------+---------------+\n",
      "| Houston|               22|              3|\n",
      "| Houston|               21|              1|\n",
      "|New York|               14|              1|\n",
      "|New York|               13|              1|\n",
      "+--------+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('peak_calling_hour')\n",
    "\n",
    "calls_df.withColumn('peak_calling_hour',hour('call_time')).withColumn('number_of_calls',count('peak_calling_hour').over(window_spec))\\\n",
    ".select('city','peak_calling_hour','number_of_calls').distinct()\\\n",
    ".orderBy(col('peak_calling_hour').desc(),'city').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "fe7c7fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+---------------+\n",
      "|    city|peak_calling_hour|number_of_calls|\n",
      "+--------+-----------------+---------------+\n",
      "| Houston|               22|              3|\n",
      "| Houston|               21|              1|\n",
      "|New York|               14|              1|\n",
      "|New York|               13|              1|\n",
      "+--------+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calls_df.createOrReplaceTempView('callsdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select city,hour(call_time) as peak_calling_hour from callsdf)\n",
    "             select city,peak_calling_hour,count(peak_calling_hour) as number_of_calls from main\n",
    "                  group by peak_calling_hour,city order by peak_calling_hour desc, city\n",
    "       ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a0790a",
   "metadata": {},
   "source": [
    "#### 101 222 Find Third Transaction M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "155ee782",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/101m_transactions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "23ba3f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- spend: double (nullable = true)\n",
      " |-- transaction_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "3f0237d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1637:===========================================>         (82 + 4) / 100]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+----------------------+\n",
      "|user_id|third_transaction_spend|third_transaction_date|\n",
      "+-------+-----------------------+----------------------+\n",
      "|      1|                  65.56|   2023-11-18 13:49:42|\n",
      "+-------+-----------------------+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1639:==============================================>       (65 + 4) / 75]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('user_id')\n",
    "window_rnk = Window.partitionBy('user_id').orderBy(col('transaction_date').asc())\n",
    "\n",
    "transactions_df.withColumn('cnt',count('user_id').over(window_spec)).filter(col('cnt') > 2)\\\n",
    ".withColumn('rnk',rank().over(window_rnk)).filter(col('rnk') <= 3)\\\n",
    ".withColumn('1st',lead('spend',1).over(window_rnk))\\\n",
    ".withColumn('1stdate',lead('transaction_date',1).over(window_rnk))\\\n",
    ".withColumn('2st',lead('1st',1).over(window_rnk))\\\n",
    ".withColumn('2stdate',lead('1stdate',1).over(window_rnk))\\\n",
    ".withColumn('logic',when( ((col('2st') > col('1st')) & (col('2st') > col('1st')) ),lit('1')).otherwise(lit('0')))\\\n",
    ".filter(col('logic') == 1).select('user_id',col('2st').alias('third_transaction_spend'),col('2stdate').alias('third_transaction_date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "089007c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+----------------------+\n",
      "|user_id|third_transaction_spend|third_transaction_date|\n",
      "+-------+-----------------------+----------------------+\n",
      "|      1|                  65.56|   2023-11-18 13:49:42|\n",
      "+-------+-----------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.createOrReplaceTempView('transactionsdf')\n",
    "\n",
    "spark.sql('''\n",
    "   with main as (select user_id,spend,transaction_date,rank() over(partition by user_id order by transaction_date asc) as rnk from transactionsdf),\n",
    "         one as (select user_id,spend,transaction_date,\n",
    "                      lead(spend,1) over(partition by user_id order by transaction_date asc) as 1st,\n",
    "                      lead(transaction_date,1) over(partition by user_id order by transaction_date asc) as 1stdate\n",
    "                      from main where rnk <= 3),\n",
    "         two as (select user_id,spend,transaction_date,1st,1stdate,\n",
    "                      lead(1st,1) over(partition by user_id order by transaction_date asc) as 2st,\n",
    "                      lead(1stdate,1) over(partition by user_id order by transaction_date asc) as 2stdate\n",
    "                      from one),\n",
    "       three as (select user_id,2st as third_transaction_spend,case when (2st > 1st) and (2st > spend) then 1 else 0 end as flag, \n",
    "                      2stdate as third_transaction_date from two)\n",
    "                 select user_id,third_transaction_spend,third_transaction_date from three where flag = 1\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edffc12b",
   "metadata": {},
   "source": [
    "#### 102 224 Manager of the Largest Department M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "cad9d61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/102m_employees.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "debffce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: integer (nullable = true)\n",
      " |-- emp_name: string (nullable = true)\n",
      " |-- dep_id: integer (nullable = true)\n",
      " |-- position: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "a294680a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/17 00:18:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|manager_name|dep_id|\n",
      "+------------+------+\n",
      "|    Isabella|   101|\n",
      "|      Joseph|   100|\n",
      "+------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1782:==================================================> (196 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('dep_id')\n",
    "window_rnk = Window.orderBy(col('cnt').desc())\n",
    "\n",
    "employees_df.withColumn('cnt',count('dep_id').over(window_spec))\\\n",
    ".withColumn('rnk',rank().over(window_rnk)).filter( (col('rnk') == 1) & (col('position') == lit('Manager')))\\\n",
    ".select(col('emp_name').alias('manager_name'),'dep_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "39efd26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/17 00:25:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|manager_name|dep_id|\n",
      "+------------+------+\n",
      "|    Isabella|   101|\n",
      "|      Joseph|   100|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.createOrReplaceTempView('employeesdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with one as (select emp_id,emp_name,dep_id,position,count(dep_id) over(partition by dep_id) as cnt from employeesdf),\n",
    "       two as (select emp_id,emp_name,dep_id,position,cnt,rank() over(order by cnt desc) as rnk from one)  \n",
    "               select emp_name as manager_name,dep_id from two where position = \"Manager\" and rnk = 1\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa1b4fe",
   "metadata": {},
   "source": [
    "#### 103 225 Class Performance M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "5fbfafbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/103m_scores.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7977c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- student_id: integer (nullable = true)\n",
      " |-- student_name: string (nullable = true)\n",
      " |-- assignment1: integer (nullable = true)\n",
      " |-- assignment2: integer (nullable = true)\n",
      " |-- assignment3: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "06b10615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|difference_in_score|\n",
      "+-------------------+\n",
      "|              111.0|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores_df.withColumn('sm',array('assignment1','assignment2','assignment3'))\\\n",
    ".withColumn('sum',aggregate('sm',lit(0.0),lambda acc, x: acc + x))\\\n",
    ".select( (expr(\"max(sum)\").alias('mx') - expr(\"min(sum)\").alias('mi')).alias('difference_in_score') ) .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "06da0f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|difference_in_score|\n",
      "+-------------------+\n",
      "|                111|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores_df.createOrReplaceTempView('scoresdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select (assignment1 + assignment2 + assignment3) as sum_array from scoresdf)\n",
    "               select (max(sum_array) - min(sum_array)) as difference_in_score from main\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200b1fc5",
   "metadata": {},
   "source": [
    "#### 104 227 Friday Purchases I M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c884fb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/104m_purchases.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "72e376ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- purchase_date: string (nullable = true)\n",
      " |-- amount_spend: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purchases_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cd0ecff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------------+\n",
      "|weekofmonth|purchase_date|total_amount|\n",
      "+-----------+-------------+------------+\n",
      "|          1|   2023-11-03|        5117|\n",
      "|          4|   2023-11-24|       21692|\n",
      "+-----------+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('purchase_date')\n",
    "\n",
    "purchases_df.withColumn('weekday',dayofweek('purchase_date'))\\\n",
    ".withColumn('weekofyear',weekofyear('purchase_date'))\\\n",
    ".withColumn('dayofmonth',dayofmonth('purchase_date'))\\\n",
    ".withColumn('firstdayofmonth',expr(\"date_sub(purchase_date, dayofmonth(purchase_date) - 1)\"))\\\n",
    ".withColumn('fistweekofmont',weekofyear('firstdayofmonth'))\\\n",
    ".withColumn('weekofmonth',expr(\"weekofyear - fistweekofmont + 1\"))\\\n",
    ".orderBy('purchase_date').filter(col('weekday') == 6 )\\\n",
    ".withColumn('total_amount',sum('amount_spend').over(window_spec))\\\n",
    ".select('weekofmonth','purchase_date','total_amount').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b7571702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------------+\n",
      "|weekofmonth|purchase_date|total_amount|\n",
      "+-----------+-------------+------------+\n",
      "|          1|   2023-11-03|        5117|\n",
      "|          4|   2023-11-24|       21692|\n",
      "+-----------+-------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 107:=================================================>   (187 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "purchases_df.createOrReplaceTempView('purchasesdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select purchase_date,amount_spend,dayofweek(purchase_date) as weekday,\n",
    "                                    weekofyear(purchase_date) as weekofyear,\n",
    "                                    dayofmonth(purchase_date) as dayofmonth,\n",
    "                                    date_sub(purchase_date,dayofmonth(purchase_date) -1) as firstdayofmonth\n",
    "                                    from  purchasesdf),\n",
    "       one as (select purchase_date,amount_spend,weekday,weekofyear,dayofmonth,firstdayofmonth,\n",
    "                                    weekofyear(firstdayofmonth) as fistweekofmonth,\n",
    "                                    (weekofyear - weekofyear(firstdayofmonth) + 1) as weekofmonth\n",
    "                                    from main where weekday == 6)\n",
    "               select distinct weekofmonth,purchase_date,sum(amount_spend) over(partition by purchase_date) as total_amount \n",
    "                    from one order by purchase_date\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c0c75e",
   "metadata": {},
   "source": [
    "#### 105 228 Pizza Toppings Cost Analysis M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c48f0599",
   "metadata": {},
   "outputs": [],
   "source": [
    "toppings_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/105m_toppings.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "657cd945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- topping_name: string (nullable = true)\n",
      " |-- cost: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toppings_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8d596edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+----------+\n",
      "|lst                           |total_cost|\n",
      "+------------------------------+----------+\n",
      "|Chicken,Extra Cheese,Pepperoni|1.45      |\n",
      "|Chicken,Pepperoni,Sausage     |1.75      |\n",
      "|Chicken,Extra Cheese,Sausage  |1.65      |\n",
      "|Extra Cheese,Pepperoni,Sausage|1.60      |\n",
      "+------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('ln')\n",
    "window_cnt = Window.partitionBy('srt')\n",
    "window_rnk = Window.partitionBy('srt').orderBy(col('cntsrt').desc())\n",
    "\n",
    "toppings_df.alias('d1').crossJoin(toppings_df.alias('d2')).crossJoin(toppings_df.alias('d3'))\\\n",
    ".select(array('d1.topping_name','d2.topping_name','d3.topping_name').alias('arr'))\\\n",
    ".withColumn('xxx',array_distinct('arr'))\\\n",
    ".withColumn('ln',size('xxx')).filter(col('ln') > 2)\\\n",
    ".withColumn('cnt',count('ln').over(window_spec)).select('arr').distinct().orderBy('arr')\\\n",
    ".withColumn('srt',array_sort('arr'))\\\n",
    ".withColumn('cntsrt',count('srt').over(window_cnt))\\\n",
    ".withColumn('rnk',row_number().over(window_rnk)).filter(col('rnk') == 1).select('arr')\\\n",
    ".withColumn('lst',array_join('arr',',')).select('lst')\\\n",
    ".withColumn('total_cost',when( col('lst') == lit(\"Chicken,Extra Cheese,Pepperoni\"),expr(\"0.55+0.4+0.5\"))\n",
    "                        .when( col('lst') == lit(\"Chicken,Pepperoni,Sausage\"),expr(\"0.55+0.5+0.7\"))\n",
    "                        .when( col('lst') == lit(\"Chicken,Extra Cheese,Sausage\"),expr(\"0.55+0.4+0.7\"))\n",
    "                        .when(col('lst') == lit(\"Extra Cheese,Pepperoni,Sausage\"),expr(\"0.4+0.5+0.7\")))\\\n",
    ".show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "97069605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+----------+\n",
      "|pizza                         |total_cost|\n",
      "+------------------------------+----------+\n",
      "|Chicken,Extra Cheese,Pepperoni|1.45      |\n",
      "|Chicken,Extra Cheese,Sausage  |1.75      |\n",
      "|Chicken,Pepperoni,Sausage     |1.65      |\n",
      "|Extra Cheese,Pepperoni,Sausage|1.60      |\n",
      "+------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toppings_df.createOrReplaceTempView('toppingsdf')\n",
    "\n",
    "spark.sql('''\n",
    " with one as (select d1.topping_name from toppingsdf as d1),\n",
    "      two as (select d2.topping_name from toppingsdf as d2),\n",
    "    three as (select d3.topping_name from toppingsdf as d3),\n",
    "     four as (select one.topping_name,two.topping_name,three.topping_name,\n",
    "                   array(one.topping_name,two.topping_name,three.topping_name) as arr_name\n",
    "                   from one cross join two cross join three),\n",
    "     five as (select distinct array_sort(arr_name) as arr_name from four where size(array_distinct(arr_name))  = 3 order by arr_name),\n",
    "      six as (select array_join(arr_name,',') as pizza from five)\n",
    "              select pizza, case when pizza = \"Chicken,Extra Cheese,Pepperoni\" then 0.55+0.4+0.5\n",
    "                                when pizza = \"Chicken,Extra Cheese,Sausage\" then 0.55+0.5+0.7\n",
    "                                when pizza = \"Chicken,Pepperoni,Sausage\" then 0.55+0.4+0.7\n",
    "                                when pizza = \"Extra Cheese,Pepperoni,Sausage\" then 0.4+0.5+0.7\n",
    "                                end total_cost\n",
    "                    from six\n",
    "       ''').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db62908",
   "metadata": {},
   "source": [
    "#### 106 231 Binary Tree Nodes M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "49be17e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/106m_tree.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "055b9ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- N: integer (nullable = true)\n",
      " |-- P: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "e1e85553",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_df = tree_df.select(col('N').alias('id'),coalesce(col('P'),lit('0')).alias('p_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "ca80be95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| type|\n",
      "+---+-----+\n",
      "|  1| leaf|\n",
      "|  2|inner|\n",
      "|  3| leaf|\n",
      "|  5| root|\n",
      "|  6| leaf|\n",
      "|  8|inner|\n",
      "|  9| leaf|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "root_df =  tree_df.withColumn('type',when((col('p_id')) == 0,lit('root'))).filter(col('type') == lit('root'))\\\n",
    ".select('id','type')\n",
    "\n",
    "leaf_df = tree_df.alias('d1').join(tree_df.alias('d2'),col('d1.id')==col('d2.p_id'),'anti')\\\n",
    ".withColumn('type',lit('leaf')).select('id','type')\n",
    "\n",
    "inner_df = tree_df.alias('d1').join(tree_df.alias('d2'),col('d1.id')==col('d2.p_id'),'leftsemi')\\\n",
    ".withColumn('type',lit('inner')).select('id','type')\\\n",
    ".filter((col('id') != tree_df.filter(col('p_id') == 0).select('id').collect()[0][0]))\n",
    "\n",
    "root_df.union(leaf_df).union(inner_df).orderBy('id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "7c671437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| type|\n",
      "+---+-----+\n",
      "|  1| leaf|\n",
      "|  2|inner|\n",
      "|  3| leaf|\n",
      "|  5| root|\n",
      "|  6| leaf|\n",
      "|  8|inner|\n",
      "|  9| leaf|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree_df.createOrReplaceTempView('treedf')\n",
    "\n",
    "spark.sql('''\n",
    "  with root as (select id,'root' as type from treedf where p_id = 0),\n",
    "       leaf as (select id, 'leaf' as type from treedf as d1 anti join treedf as d2 on d1.id == d2.p_id ),\n",
    "      inner as (select id, 'inner' as type from treedf as d1 left semi join treedf as d2 on d1.id == d2.p_id \n",
    "                    where id not in (select id from root))\n",
    "                select * from root union(select * from leaf) union (select * from inner) order by id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de803049",
   "metadata": {},
   "source": [
    "#### 107 232 Top Percentile Fraud M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "032929df",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/107m_fraud.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "2ef70e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- policy_id: integer (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- fraud_score: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fraud_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "0376498a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+\n",
      "|policy_id|     state|fraud_score|\n",
      "+---------+----------+-----------+\n",
      "|        1|California|       0.92|\n",
      "|       11|   Florida|       0.98|\n",
      "|        4|  New York|       0.94|\n",
      "|        7|     Texas|       0.98|\n",
      "+---------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('state')\n",
    "\n",
    "fraud_df.withColumn('per',percentile_approx('fraud_score',0.95).over(window_spec))\\\n",
    ".filter(col('fraud_score') == col('per')).select('policy_id','state','fraud_score').orderBy('state').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "5c942f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+\n",
      "|policy_id|     state|fraud_score|\n",
      "+---------+----------+-----------+\n",
      "|        1|California|       0.92|\n",
      "|       11|   Florida|       0.98|\n",
      "|        4|  New York|       0.94|\n",
      "|        7|     Texas|       0.98|\n",
      "+---------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fraud_df.createOrReplaceTempView('frauddf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select policy_id,state,fraud_score,percentile_approx(fraud_score,0.95) over(partition by state) as percentail \n",
    "                     from frauddf)\n",
    "            select policy_id,state,fraud_score from main where fraud_score == percentail order by state\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30476606",
   "metadata": {},
   "source": [
    "#### 108 233 Snaps Analysis M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "0977cfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/108m_activities.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "dacf01a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/108m_age.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "8a366c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- activity_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- activity_type: string (nullable = true)\n",
      " |-- time_spent: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activities_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "a5e711f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- age_bucket: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "age_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "333e6b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+\n",
      "|age_bucket|send_perc|open_perc|\n",
      "+----------+---------+---------+\n",
      "|     31-35|    37.84|    62.16|\n",
      "|     21-25|    54.31|    45.69|\n",
      "|     26-30|    82.26|    17.74|\n",
      "+----------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('user_id')\n",
    "\n",
    "send = activities_df.filter(col('activity_type') == lit('send'))\\\n",
    ".select('user_id',col('time_spent').alias('send_time')).withColumn('send_user',sum('send_time').over(window_spec))\n",
    "\n",
    "open = activities_df.filter(col('activity_type') == lit('open'))\\\n",
    ".select('user_id',col('time_spent').alias('open_time')).withColumn('open_user',sum('open_time').over(window_spec))\n",
    "\n",
    "age_df.join(send,'user_id','inner').join(open,'user_id','inner')\\\n",
    ".withColumn('total_user',expr(\"send_user + open_user\"))\\\n",
    ".withColumn(('send_perc'),round(expr(\"send_user/total_user\") * 100,2))\\\n",
    ".withColumn(('open_perc'),round(expr(\"open_user/total_user\") * 100,2))\\\n",
    ".select('age_bucket','send_perc','open_perc').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "badd66e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+\n",
      "|age_bucket|send_perc|open_perc|\n",
      "+----------+---------+---------+\n",
      "|     31-35|    37.84|    62.16|\n",
      "|     21-25|    54.31|    45.69|\n",
      "|     26-30|    82.26|    17.74|\n",
      "+----------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activities_df.createOrReplaceTempView('activitiesdf')\n",
    "age_df.createOrReplaceTempView('agedf')\n",
    "\n",
    "spark.sql('''\n",
    " with send_user as (select user_id,sum(time_spent) over(partition by user_id) as send_user from activitiesdf where activity_type = \"send\"),\n",
    "      open_user as (select user_id,sum(time_spent) over(partition by user_id) as open_user from activitiesdf where activity_type = \"open\"), \n",
    "      totaltime as (select agedf.user_id,age_bucket,send_user,open_user,(send_user + open_user) as total_user from agedf\n",
    "                         join send_user on send_user.user_id = agedf.user_id\n",
    "                         join open_user on open_user.user_id = agedf.user_id)\n",
    "              select distinct age_bucket,round(((send_user/total_user)*100),2) as send_perc,\n",
    "                    round(((open_user/total_user)*100),2) as open_perc from totaltime\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289ac1ec",
   "metadata": {},
   "source": [
    "#### 109 234 Friends With No Mutual Friends M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e7f5f524",
   "metadata": {},
   "outputs": [],
   "source": [
    "friends_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/109m_friends.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b22a9017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id1: integer (nullable = true)\n",
      " |-- user_id2: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "friends_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c95b36a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 628:==========================================>          (161 + 5) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|user_id1|user_id2|\n",
      "+--------+--------+\n",
      "|       6|       7|\n",
      "|       8|       9|\n",
      "+--------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('user_id1')\n",
    "\n",
    "left = friends_df.withColumn('cnt',count('user_id1').over(window_spec)).filter(col('cnt').isin(['2','3']))\\\n",
    ".withColumn('arr',explode(array('user_id1','user_id2'))).select('arr').distinct().collect()\n",
    "\n",
    "out = [left[i][0] for i in range(0,len(left))]\n",
    "\n",
    "right = friends_df.withColumn('cnt',count('user_id1').over(window_spec)).filter(col('cnt').isin(['1']))\\\n",
    ".withColumn('arr',explode(array('user_id1','user_id2'))).select('arr').distinct()\n",
    "\n",
    "right.filter(~col('arr').isin(out)).join(friends_df,right.arr == friends_df.user_id1,'inner')\\\n",
    ".select('user_id1','user_id2').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78a02f8",
   "metadata": {},
   "source": [
    "#### 110 236 Find Trending Hashtags M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "aa7513f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/110m_tweets.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "11fe45d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- tweet_id: integer (nullable = true)\n",
      " |-- tweet: string (nullable = true)\n",
      " |-- tweet_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_df.printSchema() #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "378bff45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "|  hashtag|hashtag_count|\n",
      "+---------+-------------+\n",
      "|#HappyDay|            3|\n",
      "|#TechLife|            2|\n",
      "|  #Nature|            1|\n",
      "|#WorkLife|            1|\n",
      "+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "widow_spec = Window.partitionBy('regex')\n",
    "\n",
    "tweets_df.withColumn('regex',regexp_extract('tweet',r\"(#\\w+)\",1))\\\n",
    ".withColumn('hashtag_count',count('regex').over(widow_spec))\\\n",
    ".select(col('regex').alias('hashtag'),'hashtag_count').distinct().orderBy(col('hashtag_count').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "2cd52b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "|hashtag  |hashtag_count|\n",
      "+---------+-------------+\n",
      "|#HappyDay|3            |\n",
      "|#TechLife|2            |\n",
      "|#Nature  |1            |\n",
      "|#WorkLife|1            |\n",
      "+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_df.createOrReplaceTempView('tweetsdf')\n",
    "\n",
    "spark.sql('''\n",
    " with main as (select regexp_extract(tweet,\"(#\\HappyDay|\\#TechLife|\\#Nature|#WorkLife)\", 1) as hashtag from tweetsdf)\n",
    "               select hashtag,count(hashtag) as hashtag_count from main group by hashtag order by hashtag_count desc \n",
    "         ''').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5a2680",
   "metadata": {},
   "source": [
    "#### 111 237  Find Bursty Behavior M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d4d83a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/111m_posts.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "48124524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- post_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- post_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "posts_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264f0e73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00e4a0a5",
   "metadata": {},
   "source": [
    "#### 112 238 Friday Purchase III  M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "3a61c076",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/112m_purchases.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "489006cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/112m_users.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efc24a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- purchase_date: string (nullable = true)\n",
      " |-- amount_spend: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purchases_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a54d235d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- membership: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6440d0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 704:===================================================> (390 + 4) / 400]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------------+\n",
      "|weekofmonth|membership|amount_spend|\n",
      "+-----------+----------+------------+\n",
      "|          1|   Premium|      1126.0|\n",
      "|          1|       VIP|         0.0|\n",
      "|          2|   Premium|         0.0|\n",
      "|          2|       VIP|      7473.0|\n",
      "|          3|   Premium|           0|\n",
      "|          3|       VIP|           0|\n",
      "|          4|   Premium|     17117.0|\n",
      "|          4|       VIP|     14933.0|\n",
      "+-----------+----------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('membership','weekofmonth')\n",
    "\n",
    "one = purchases_df.join(users_df,'user_id','inner')\\\n",
    ".withColumn('weekday',dayofweek('purchase_date'))\\\n",
    ".withColumn('weekofyear',weekofyear('purchase_date'))\\\n",
    ".withColumn('dayofmonth',dayofmonth('purchase_date'))\\\n",
    ".withColumn('firstdayofmonth',expr(\"date_sub(purchase_date,dayofmonth(purchase_date) -1)\"))\\\n",
    ".withColumn('firstweekofmonth',weekofyear('firstdayofmonth'))\\\n",
    ".withColumn('weekofmonth',expr(\"weekofyear - firstweekofmonth +1\"))\n",
    "\n",
    "premium = one.select('weekofmonth','membership','amount_spend').filter(col('membership') == lit('Premium')).distinct()\n",
    "vip = one.select(col('weekofmonth').alias('vipweekofmonth'),col('membership').alias('vipmembership'),col('amount_spend').alias('vipamount_spend') )\\\n",
    ".filter(col('membership') == lit('VIP')).distinct()\n",
    "\n",
    "main = premium.join(vip,premium.weekofmonth == vip.vipweekofmonth,'outer')\\\n",
    ".withColumn('weekofmonth',when(col('weekofmonth').isNull(),col('vipweekofmonth')).otherwise(col('weekofmonth') ))\\\n",
    ".withColumn('vipweekofmonth',when(col('vipweekofmonth').isNull(),col('weekofmonth')).otherwise(col('vipweekofmonth')))\n",
    "\n",
    "x = main.select('weekofmonth',coalesce('membership',lit('Premium')).alias('membership'),\n",
    "                            coalesce('amount_spend',lit('0')).alias('amount_spend')).distinct()\n",
    "\n",
    "y = main.select(col('vipweekofmonth').alias('weekofmonth'),coalesce('vipmembership',lit('VIP')).alias('membership'),\n",
    "                            coalesce('vipamount_spend',lit('0')).alias('amount_spend')).distinct()\n",
    "left = x.union(y).orderBy('weekofmonth','membership').withColumn('amount_spend',sum('amount_spend').over(window_spec)).distinct()\n",
    "\n",
    "z = one.select('weekofmonth').filter(col('weekofmonth') == 3).crossJoin(left.select('membership').distinct())\\\n",
    ".select('weekofmonth','membership',lit('0').alias('total_amount'))\n",
    "\n",
    "left.union(z).orderBy('weekofmonth','membership').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16558963",
   "metadata": {},
   "source": [
    "#### 113 239 Find Longest Calls M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c04cbb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "contacts_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/113m_contacts.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "ac2da9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "calls_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/113m_calls.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "98473d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contacts_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "766ab537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contact_id: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calls_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ff039715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------------------+\n",
      "|first_name|    type|duration_formatted|\n",
      "+----------+--------+------------------+\n",
      "|   Michael|incoming|          00:07:00|\n",
      "|      Jane|incoming|          00:05:00|\n",
      "|     Emily|incoming|          00:03:00|\n",
      "|     Alice|outgoing|          00:06:00|\n",
      "|     Emily|outgoing|          00:04:40|\n",
      "|      Jane|outgoing|          00:04:00|\n",
      "+----------+--------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/18 15:01:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/18 15:01:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.orderBy(col('duration').desc())\n",
    "\n",
    "indoming = calls_df.filter(col('type') == lit('incoming'))\\\n",
    ".withColumn('rnk',rank().over(window_spec)).filter(col('rnk') <= 3)\n",
    "\n",
    "outgoing = calls_df.filter(col('type') == lit('outgoing'))\\\n",
    ".withColumn('rnk',rank().over(window_spec)).filter(col('rnk') <= 3)\n",
    "\n",
    "one = indoming.union(outgoing).join(contacts_df,col('id') == col('contact_id'),'inner')\\\n",
    ".select('first_name','type','duration')\\\n",
    ".withColumn('hours',floor(expr(\"duration/3600\")))\\\n",
    ".withColumn('minutes',floor(expr(\"duration % 3600 / 60\")))\\\n",
    ".withColumn('seconds',floor(expr(\"duration % 60\")))\\\n",
    ".withColumn('duration_formatted',concat(lpad('hours',2,'0'),lit(':'),\n",
    "                                        lpad('minutes',2,'0'),lit(':'),\n",
    "                                        lpad('seconds',2,'0')))\\\n",
    ".select('first_name','type','duration_formatted').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d8e4a972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/18 15:22:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/18 15:22:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------+------------------+\n",
      "|first_name|    type|duration|duration_formatted|\n",
      "+----------+--------+--------+------------------+\n",
      "|     Alice|outgoing|     360|          00:06:00|\n",
      "|   Michael|incoming|     420|          00:07:00|\n",
      "|      Jane|outgoing|     240|          00:04:00|\n",
      "|      Jane|incoming|     300|          00:05:00|\n",
      "|     Emily|outgoing|     280|          00:04:40|\n",
      "|     Emily|incoming|     180|          00:03:00|\n",
      "+----------+--------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contacts_df.createOrReplaceTempView('contactsdf')\n",
    "calls_df.createOrReplaceTempView('callsdf')\n",
    "\n",
    "spark.sql('''\n",
    " with one as (select contact_id,type,duration,rank() over(order by duration) as rnk from callsdf where type = \"incoming\"),\n",
    "      two as (select contact_id,type,duration,rank() over(order by duration) as rnk from callsdf where type = \"outgoing\"),\n",
    "    third as ((select contact_id,type,duration from one where rnk >= 3) \n",
    "                    union (select contact_id,type,duration from two where rnk >=3)),\n",
    "     four as (select first_name ,type,duration,floor(duration/3600) as hour,\n",
    "                   floor(duration/60) as minutes,floor(duration%60) as seconds from contactsdf join third on id = contact_id)\n",
    "             select first_name ,type,duration,concat(lpad(hour,2,'0'),\":\",lpad(minutes,2,'0'),\":\",lpad(seconds,2,'0')) as duration_formatted\n",
    "                  from four\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fca76b5",
   "metadata": {},
   "source": [
    "#### 114 240 Server Utilization Time M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "1311d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "servers_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/114m_servers.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "289aa57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- server_id: integer (nullable = true)\n",
      " |-- status_time: string (nullable = true)\n",
      " |-- session_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "servers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "7a6a8fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|total_uptime_days|\n",
      "+-----------------+\n",
      "|                1|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy(\"server_id\").orderBy(\"status_time\")\n",
    "\n",
    "servers_df.withColumn(\"next_status_time\", lead(\"status_time\").over(window_spec))\\\n",
    ".filter(col(\"session_status\") == \"start\")\\\n",
    ".withColumn(\"duration_seconds\", unix_timestamp(\"next_status_time\") - unix_timestamp(\"status_time\"))\\\n",
    ".agg((sum(\"duration_seconds\") / 86400).cast('int').alias(\"total_uptime_days\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "a2c47392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|total_uptime_days|\n",
      "+-----------------+\n",
      "|                1|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "servers_df.createOrReplaceTempView('serversdf')\n",
    "\n",
    "spark.sql('''\n",
    " with one as (select server_id,status_time,lead(status_time,1) over(partition by server_id order by status_time) as nex_status__time,\n",
    "                   session_status from serversdf),\n",
    "      two as (select server_id, (unix_timestamp(nex_status__time) - unix_timestamp(status_time)) as duration_sec \n",
    "                   from one where session_status = \"start\")\n",
    "              select cast(sum((duration_sec/86400)) as int) as total_uptime_days from two\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f32aa88",
   "metadata": {},
   "source": [
    "#### 115 241 Consecutive Available Seats II M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "eee072de",
   "metadata": {},
   "outputs": [],
   "source": [
    "cinema_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/115m_cinema.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "0815e16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- seat_id: integer (nullable = true)\n",
      " |-- free: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cinema_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "6acd7216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+---------------------+\n",
      "|first_seat_id|last_seat_id|consecutive_seats_len|\n",
      "+-------------+------------+---------------------+\n",
      "|            3|           5|                    3|\n",
      "+-------------+------------+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/18 20:53:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.orderBy(col('seat_id').asc())\n",
    "window_rnk = Window.partitionBy('free','diff').orderBy(col('seat_id').asc(),col('diff').asc())\n",
    "window_cnt = Window.partitionBy('diff')\n",
    "\n",
    "cinema_df.withColumn('led',lead('free',1).over(window_spec))\\\n",
    ".withColumn('dif',expr(\"free - led \"))\\\n",
    ".select('seat_id','free','led',coalesce('dif',lit('0')).alias('diff'))\\\n",
    ".withColumn('rnk',rank().over(window_rnk))\\\n",
    ".withColumn('cnt',count('diff').over(window_cnt))\\\n",
    ".withColumn('min',min('rnk').over(window_cnt))\\\n",
    ".withColumn('max',max('rnk').over(window_cnt))\\\n",
    ".withColumn('first_seat_id',when( ((col('diff') == 0) & (col('rnk') == col('min'))),col('seat_id')))\\\n",
    ".withColumn('last_seat_id',when( ((col('diff') == 0) & (col('rnk') == col('max'))),col('seat_id')))\\\n",
    ".withColumn('consecutive_seats_len',when((col('diff') == 0) & (col('rnk') == 1)  ,col('cnt')))\\\n",
    ".select( sum(coalesce('first_seat_id',lit(0))).alias('first_seat_id'),\n",
    "         sum(coalesce('last_seat_id',lit(0))).alias('last_seat_id'),\n",
    "         sum(coalesce('consecutive_seats_len',lit(0))).alias('consecutive_seats_len')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "7d7389a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+---------------------+\n",
      "|first_seat_id|last_seat_id|consecutive_seats_len|\n",
      "+-------------+------------+---------------------+\n",
      "|            3|           5|                    3|\n",
      "+-------------+------------+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/18 21:44:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "cinema_df.createOrReplaceTempView('cinemadf')\n",
    "\n",
    "spark.sql('''\n",
    " with one as (select seat_id,free,lead(free,1) over(order by seat_id asc) as ledfree,\n",
    "                   coalesce((free - lead(free,1) over(order by seat_id asc)),0) as diff from cinemadf),\n",
    "      two as (select seat_id,free,ledfree,diff,rank() over(partition by free,diff order by seat_id asc,diff asc) as rnk,\n",
    "                   count(diff) over(partition by diff) as cnt from one),\n",
    "    three as (select seat_id,free,ledfree,diff,rnk,cnt,\n",
    "                     min(rnk) over(partition by diff) as mn,\n",
    "                     max(rnk) over(partition by diff) as mx from two),\n",
    "     four as (select seat_id,free,ledfree,diff,rnk,cnt,mn,mx,\n",
    "                     case when (mn = rnk) and (diff = 0) then seat_id end as first_seat_id, \n",
    "                     case when (mx = rnk) and (diff = 0) then seat_id end as last_seat_id,\n",
    "                     case when (rnk = 1)  and (diff = 0) then cnt end cntseat from three)\n",
    "              select sum(coalesce(first_seat_id,0)) as first_seat_id,\n",
    "                     sum(coalesce(last_seat_id,0)) as last_seat_id,\n",
    "                     sum(coalesce(cntseat,0)) as consecutive_seats_len from four\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f5914a",
   "metadata": {},
   "source": [
    "#### 116 243 alculate Parking Fees and Duration M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "5655f5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/116m_parking.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "82fae14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- lot_id: integer (nullable = true)\n",
      " |-- car_id: integer (nullable = true)\n",
      " |-- entry_time: string (nullable = true)\n",
      " |-- exit_time: string (nullable = true)\n",
      " |-- fee_paid: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parking_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "bdfa4a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1855:===================================>                (136 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+--------------+------+\n",
      "|car_id|total_fee_paid|avg_hourly_fee|lot_id|\n",
      "+------+--------------+--------------+------+\n",
      "|  1001|          18.0|           2.4|     1|\n",
      "|  1002|           6.0|          1.33|     2|\n",
      "+------+--------------+--------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1855:=================================================>  (189 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('car_id','lot_id')\n",
    "window_sum = Window.partitionBy('car_id')\n",
    "window_rnk = Window.partitionBy('car_id').orderBy(col('car_id').desc(),col('lot_id').asc())\n",
    "\n",
    "parking_df.withColumn('diff',expr(\"unix_timestamp(exit_time) - unix_timestamp(entry_time)\")/3600)\\\n",
    ".withColumn('total_fee_paid',sum('fee_paid').over(window_sum))\\\n",
    ".withColumn('total_houurs',sum('diff').over(window_sum))\\\n",
    ".withColumn('avg_hourly_fee',round(expr(\"total_fee_paid/total_houurs\"),2))\\\n",
    ".withColumn('most_time_lot',sum('diff').over(window_spec))\\\n",
    ".withColumn('rnk',rank().over(window_rnk)).filter(col('rnk') == 1)\\\n",
    ".select('car_id','total_fee_paid','avg_hourly_fee','lot_id').distinct().orderBy('car_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "df36c2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+--------------+------+\n",
      "|car_id|total_fee_paid|avg_hourly_fee|lot_id|\n",
      "+------+--------------+--------------+------+\n",
      "|  1001|          18.0|           2.4|     1|\n",
      "|  1002|           6.0|          1.33|     2|\n",
      "+------+--------------+--------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1957:========================================>           (156 + 5) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parking_df.createOrReplaceTempView('parkingdf')\n",
    "\n",
    "spark.sql('''\n",
    " with one as (select lot_id,car_id,entry_time,exit_time,((unix_timestamp(exit_time) - unix_timestamp(entry_time))/3600) as diff,\n",
    "                   fee_paid,sum(fee_paid) over(partition by car_id) as total_fee_paid from parkingdf),\n",
    "      two as (select lot_id,car_id,diff,fee_paid,total_fee_paid,sum(diff) over(partition by car_id) as total_hours,\n",
    "                   sum(diff) over(partition by car_id,lot_id) as most_time_lot,\n",
    "                   rank() over(partition by car_id order by car_id desc, lot_id asc) as rnk from one),\n",
    "    three as (select lot_id,car_id,diff,fee_paid,total_fee_paid,total_hours,most_time_lot,\n",
    "                    round(total_fee_paid/total_hours,2) as avg_hourly_fee,rnk  from two)\n",
    "              select distinct car_id,total_fee_paid,avg_hourly_fee,lot_id from three where rnk = 1 order by car_id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3ce324",
   "metadata": {},
   "source": [
    "#### 117 245 Find Top Scoring Students M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d832e3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "students_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/117m_students.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "09ba6256",
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/117m_courses.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "de07b8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "enrollments_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/117m_enrollments.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "301cd541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- student_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- major: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "455afe04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- course_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- credits: integer (nullable = true)\n",
      " |-- major: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "courses_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "70c48149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- student_id: integer (nullable = true)\n",
      " |-- course_id: integer (nullable = true)\n",
      " |-- semester: string (nullable = true)\n",
      " |-- grade: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enrollments_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "809b4233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|student_id|\n",
      "+----------+\n",
      "|         1|\n",
      "|         3|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('student_id','grade')\n",
    "\n",
    "enrollments_df.withColumn('cnt',count('grade').over(window_spec)).filter(col('cnt') == 2)\\\n",
    ".select('student_id').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "c42c7306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|student_id|\n",
      "+----------+\n",
      "|         1|\n",
      "|         3|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students_df.createOrReplaceTempView('studentsdf')\n",
    "courses_df.createOrReplaceTempView('coursesdf')\n",
    "enrollments_df.createOrReplaceTempView('enrollmentsdf')\n",
    "\n",
    "spark.sql('''\n",
    " with one as (select student_id,count(grade) over(partition by student_id,grade)  cnt from enrollmentsdf)\n",
    "              select distinct student_id from one where cnt = 2 order by student_id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0169a4ff",
   "metadata": {},
   "source": [
    "#### 118 247 itwise User Permissions Analysis M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a3ab41bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/118m_user.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfa5649a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- permissions: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "035b8cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+\n",
      "|common_perms|any_perms|\n",
      "+------------+---------+\n",
      "|           0|       15|\n",
      "+------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/19 19:16:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.orderBy(col('user_id').asc())\n",
    "\n",
    "users_df.withColumn('one',lead('permissions',1).over(window_spec))\\\n",
    ".withColumn('two',lead('permissions',2).over(window_spec))\\\n",
    ".withColumn('three',lead('permissions',3).over(window_spec)).filter(col('user_id') == 1)\\\n",
    ".withColumn('common_perms',expr(\"permissions & one & two & three\"))\\\n",
    ".withColumn('any_perms',expr(\"permissions | one | two | three\")).select('common_perms','any_perms').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "234f29a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+\n",
      "|common_perms|any_perms|\n",
      "+------------+---------+\n",
      "|           0|       15|\n",
      "+------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/19 19:24:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "users_df.createOrReplaceTempView('usersdf')\n",
    "\n",
    "spark.sql('''\n",
    " with one as (select user_id,permissions,\n",
    "                  lead(permissions,1) over(order by user_id asc) as one,\n",
    "                  lead(permissions,2) over(order by user_id asc) as two,\n",
    "                  lead(permissions,3) over(order by user_id asc) as three\n",
    "                  from usersdf)\n",
    "             select (permissions & one & two & three) as common_perms,(permissions | one | two | three) as any_perms \n",
    "                  from one where user_id = 1\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f791ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f69c9936",
   "metadata": {},
   "source": [
    "#### 119 248 Odd and Even Transactions M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "e53118a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/119m_transactions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06f73e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- transaction_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a25a9333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+--------+\n",
      "|transaction_date|odd_sum|even_sum|\n",
      "+----------------+-------+--------+\n",
      "|      2024-07-01|     75|     350|\n",
      "|      2024-07-02|      0|     350|\n",
      "|      2024-07-03|      0|     120|\n",
      "+----------------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('transaction_date')\n",
    "\n",
    "transactions_df.withColumn('even',when(col('amount')%2 == 0,col('amount')))\\\n",
    ".withColumn('odd',when(col('amount')%2 !=0,col('amount')))\\\n",
    ".withColumn('odd_sum',sum('odd').over(window_spec))\\\n",
    ".withColumn('even_sum',sum('even').over(window_spec))\\\n",
    ".select('transaction_date',coalesce('odd_sum',lit('0')).alias('odd_sum'),coalesce('even_sum',lit('0')).alias('even_sum'))\\\n",
    ".distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96a8e81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+--------+\n",
      "|transaction_date|odd_sum|even_sum|\n",
      "+----------------+-------+--------+\n",
      "|      2024-07-01|     75|     350|\n",
      "|      2024-07-02|      0|     350|\n",
      "|      2024-07-03|      0|     120|\n",
      "+----------------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.createOrReplaceTempView('transactionsdf')\n",
    "\n",
    "spark.sql('''\n",
    " with one as (select amount,transaction_date,case when amount%2 == 0 then amount end as even,\n",
    "                                         case when amount%2 != 0 then amount end as odd from transactionsdf),\n",
    "      two as (select transaction_date,sum(even) over(partition by transaction_date) as even_sum,\n",
    "                                      sum(odd) over(partition by transaction_date) as odd_sum from one)\n",
    "              select distinct transaction_date,coalesce(odd_sum,'0') as odd_sum,even_sum from two\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7a9aee",
   "metadata": {},
   "source": [
    "#### 120 249 Customer Purchasing Behavior Analysis M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "5a808656",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/120m_transactions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "67cac6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = (spark.read\n",
    "             .option('header',True)\n",
    "             .option('inferSchema',True)\n",
    "             .format('csv')\n",
    "             .load('../../data/database/120m_products.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70022542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- transaction_date: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83c6a605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "28fc447a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----------------+-----------------+----------------------+--------+-------------+\n",
      "|customer_id|total_amount|transaction_count|unique_categories|avg_transaction_amount|category|loyalty_score|\n",
      "+-----------+------------+-----------------+-----------------+----------------------+--------+-------------+\n",
      "|        101|       450.0|                3|                3|                 150.0|       C|         34.5|\n",
      "|        102|       300.0|                2|                2|                 150.0|       C|         23.0|\n",
      "+-----------+------------+-----------------+-----------------+----------------------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('customer_id')\n",
    "window_cat = Window.partitionBy('customer_id','category')\n",
    "window_rnk = Window.partitionBy('customer_id').orderBy(col('transaction_date').desc())\n",
    "\n",
    "products_df = products_df.withColumnRenamed('product_id','productid')\n",
    "df = products_df.join(transactions_df,products_df.productid == transactions_df.product_id,'inner')\n",
    "\n",
    "df1 = df.withColumn('total_amount',sum('amount').over(window_spec))\\\n",
    ".withColumn('transaction_count',count('transaction_id').over(window_spec))\\\n",
    ".withColumn('unique_categories',count('category').over(window_spec))\\\n",
    ".withColumn('avg_transaction_amount',expr(\"total_amount/transaction_count\"))\\\n",
    ".withColumn('top_cat',count('category').over(window_cat))\\\n",
    ".withColumn('top_category',rank().over(window_rnk))\\\n",
    ".withColumn('loyalty_score',expr(\"(transaction_count * 10) + (total_amount/100)\"))\\\n",
    ".select('customer_id','total_amount','transaction_count','unique_categories','avg_transaction_amount',\n",
    "        'category','loyalty_score','top_category').distinct().filter(col('top_category') == 1)\\\n",
    ".select('customer_id','total_amount','transaction_count','unique_categories','avg_transaction_amount',\n",
    "        'category','loyalty_score').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a86b2156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----------------+-----------------+----------------------+--------+-------------+\n",
      "|customer_id|total_amount|transaction_count|unique_categories|avg_transaction_amount|category|loyalty_score|\n",
      "+-----------+------------+-----------------+-----------------+----------------------+--------+-------------+\n",
      "|        101|       450.0|                3|                3|                 150.0|       C|         34.5|\n",
      "|        102|       300.0|                2|                2|                 150.0|       C|         23.0|\n",
      "+-----------+------------+-----------------+-----------------+----------------------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.createOrReplaceTempView('transactionsdf')\n",
    "products_df.createOrReplaceTempView('productsdf')\n",
    "\n",
    "spark.sql('''\n",
    " with one as (select customer_id,product_id,transaction_date,amount,category,price,\n",
    "                   sum(amount) over(partition by customer_id) as total_amount,\n",
    "                   count(transaction_id) over(partition by customer_id) as transaction_count,\n",
    "                   count(category) over(partition by customer_id) as unique_categories\n",
    "                   from transactionsdf join productsdf on transactionsdf.product_id = productsdf.productid),\n",
    "      two as (select customer_id,product_id,transaction_date,amount,category,price,total_amount,\n",
    "                   transaction_count,unique_categories,(total_amount/transaction_count) as avg_transaction_amount,\n",
    "                   ((transaction_count * 10) + (total_amount/100)) as loyalty_score,\n",
    "                   count(category) over(partition by 'customer_id','category') as top_cat\n",
    "                   from one),\n",
    "    three as (select customer_id,product_id,transaction_date,amount,category,price,total_amount,\n",
    "                   transaction_count,unique_categories,avg_transaction_amount,loyalty_score,top_cat,\n",
    "                   rank() over(partition by customer_id order by transaction_date desc) as rnk from two)\n",
    "              select customer_id,total_amount,transaction_count,unique_categories,avg_transaction_amount,category,loyalty_score\n",
    "                     from three where rnk = 1\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae3cf74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
