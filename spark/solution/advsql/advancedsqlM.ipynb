{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd1ef261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/21 19:28:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/21 19:28:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('database').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbbc271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (col,expr,count,countDistinct,datediff,to_date,date_add,year,month,lag,lead,rank,max,min,round,\n",
    "                                   sum,when,lit,desc,coalesce,abs,greatest,least,array,array_sort,substring, explode,\n",
    "                                   collect_list,array_intersect,datediff,unix_timestamp,rank,dense_rank,least,greatest,\n",
    "                                   row_number\n",
    "                                  )\n",
    "from pyspark.sql.types import (StructField,StructType,\n",
    "                    IntegerType,StringType,DateType,TimestampType )\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fac509",
   "metadata": {},
   "source": [
    "#### 24 Customers Who Bought Products A and B but Not C MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "836b1c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "customers_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/24_customers.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee68dc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/24_orders.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03f7426a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16229c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d9de26b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|customer_id|customer_name|\n",
      "+-----------+-------------+\n",
      "|          3|    Elizabeth|\n",
      "+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prdc_df = orders_df.filter(col('product_name') == lit('C')).select('customer_id')\n",
    "prdc = [prdc_df.collect()[i][0] for i in range(0,len(prdc_df.collect()))]\n",
    "\n",
    "orders_df.filter(col('product_name').isin(['A','B'])).select('customer_id')\\\n",
    "         .filter(~col('customer_id').isin(prdc)).groupBy('customer_id').agg(count((col('customer_id'))).alias('cust_count_id'))\\\n",
    "         .filter(col('cust_count_id') == 2).join(customers_df, orders_df.customer_id == customers_df.customer_id,'inner')\\\n",
    "         .select(customers_df.customer_id,customers_df.customer_name).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7a6ec5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|customer_id|customer_name|\n",
      "+-----------+-------------+\n",
      "|          3|    Elizabeth|\n",
      "+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df.createOrReplaceTempView('customersdf')\n",
    "orders_df.createOrReplaceTempView('ordersdf')\n",
    "\n",
    "spark.sql(' with c_id as (select customer_id from ordersdf where product_name == \"C\" ), \\\n",
    "               gpc_id as (select customer_id,count(\"customer_id\") as cnt_id from ordersdf \\\n",
    "                          where product_name in (\"A\",\"B\") and customer_id not in (select customer_id from c_id) group by customer_id) \\\n",
    "                          select gpc_id.customer_id,customer_name from gpc_id \\\n",
    "                          join customersdf on gpc_id.customer_id ==  customersdf.customer_id \\\n",
    "                          where gpc_id.cnt_id == 2' ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac125f2d",
   "metadata": {},
   "source": [
    "#### 25 Highest Grade For Each Student MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3f27c505",
   "metadata": {},
   "outputs": [],
   "source": [
    "enrollments_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/25_enrollments.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7d5674be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- student_id: integer (nullable = true)\n",
      " |-- course_id: integer (nullable = true)\n",
      " |-- grade: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enrollments_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "486d16b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----+\n",
      "|student_id|course_id|grade|\n",
      "+----------+---------+-----+\n",
      "|         1|        2|   99|\n",
      "|         2|        2|   95|\n",
      "|         3|        3|   82|\n",
      "+----------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mx_grade_df = enrollments_df.groupBy('student_id').agg(max(col('grade')).alias('mx_grade')) \\\n",
    "              .select(col('student_id').alias('stu_id'),col('mx_grade'))\n",
    "\n",
    "lg_df = enrollments_df.join(mx_grade_df, (mx_grade_df.stu_id == enrollments_df.student_id) \n",
    "                      & (mx_grade_df.mx_grade == enrollments_df.grade),'inner' )\\\n",
    "                      .select('stu_id',enrollments_df.course_id,'mx_grade').orderBy('stu_id','course_id')\n",
    "\n",
    "window_spec = Window.partitionBy(col('stu_id')).orderBy(col('stu_id'),col('course_id'))\n",
    "\n",
    "lg_df.withColumn('rnk',rank().over(window_spec)).filter(col('rnk') == 1) \\\n",
    "     .select(col('stu_id').alias('student_id'),'course_id',col('mx_grade').alias('grade')).orderBy('student_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "8cd1eed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|student_id|course_id|mxgrade|\n",
      "+----------+---------+-------+\n",
      "|         1|        2|     99|\n",
      "|         2|        2|     95|\n",
      "|         3|        3|     82|\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enrollments_df.createOrReplaceTempView('enrollmentsdf')\n",
    "\n",
    "spark.sql(''' \n",
    "    with mx_grade as (select student_id as stu_id ,max(grade) as mxgrade from enrollmentsdf group by student_id), \n",
    "               lg as (select student_id, course_id, mxgrade from enrollmentsdf join mx_grade on  \n",
    "                      enrollmentsdf.student_id == mx_grade.stu_id where enrollmentsdf.grade == mx_grade.mxgrade),\n",
    "              rnk as (select student_id,course_id,mxgrade,rank() over(partition by student_id \n",
    "                      order by course_id) as rnk from lg)\n",
    "                     select student_id,course_id,mxgrade from rnk where rnk == 1 order by student_id \n",
    "       ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eb2d11",
   "metadata": {},
   "source": [
    "#### 26 Evaluate Boolean Expression MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "313ff0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "variables_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/26_variables.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76a8c8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "expressions_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/26_expressions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e04ddcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-------------+-----+\n",
      "|left_operand|operator|right_operand|value|\n",
      "+------------+--------+-------------+-----+\n",
      "|           x|       >|            y|false|\n",
      "|           x|       <|            y| true|\n",
      "|           x|       =|            y|false|\n",
      "|           y|       >|            x| true|\n",
      "|           y|       <|            x|false|\n",
      "|           x|       =|            x| true|\n",
      "+------------+--------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = 66\n",
    "y = 77\n",
    "expressions_df.withColumn('xx',when(col('left_operand') == lit('x'),66) \\\n",
    "                              .when(col('left_operand') == lit('y'),77)) \\\n",
    "              .withColumn('yy',when(col('right_operand') == lit('x'),66) \\\n",
    "                              .when(col('right_operand') == lit('y'),77)) \\\n",
    "              .withColumn('value',when(col('operator') == '>', col('xx') > col('yy')) \\\n",
    "                                 .when(col('operator') == '<', col('xx') < col('yy')) \\\n",
    "                                 .when(col('operator') == '=', col('xx') == col('yy'))) \\\n",
    "                                 .select('left_operand','operator','right_operand','value').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b05b07ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-------------+-----+\n",
      "|left_operand|operator|right_operand|value|\n",
      "+------------+--------+-------------+-----+\n",
      "|           x|       >|            y|false|\n",
      "|           x|       <|            y| true|\n",
      "|           x|       =|            y|false|\n",
      "|           y|       >|            x|false|\n",
      "|           y|       <|            x| true|\n",
      "|           x|       =|            x|false|\n",
      "+------------+--------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expressions_df.createOrReplaceTempView('expressionsdf')\n",
    "\n",
    "spark.sql(''' \n",
    "   with base as (select left_operand,operator,right_operand,case when left_operand == \"x\" then 66 else 77 end as xx,  \n",
    "                        case when right_operand == \"y\" then 77 else 66 end as yy from expressionsdf) \n",
    "            select left_operand,operator,right_operand, case when operator = \">\" then \"xx\" > \"yy\"\n",
    "                                                            when operator = \"<\" then \"xx\" < \"yy\"\n",
    "                                                            when operator = \"=\" then \"xx\" = \"yy\" end as value \n",
    "                                                             from base\n",
    "  ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03282d6f",
   "metadata": {},
   "source": [
    "#### 27 Team Scores in Football Tournament MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ff383f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/27_teams.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e012a328",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/27_matches.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "97d7ce2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- team_id: integer (nullable = true)\n",
      " |-- team_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teams_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8e291ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- match_id: integer (nullable = true)\n",
      " |-- host_team: integer (nullable = true)\n",
      " |-- guest_team: integer (nullable = true)\n",
      " |-- host_goals: integer (nullable = true)\n",
      " |-- guest_goals: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matches_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bb71a079",
   "metadata": {},
   "outputs": [],
   "source": [
    "one = matches_df.withColumnRenamed('host_team','team').withColumn('points',when( ( col('host_goals') - col('guest_goals') ) != -1,0)) \\\n",
    "                        .withColumn('num_points',when((col('host_goals') > col('guest_goals')),3)\\\n",
    "                        .when((col('host_goals') == col('guest_goals')),1)) \\\n",
    "                        .select('team','num_points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "41d6612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "two = matches_df.withColumnRenamed('guest_team','team').withColumn('points',when( ( col('host_goals') - col('guest_goals') ) != -1,0)) \\\n",
    "                        .withColumn('num_points',when((col('host_goals') < col('guest_goals')),3)\\\n",
    "                        .when((col('host_goals') == col('guest_goals')),1).otherwise(0)) \\\n",
    "                        .select('team','num_points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "64e010dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------+\n",
      "|team_id|  team_name|num_points|\n",
      "+-------+-----------+----------+\n",
      "|     10|Leetcode FC|         7|\n",
      "|     20| NewYork FC|         3|\n",
      "|     30| Atlanta FC|         1|\n",
      "|     40| Chicago FC|         0|\n",
      "|     50| Toronto FC|         3|\n",
      "+-------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "one.union(two).groupBy(col('team')).agg(sum(col('num_points')).alias('num_points'))\\\n",
    "  .join(teams_df,one.team == teams_df.team_id,'right')\\\n",
    "  .select('team_id','team_name',coalesce(col('num_points'),lit('0')).alias('num_points')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5f31cc4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------+\n",
      "|team_id|  team_name|num_points|\n",
      "+-------+-----------+----------+\n",
      "|     10|Leetcode FC|         7|\n",
      "|     20| NewYork FC|         3|\n",
      "|     30| Atlanta FC|         1|\n",
      "|     40| Chicago FC|         0|\n",
      "|     50| Toronto FC|         3|\n",
      "+-------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    with one as (select host_team as team, case when host_goals > guest_goals then 3 \n",
    "                                             when host_goals == guest_goals then 1 end as num_points from matchesdf),\n",
    "         two as (select guest_team as team,case when host_goals < guest_goals then 3 \n",
    "                                              when host_goals == guest_goals then 1 end as num_points from matchesdf),\n",
    "       three as (select team, sum(num_points) as num_points from (select * from one union all select * from two rigt) \n",
    "                                        group by team)\n",
    "                 select team_id,team_name,coalesce(num_points,'0') as num_points from teamsdf left join three  on team_id == team\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bdafca",
   "metadata": {},
   "source": [
    "#### 28 Apples & Oranges MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "851345f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/28_sales.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b4cb7e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sale_date: string (nullable = true)\n",
      " |-- fruit: string (nullable = true)\n",
      " |-- sold_num: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e616b863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------+\n",
      "| sale_date|  fruit|sold_num|\n",
      "+----------+-------+--------+\n",
      "|2020-05-01| apples|      10|\n",
      "|2020-05-01|oranges|       8|\n",
      "|2020-05-02| apples|      15|\n",
      "|2020-05-02|oranges|      15|\n",
      "|2020-05-03| apples|      20|\n",
      "|2020-05-03|oranges|       0|\n",
      "|2020-05-04| apples|      15|\n",
      "|2020-05-04|oranges|      16|\n",
      "+----------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "654c1232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "| sale_date|diiff|\n",
      "+----------+-----+\n",
      "|2020-05-01|    2|\n",
      "|2020-05-02|    0|\n",
      "|2020-05-03|   20|\n",
      "|2020-05-04|   -1|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "apples_df = sales_df.filter(col('fruit') == 'apples')\n",
    "oranges_df = sales_df.filter(col('fruit') == 'oranges').withColumnRenamed('sale_date','saledate').withColumnRenamed('sold_num','soldnum')\n",
    "apples_df.join(oranges_df,apples_df.sale_date == oranges_df.saledate,'inner')\\\n",
    ".select('sale_date',expr(\"sold_num -soldnum \").alias('diiff')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "cb9e05c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "| sale_date|diff|\n",
      "+----------+----+\n",
      "|2020-05-01|   2|\n",
      "|2020-05-02|   0|\n",
      "|2020-05-03|  20|\n",
      "|2020-05-04|  -1|\n",
      "+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.createOrReplaceTempView('salesdf')\n",
    "\n",
    "spark.sql('''\n",
    "   with apple as (select sale_date,fruit,sold_num from salesdf where fruit = \"apples\"), \n",
    "      oranges as (select sale_date,fruit,sold_num from salesdf where fruit = \"oranges\") \n",
    "                  select apple.sale_date,apple.sold_num - oranges.sold_num  as diff from apple \n",
    "                         join oranges on apple.sale_date = oranges.sale_date\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eda8d30",
   "metadata": {},
   "source": [
    "#### 29 Number of Calls Between Two Persons PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "19033c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "calls_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/29_calls.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "23276f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- from_id: integer (nullable = true)\n",
      " |-- to_id: integer (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calls_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "eaa23c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----------+-------------+\n",
      "|person_01|person_02|call_count|call_duration|\n",
      "+---------+---------+----------+-------------+\n",
      "|        1|        2|         2|           70|\n",
      "|        1|        3|         1|           20|\n",
      "|        3|        4|         4|          999|\n",
      "+---------+---------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calls_df.withColumn('pair_key',expr(\"array_sort(array(from_id,to_id))\"))\\\n",
    "         .groupBy(col('pair_key')).agg(count(col('duration')).alias('call_count'),sum(col('duration')).alias('call_duration'))\\\n",
    "         .select( col('pair_key')[0].alias('person_01'),col('pair_key')[1].alias('person_02'),'call_count','call_duration').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "1a7d50c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----------+--------+\n",
      "|from_id|to_id|call_count|duration|\n",
      "+-------+-----+----------+--------+\n",
      "|      1|    2|         2|      70|\n",
      "|      1|    3|         1|      20|\n",
      "|      3|    4|         4|     999|\n",
      "+-------+-----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calls_df.createOrReplaceTempView('callsdf')\n",
    "\n",
    "spark.sql('''\n",
    "            select a.pair_key[0] as from_id,a.pair_key[1] as to_id,call_count,duration from \n",
    "            (select array_sort(array(from_id,to_id))as pair_key,count(duration) as call_count,sum(duration) as duration \n",
    "                          from callsdf group by pair_key) a\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2635de71",
   "metadata": {},
   "source": [
    "#### 30 Countries You Can Safely Invest In MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d98562d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "person_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/30_person.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a085e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/30_country.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55a81626",
   "metadata": {},
   "outputs": [],
   "source": [
    "calls_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/30_calls.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "606bc156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- phone_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "60c6bdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- country_code: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6638e86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- caller_id: integer (nullable = true)\n",
      " |-- callee_id: integer (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calls_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "66fab141",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df =calls_df.withColumn('callarrays',array_sort(array(col('caller_id'),col('callee_id')))) \\\n",
    "  .agg((sum(col('duration'))/count(col('duration'))).alias('total_avg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "5121d630",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df= calls_df.withColumn('callarrays',array_sort(array(col('caller_id'),col('callee_id')))).groupBy(col('callarrays'))\\\n",
    "  .agg(count(col('duration')).alias('cnt'),sum(col('duration')).alias('sumduration'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "de0e2f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "israel_df = final_df.withColumn(\"exploded_callarrays\", explode(final_df[\"callarrays\"])).filter(col('exploded_callarrays').isin([7,9]))\\\n",
    ".select( (sum(col('sumduration'))/sum(col('cnt'))).alias('israel')).select(lit('israel').alias('country'),col('israel').alias('avg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "e4b9336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Morocco_df = final_df.withColumn(\"exploded_callarrays\", explode(final_df[\"callarrays\"])).filter(col('exploded_callarrays').isin([1,2]))\\\n",
    ".select( (sum(col('sumduration'))/sum(col('cnt'))).alias('Morocco')).select(lit('Morocco').alias('country'),col('Morocco').alias('avg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "6e42bc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "peru_df = final_df.withColumn(\"exploded_callarrays\", explode(final_df[\"callarrays\"])).filter(col('exploded_callarrays').isin([3,12]))\\\n",
    ".select( (sum(col('sumduration'))/sum(col('cnt'))).alias('Peru')).select(lit('Peru').alias('country'),col('Peru').alias('avg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "e1eb33f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Peru'"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "israel_df.union(Morocco_df).union(peru_df).orderBy(desc('avg')).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "0f2ffb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|country|   avg|\n",
      "+-------+------+\n",
      "|   Peru|145.67|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person_df.createOrReplaceTempView('persondf')\n",
    "country_df.createOrReplaceTempView('countrydf')\n",
    "calls_df.createOrReplaceTempView('callsdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with code_list as (select collect_list(id) as coldelst,countrydf.name from person_df join countrydf on \n",
    "                       substring(person_df.phone_number,1,3) == countrydf.country_code group by countrydf.name),\n",
    "      code_array as (select array_sort(array(caller_id,callee_id)) as code, count(duration) as count_call, \n",
    "                       sum(duration) as total_duration from callsdf group by code),\n",
    "    explode_code as (select code,count_call,total_duration,explode(code) as ex_code from code_array),\n",
    "           logic as (select country,sum(avg)/count(country) as avg from (select case when ex_code in (3,12) then \"Peru\"\n",
    "                                 when ex_code in (1,2)  then \"Morocco\"\n",
    "                                 when ex_code in (7,9)  then \"Israel\" end as country,\n",
    "                            case when ex_code in (3,12) then round(sum(total_duration)/sum(count_call),2) \n",
    "                                 when ex_code in (1,2) then round(sum(total_duration)/sum(count_call),2) \n",
    "                                 when ex_code in (7,9) then round(sum(total_duration)/sum(count_call),2) end as avg \n",
    "                            from explode_code  group by ex_code) group by country),\n",
    "      global_avg as (select sum(duration)/count(duration) as global_avg from callsdf )\n",
    "                    select * from logic where avg > (select global_avg from  global_avg)\n",
    "         ''').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98da074e",
   "metadata": {},
   "source": [
    "#### 31 Tree Node M "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dff4b003",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/31_tree.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "48db1b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- p_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "da51c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_df = tree_df.withColumn('type',when((col('p_id')).isNull(),lit('root'))).filter(col('type') == lit('root'))\\\n",
    ".select('id','type')\n",
    "\n",
    "leaf_df = tree_df.alias('d1').join(tree_df.alias('d2'),col('d1.id')==col('d2.p_id'),'anti')\\\n",
    ".withColumn('type',lit('leaf')).select('id','type')\n",
    "\n",
    "inner_df = tree_df.alias('d1').join(tree_df.alias('d2'),col('d1.id')==col('d2.p_id'),'leftsemi')\\\n",
    ".withColumn('type',lit('inner')).select('id','type')\\\n",
    ".filter((col('id') != tree_df.filter(col('p_id').isNull()).select('id').collect()[0][0]))\n",
    "\n",
    "root_df.union(leaf_df).union(inner_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "c94c8883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| type|\n",
      "+---+-----+\n",
      "|  1| root|\n",
      "|  2|inner|\n",
      "|  3| leaf|\n",
      "|  4| leaf|\n",
      "|  5| leaf|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "   tree_df.withColumn('type',when((col('p_id')).isNull(),lit('root'))).filter(col('type') == lit('root'))\\\n",
    "   .select('id','type')\\\n",
    ".union(tree_df.alias('d1').join(tree_df.alias('d2'),col('d1.id')==col('d2.p_id'),'anti')\\\n",
    "   .withColumn('type',lit('leaf')).select('id','type'))\\\n",
    ".union(tree_df.alias('d1').join(tree_df.alias('d2'),col('d1.id')==col('d2.p_id'),'leftsemi')\\\n",
    "   .withColumn('type',lit('inner')).select('id','type')\\\n",
    "   .filter((col('id') != tree_df.filter(col('p_id').isNull()).select('id').collect()[0][0]))).orderBy('id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "588f2134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| type|\n",
      "+---+-----+\n",
      "|  1| root|\n",
      "|  2|inner|\n",
      "|  3| leaf|\n",
      "|  4| leaf|\n",
      "|  5| leaf|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree_df.createOrReplaceTempView('treedf')\n",
    "\n",
    "spark.sql('''\n",
    "  with root as (select id,'root' as type from treedf where p_id is null),\n",
    "       leaf as (select id, 'leaf' as type from treedf as d1 anti join treedf as d2 on d1.id == d2.p_id ),\n",
    "      inner as (select id, 'inner' as type from treedf as d1 left semi join treedf as d2 on d1.id == d2.p_id \n",
    "                    where id not in (select id from root))\n",
    "                select * from root union(select * from leaf) union (select * from inner) order by id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b076200",
   "metadata": {},
   "source": [
    "#### 32 Game Play Analysis III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "400d0357",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/32_activity.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "67e1d31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- player_id: integer (nullable = true)\n",
      " |-- device_id: integer (nullable = true)\n",
      " |-- event_date: string (nullable = true)\n",
      " |-- games_played: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "8d653d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+\n",
      "|player_id|event_date|gameplayed|\n",
      "+---------+----------+----------+\n",
      "|        1|2016-03-01|         5|\n",
      "|        1|2016-05-02|        11|\n",
      "|        1|2017-06-25|        12|\n",
      "|        3|2016-03-02|         0|\n",
      "|        3|2018-07-03|         5|\n",
      "+---------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  to calculate a rolling sum over the previous 2 rows including the current row:\n",
    "window_spec = Window.partitionBy('player_id').orderBy('player_id','event_date').rowsBetween(-2,0)\n",
    "\n",
    "activity_df.withColumn('gameplayed',sum('games_played').over(window_spec))\\\n",
    "    .select('player_id','event_date',col('gameplayed')).show()\n",
    "                                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "b7553834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+\n",
      "|player_id|event_date|gameplayed|\n",
      "+---------+----------+----------+\n",
      "|        1|2016-03-01|         5|\n",
      "|        1|2016-05-02|        11|\n",
      "|        1|2017-06-25|        12|\n",
      "|        3|2016-03-02|         0|\n",
      "|        3|2018-07-03|         5|\n",
      "+---------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity_df.createOrReplaceTempView('activitydf')\n",
    "\n",
    "spark.sql('''\n",
    "          select player_id,event_date, sum(games_played) over(partition by player_id order by player_id,event_date\n",
    "          ROWS BETWEEN  2 PRECEDING AND CURRENT ROW) as gameplayed from activitydf\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6143b390",
   "metadata": {},
   "source": [
    "#### 33 Grand Slam Titles MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8c9afaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/33_players.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5167e702",
   "metadata": {},
   "outputs": [],
   "source": [
    "championships_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/33_championships.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "aac471fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- player_id: integer (nullable = true)\n",
      " |-- player_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "players_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "f7228bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- Wimbledon: integer (nullable = true)\n",
      " |-- Fr_open: integer (nullable = true)\n",
      " |-- US_open: integer (nullable = true)\n",
      " |-- Au_open: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "championships_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "4419441b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-------+-------+-------+\n",
      "|year|Wimbledon|Fr_open|US_open|Au_open|\n",
      "+----+---------+-------+-------+-------+\n",
      "|2018|        1|      1|      1|      1|\n",
      "|2019|        1|      1|      2|      2|\n",
      "|2020|        2|      1|      2|      2|\n",
      "+----+---------+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "championships_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "26d2d3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------------+\n",
      "|player_id|player_name|grand_slams_count|\n",
      "+---------+-----------+-----------------+\n",
      "|        1|      Nadal|                7|\n",
      "|        2|    Federer|                5|\n",
      "+---------+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "championships_df.select('year',col('Wimbledon').alias('title'))\\\n",
    ".union(championships_df.select('year',col('Fr_open').alias('title'))) \\\n",
    ".union(championships_df.select('year',col('US_open').alias('title'))) \\\n",
    ".union(championships_df.select('year',col('Au_open').alias('title')))\\\n",
    ".groupBy('title').agg(count(col('title')).alias('grand_slams_count')) \\\n",
    ".join(players_df,col('title') == col('player_id'),'inner' )\\\n",
    ".select('player_id','player_name','grand_slams_count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "572e680c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------------+\n",
      "|player_id|player_name|grand_slams_count|\n",
      "+---------+-----------+-----------------+\n",
      "|        1|      Nadal|                7|\n",
      "|        2|    Federer|                5|\n",
      "+---------+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "players_df.createOrReplaceTempView('playersdf')\n",
    "championships_df.createOrReplaceTempView('championshipsdf')\n",
    "\n",
    "spark.sql(''' \n",
    "  with main as (select title,count(title) as grand_slams_count from\n",
    "                 (select Wimbledon as title from championshipsdf\n",
    "                      union all(select Fr_open as title from championshipsdf)\n",
    "                      union all(select US_open as title from championshipsdf)\n",
    "                      union all(select Au_open as title from championshipsdf)) group by title)\n",
    "                select player_id,player_name,grand_slams_count from playersdf join main on player_id == title\n",
    "\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c84d4f",
   "metadata": {},
   "source": [
    "#### 34 Leetflex Banned Accounts MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "404001eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loginfo_schema = StructType([\n",
    "                        StructField('account_id',IntegerType()),\n",
    "                        StructField('ip_address',IntegerType()),\n",
    "                        StructField('login',TimestampType()),\n",
    "                        StructField('logout',TimestampType()),\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "7e81b907",
   "metadata": {},
   "outputs": [],
   "source": [
    "loginfo_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .schema(loginfo_schema )\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/34_loginfo.csv')\n",
    "               .withColumn('diff',unix_timestamp(col('logout')) - unix_timestamp(col('login')))  \n",
    "               .withColumn('minutes',round( (col('diff')/60),2))\n",
    "               .withColumn('hours',round( (col('diff')/3600),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "37dda3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- account_id: integer (nullable = true)\n",
      " |-- ip_address: integer (nullable = true)\n",
      " |-- login: timestamp (nullable = true)\n",
      " |-- logout: timestamp (nullable = true)\n",
      " |-- diff: long (nullable = true)\n",
      " |-- minutes: double (nullable = true)\n",
      " |-- hours: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loginfo_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "63590b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------------+-------------------+-----+-------+-----+\n",
      "|account_id|ip_address|              login|             logout| diff|minutes|hours|\n",
      "+----------+----------+-------------------+-------------------+-----+-------+-----+\n",
      "|         1|         1|2021-02-01 09:00:00|2021-02-01 09:30:00| 1800|   30.0|  0.5|\n",
      "|         1|         2|2021-02-01 08:00:00|2021-02-01 11:30:00|12600|  210.0|  3.5|\n",
      "|         2|         6|2021-02-01 20:30:00|2021-02-01 22:00:00| 5400|   90.0|  1.5|\n",
      "|         2|         7|2021-02-02 20:30:00|2021-02-02 22:00:00| 5400|   90.0|  1.5|\n",
      "|         3|         9|2021-02-01 16:00:00|2021-02-01 16:59:59| 3599|  59.98|  1.0|\n",
      "|         3|        13|2021-02-01 17:00:00|2021-02-01 17:59:59| 3599|  59.98|  1.0|\n",
      "|         4|        10|2021-02-01 16:00:00|2021-02-01 17:00:00| 3600|   60.0|  1.0|\n",
      "|         4|        11|2021-02-01 17:00:00|2021-02-01 17:59:59| 3599|  59.98|  1.0|\n",
      "+----------+----------+-------------------+-------------------+-----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loginfo_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "ec689b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|account_id|\n",
      "+----------+\n",
      "|         1|\n",
      "|         1|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loginfo_df.alias('d1').join(loginfo_df.alias('d2'),(col('d1.account_id') == col('d2.account_id') )\n",
    "                            & (unix_timestamp(col('d1.login')) < unix_timestamp(col('d2.logout')) )\n",
    "                            & (unix_timestamp(col('d1.logout')) > unix_timestamp(col('d2.login')) )\n",
    "                            & (col('d1.ip_address') != col('d2.ip_address')), 'inner')\\\n",
    "          .select(col('d1.account_id')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c8b843",
   "metadata": {},
   "source": [
    "#### 35 The Most Recent Three Orders MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "b3d0e97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/35_customers.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "283d250d",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/35_orders.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "4ca503a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "fa39c895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- cost: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "a6cdc0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------+----------+\n",
      "|     name|customer_id|order_id|order_date|\n",
      "+---------+-----------+--------+----------+\n",
      "|Annabelle|          3|       7|2020-08-01|\n",
      "|Annabelle|          3|       3|2020-07-31|\n",
      "| Jonathan|          2|       9|2020-08-07|\n",
      "| Jonathan|          2|       6|2020-08-01|\n",
      "| Jonathan|          2|       2|2020-07-30|\n",
      "|   Marwan|          4|       4|2020-07-29|\n",
      "|  Winston|          1|       8|2020-08-03|\n",
      "|  Winston|          1|       1|2020-07-31|\n",
      "|  Winston|          1|      10|2020-07-15|\n",
      "+---------+-----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('name').orderBy(col('name').asc(),col('order_date').desc())\n",
    "orders_df.join(customers_df,orders_df.customer_id == customers_df.customer_id,'inner')\\\n",
    ".select('name',orders_df.customer_id,'order_id','order_date' )\\\n",
    ".withColumn('rnk',rank().over(window_spec)).filter(col('rnk') != 4)\\\n",
    ".select('name','customer_id','order_id','order_date').orderBy(col('name').asc(),col('order_date').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "cbfcd780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------+----------+\n",
      "|     name|customer_id|order_id|order_date|\n",
      "+---------+-----------+--------+----------+\n",
      "|Annabelle|          3|       7|2020-08-01|\n",
      "|Annabelle|          3|       3|2020-07-31|\n",
      "| Jonathan|          2|       9|2020-08-07|\n",
      "| Jonathan|          2|       6|2020-08-01|\n",
      "| Jonathan|          2|       2|2020-07-30|\n",
      "|   Marwan|          4|       4|2020-07-29|\n",
      "|  Winston|          1|       8|2020-08-03|\n",
      "|  Winston|          1|       1|2020-07-31|\n",
      "|  Winston|          1|      10|2020-07-15|\n",
      "+---------+-----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df.createOrReplaceTempView('customersdf')\n",
    "orders_df.createOrReplaceTempView('ordersdf')\n",
    "\n",
    "spark.sql('''\n",
    "   with main as (select name,customersdf.customer_id,order_id,order_date from customersdf \n",
    "                    join ordersdf on customersdf.customer_id == ordersdf.customer_id),\n",
    "         rnk as (select name,customer_id,order_id,order_date,rank() over(PARTITION BY name \n",
    "                    ORDER BY name ASC,order_date DESC) as rnk from main)\n",
    "                select name,customer_id,order_id,order_date from rnk where rnk != 4 order by name asc, order_date desc\n",
    "                \n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ce8bee",
   "metadata": {},
   "source": [
    "#### 36 Maximum Transaction Each Day MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "977e91fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_schema = StructType([\n",
    "                        StructField('transaction_id',IntegerType()),\n",
    "                        StructField('day',TimestampType()),\n",
    "                        StructField('amount',IntegerType())\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "63c3d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/36_transactions.csv')\n",
    "               .withColumn('date',to_date('day','yyyy-M-dd HH:mm:ss')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3325adb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "803309bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|transaction_id|\n",
      "+--------------+\n",
      "|             1|\n",
      "|             5|\n",
      "|             6|\n",
      "|             8|\n",
      "+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 65:===================================================>  (189 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "widonw_spec = Window.partitionBy('date').orderBy(col('amount').desc())\n",
    "\n",
    "transactions_df.withColumn('rnk',dense_rank().over(widonw_spec)).filter(col('rnk') == 1)\\\n",
    " .select('transaction_id').orderBy('transaction_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8283ce6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|transaction_id|\n",
      "+--------------+\n",
      "|             1|\n",
      "|             5|\n",
      "|             6|\n",
      "|             8|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.createOrReplaceTempView('transactionsdf')\n",
    "\n",
    "spark.sql('''\n",
    "          select transaction_id from \n",
    "          (select transaction_id,amount,date,dense_rank() OVER(PARTITION BY date ORDER BY amount DESC) as rnk\n",
    "              from transactionsdf) where rnk == 1 order by transaction_id\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26ea6e0",
   "metadata": {},
   "source": [
    "#### 37 Project Employees III MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f7a56d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/37_project.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "000d19ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/37_employee.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6162bf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- project_id: integer (nullable = true)\n",
      " |-- employee_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "project_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "273ffbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- experience_years: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f2f8f5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|project_id|employee_id|\n",
      "+----------+-----------+\n",
      "|         1|          1|\n",
      "|         1|          3|\n",
      "|         2|          1|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "projectdf = project_df.join(employee_df,project_df.employee_id == employee_df.employee_id, 'inner' )\\\n",
    ".select('project_id',project_df.employee_id,'experience_years')\n",
    "\n",
    "window_spec = Window.partitionBy('project_id').orderBy(col('experience_years').desc())\n",
    "\n",
    "projectdf.withColumn('rnk',rank().over(window_spec)).filter(col('rnk') == 1)\\\n",
    ".select('project_id','employee_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b114ec66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|project_id|employee_id|\n",
      "+----------+-----------+\n",
      "|         1|          1|\n",
      "|         1|          3|\n",
      "|         2|          1|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "project_df.createOrReplaceTempView('projectdf')\n",
    "employee_df.createOrReplaceTempView('employeedf')\n",
    "\n",
    "spark.sql('''\n",
    "  with main as (select project_id,projectdf.employee_id as employee_id,experience_years from projectdf join employeedf on\n",
    "                    projectdf.employee_id == employeedf.employee_id)\n",
    "                select project_id,employee_id from  \n",
    "                   (select project_id,employee_id,experience_years,rank() OVER(PARTITION BY project_id \n",
    "                       ORDER BY experience_years DESC) as rnk from main) where rnk = 1\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180a9c75",
   "metadata": {},
   "source": [
    "#### 38 Find the Start and End Number of Continuous Ranges MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6f645828",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/38_logs.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e3f5eada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- log_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e28d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "df_grouped = df_with_groups.groupBy(\"group_id\").agg(\n",
    "    col(\"log_id\").alias(\"end_id\"),\n",
    "    lag(\"log_id\").over(Window.partitionBy(\"group_id\").orderBy(\"log_id\")).alias(\"start_id\")\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "2cdc177c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|start|end|\n",
      "+-----+---+\n",
      "|    1|  3|\n",
      "|    7|  8|\n",
      "|   10| 10|\n",
      "+-----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/21 16:04:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/21 16:04:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/21 16:04:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/21 16:04:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/21 16:04:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/21 16:04:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.orderBy(\"log_id\")\n",
    "\n",
    "df_with_lag = logs_df.withColumn(\"prev_log_id\", lag(\"log_id\").over(window_spec))\n",
    "\n",
    "df_with_breaks = df_with_lag.withColumn(\"is_new_group\", when((col(\"log_id\") - col(\"prev_log_id\")) > 1, 1).otherwise(0))\n",
    "\n",
    "df_with_groups = df_with_breaks.withColumn(\"group_id\", sum(\"is_new_group\").over(Window.orderBy(\"log_id\")))\n",
    "\n",
    "logic_df = df_with_groups.withColumn('strend',rank().over(Window.partitionBy('group_id').orderBy('log_id')))\\\n",
    ".withColumn('start',when(col('strend') == min('strend').over(Window.partitionBy('group_id')),col('log_id')).otherwise(0))\\\n",
    ".withColumn('end',when(col('strend')   == max('strend').over(Window.partitionBy('group_id')),col('log_id')).otherwise(0))\n",
    "\n",
    "start_df = logic_df.select('start').withColumn('rnk',dense_rank().over(Window.orderBy(col('start')))).filter(col('rnk') != 1)\n",
    "end_df = logic_df.select('end').withColumn('rnk',dense_rank().over(Window.orderBy(col('end')))).filter(col('rnk') != 1)\n",
    "\n",
    "start_df.join(end_df,start_df.rnk == end_df.rnk,'inner').select('start','end').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "7b220d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|start|end|\n",
      "+-----+---+\n",
      "|    1|  3|\n",
      "|    7|  8|\n",
      "|   10| 10|\n",
      "+-----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/21 19:58:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/21 19:58:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/21 19:58:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/21 19:58:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/21 19:58:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/21 19:58:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "logs_df.createOrReplaceTempView('logsdf')\n",
    "\n",
    "spark.sql('''\n",
    "  with main as (select log_id, sum(is_new_group) OVER(ORDER BY log_id) as group_id  from \n",
    "                    (select log_id,case when (log_id - prev_log_id) > 1 then 1 else 0  end is_new_group from \n",
    "                        (select log_id,lag(log_id) OVER(ORDER BY log_id) as prev_log_id from logsdf)) group by log_id,is_new_group),    \n",
    "      start as (select distinct min(log_id) OVER(PARTITION BY group_id ) as start from main),\n",
    "        end as (select distinct max(log_id) OVER(PARTITION BY group_id ) as end from main),\n",
    "     end_id as (select end,row_number() OVER(ORDER BY end) as row_id from end), \n",
    "   start_id as (select start,row_number() OVER(ORDER BY start) as row_id from start)\n",
    "                select start,end from start_id join end_id on start_id.row_id == end_id.row_id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dc1138",
   "metadata": {},
   "source": [
    "#### 39 The Most Frequently Ordered Products for Each Customer MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "c3c035ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/39_customers.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "44c93214",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/39_orders.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "2fc741db",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/39_products.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "d1307e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "2b8fbac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "3a5ea45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "09cb2d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------------+\n",
      "|customer_id|product_id|product_name|\n",
      "+-----------+----------+------------+\n",
      "|          1|         2|       mouse|\n",
      "|          2|         2|       mouse|\n",
      "|          2|         1|    keyboard|\n",
      "|          2|         3|      screen|\n",
      "|          3|         3|      screen|\n",
      "|          4|         1|    keyboard|\n",
      "+-----------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('customer_id','product_id')\n",
    "\n",
    "orders_df.select('customer_id','product_id').withColumn('cnt',count(col('product_id')).over(window_spec))\\\n",
    ".withColumn('rnk',rank().over(Window.partitionBy('customer_id').orderBy(col('cnt').desc())).alias('rnk'))\\\n",
    ".filter(col('rnk') == 1).select('customer_id','product_id').distinct().orderBy('customer_id')\\\n",
    ".join(products_df,products_df.product_id == orders_df.product_id ,'inner')\\\n",
    ".select('customer_id',products_df.product_id,'product_name').orderBy('customer_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "8cb1e3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------------+\n",
      "|customer_id|product_id|product_name|\n",
      "+-----------+----------+------------+\n",
      "|          1|         2|       mouse|\n",
      "|          2|         2|       mouse|\n",
      "|          2|         3|      screen|\n",
      "|          2|         1|    keyboard|\n",
      "|          3|         3|      screen|\n",
      "|          4|         1|    keyboard|\n",
      "+-----------+----------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 637:==================================================>  (189 + 6) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "orders_df.createOrReplaceTempView('ordersdf')\n",
    "products_df.createOrReplaceTempView('productsdf')\n",
    "\n",
    "spark.sql('''\n",
    "   with main as (select customer_id,product_id,rank() OVER(PARTITION BY customer_id ORDER BY \n",
    "                     count(product_id) OVER(PARTITION BY customer_id,product_id) DESC) as rnk from ordersdf) \n",
    "                select distinct customer_id,main.product_id as product_id,product_name from main join \n",
    "                     productsdf on main.product_id = productsdf.product_id where rnk == 1 order by customer_id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f90a4d",
   "metadata": {},
   "source": [
    "#### 40 Biggest Window Between Visits MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "72e83aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "uservisits_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/40_uservisits.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "e4d78f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- visit_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uservisits_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "3ee1ea2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|user_id|days|\n",
      "+-------+----+\n",
      "|      1|  39|\n",
      "|      2|  65|\n",
      "|      3|  51|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('user_id').orderBy('visit_date')\n",
    "uservisits_df.withColumn('diff',when( lead('visit_date',1).over(window_spec).isNull(),'2021-1-1')\\\n",
    "   .otherwise(lead('visit_date',1).over(window_spec)) )\\\n",
    "   .groupBy('user_id').agg(max(datediff('diff','visit_date')).alias('days')).orderBy('user_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "638afbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/21 23:18:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/21 23:18:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|user_id|days|\n",
      "+-------+----+\n",
      "|      1|  39|\n",
      "|      2|  65|\n",
      "|      3|  51|\n",
      "+-------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/21 23:18:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "x = uservisits_df.select('user_id').distinct().collect()\n",
    "lst = []\n",
    "for i in range(0,len(x)):\n",
    "    user_id = x[i][0]\n",
    "    lst.append(\n",
    "    uservisits_df.withColumn('rnk',rank().over(window_spec)).filter(col('user_id') == x[i][0] )\\\n",
    "   .withColumn('diff',when( lead('visit_date',1,0).over(Window.orderBy('visit_date')) == 0,'2021-1-1')\\\n",
    "   .otherwise(lead('visit_date',1,0).over(Window.orderBy('visit_date'))) )\\\n",
    "   .select(lit(user_id).alias('user_id'),max(expr(\"datediff(diff,visit_date)\")).alias('days')).collect())\n",
    "\n",
    "flattened_rows = [row for sublist in lst for row in sublist]\n",
    "\n",
    "spark.createDataFrame(flattened_rows).orderBy('user_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "6acbf458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|user_id|days|\n",
      "+-------+----+\n",
      "|      1|  39|\n",
      "|      2|  65|\n",
      "|      3|  51|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "        with main as (select user_id,visit_date,\n",
    "                      CASE WHEN lead(visit_date,1,0) OVER(PARTITION BY user_id ORDER BY visit_date) == 0 THEN '2021-1-1'\n",
    "                           ELSE lead(visit_date,1,0) OVER(PARTITION BY user_id ORDER BY visit_date) \n",
    "                           END lead_date \n",
    "                      FROM uservisitsdf)\n",
    "                select user_id,max(datediff(lead_date,visit_date)) as days from main group by user_id order by user_id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da96c07",
   "metadata": {},
   "source": [
    "#### 41 All People Report to the Given Manager MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89b8e3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/41_employees.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e249fc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- manager_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1cc20fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = employees_df.select('manager_id','employee_id')\\\n",
    ".join(employees_df.select('employee_id','manager_id').withColumnRenamed('employee_id','manager_id'),on = 'manager_id')\\\n",
    ".collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "560ff799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+\n",
      "|manager_id|employee_id|manager_id|\n",
      "+----------+-----------+----------+\n",
      "|         1|          1|         1|\n",
      "|         3|          3|         3|\n",
      "|         1|          2|         1|\n",
      "|         2|          4|         1|\n",
      "|         4|          7|         2|\n",
      "|         3|          8|         3|\n",
      "|         8|          9|         3|\n",
      "|         1|         77|         1|\n",
      "+----------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(lst).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b6cd3a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|employee_id|\n",
      "+-----------+\n",
      "|          2|\n",
      "|          4|\n",
      "|          7|\n",
      "|         77|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(lst,[\"manager_id_1\", \"employee_id\", \"manager_id_2\"])\\\n",
    ".filter( (col('manager_id_2') <= 2) & (col('employee_id') != 1) ).select('employee_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "31a7aac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|emp|\n",
      "+---+\n",
      "|  2|\n",
      "|  4|\n",
      "|  7|\n",
      "| 77|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.createOrReplaceTempView('employeesdf')\n",
    "\n",
    "spark.sql('''\n",
    "   with   three as (select one.employee_id,one.manager_id,two.employee_id as emp ,two.manager_id  from employeesdf as one \n",
    "               join employeesdf as two on one.employee_id == two.manager_id \n",
    "               where one.manager_id <= 2 and two.employee_id != 1 )\n",
    "               select emp from three\n",
    "               ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca891691",
   "metadata": {},
   "source": [
    "#### 42 Page Recommendations MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "fa0c7e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "friendship_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/42_friendship.csv'))\n",
    "\n",
    "liks_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/42_likes.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "8eef6bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user1_id: integer (nullable = true)\n",
      " |-- user2_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "friendship_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "23756dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- page_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "liks_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c024f11",
   "metadata": {},
   "source": [
    "#### 43 Students With Invalid Departments EP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "a969501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "departments_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/43_departments.csv'))\n",
    "\n",
    "students_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/43_students.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "dd8a9065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departments_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "05118068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- department_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "c5a46658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  2|   John|\n",
      "|  3|  Steve|\n",
      "|  4|Jasmine|\n",
      "|  7| Daiana|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students_df.join(departments_df,students_df.department_id == departments_df.id, 'anti')\\\n",
    " .select('id','name').orderBy('id').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "5f87ad7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  2|   John|\n",
      "|  3|  Steve|\n",
      "|  4|Jasmine|\n",
      "|  7| Daiana|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students_df.createOrReplaceTempView('studentsdf')\n",
    "departments_df.createOrReplaceTempView('departmentsdf')\n",
    "\n",
    "spark.sql('''\n",
    "          select id,name from studentsdf anti join departmentsdf on \n",
    "               studentsdf.department_id == departmentsdf.id order by id\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdc3ff1",
   "metadata": {},
   "source": [
    "#### 44 Department Highest Salary M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "2dcbd505",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/43_employee.csv'))\n",
    "\n",
    "department_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/43_department.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "cef85f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- department: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "261cf410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "department_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "c3a27088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+\n",
      "|dep_name|emp_name|salary|\n",
      "+--------+--------+------+\n",
      "|   Sales|   Henry| 80000|\n",
      "|      IT|     Jim| 90000|\n",
      "|      IT|     Max| 90000|\n",
      "+--------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('dep_name').orderBy(col('salary').desc())\n",
    "employee_df.join(department_df,employee_df.department == department_df.id,'inner')\\\n",
    ".select((employee_df.name).alias('emp_name'),(department_df.name).alias('dep_name'),'salary')\\\n",
    ".withColumn('mx',rank().over(window_spec)).filter(col('mx') == 1).select('dep_name','emp_name','salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "dd9f5cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+\n",
      "|dep_name|emp_name|salary|\n",
      "+--------+--------+------+\n",
      "|      IT|     Jim| 90000|\n",
      "|      IT|     Max| 90000|\n",
      "|   Sales|   Henry| 80000|\n",
      "+--------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.createOrReplaceTempView('employeedf')\n",
    "department_df.createOrReplaceTempView('departmentdf')\n",
    "\n",
    "spark.sql(''' select dep_name,emp_name,salary from \n",
    "              (select dep.name as dep_name,emp.name as emp_name,emp.salary,\n",
    "                      rank() OVER(PARTITION BY department ORDER BY salary) as rnk from employeedf as emp\n",
    "                        join departmentdf as dep on dep.id = emp.department) where rnk = 2\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a5bca2",
   "metadata": {},
   "source": [
    "#### 45 The Most Recent Orders for Each Product MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "53e20e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../dataadvsql/45_customers.csv'))\n",
    "\n",
    "orders_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/45_orders.csv'))\n",
    "\n",
    "products_df = (spark.read\n",
    "               .option('header',True)\n",
    "               .option('inferSchema',True)\n",
    "               .format('csv')\n",
    "               .load('../../data/advsql/45_products.csv')\n",
    "               .withColumnRenamed('product_id','productid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "e0da370b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "2bbf9df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "4b422a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- productid: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "176c5129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+----------+\n",
      "|order_id|order_date|customer_id|product_id|\n",
      "+--------+----------+-----------+----------+\n",
      "|       1|2020-07-31|          1|         1|\n",
      "|       2|2020-07-30|          2|         2|\n",
      "|       3|2020-08-29|          3|         3|\n",
      "|       4|2020-07-29|          4|         1|\n",
      "|       5|2020-06-10|          1|         2|\n",
      "|       6|2020-08-01|          2|         1|\n",
      "|       7|2020-08-01|          3|         1|\n",
      "|       8|2020-08-03|          1|         2|\n",
      "|       9|2020-08-07|          2|         3|\n",
      "|      10|2020-07-15|          1|         2|\n",
      "+--------+----------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "aaf6b35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+--------+----------+\n",
      "|product_name|product_id|order_id|order_date|\n",
      "+------------+----------+--------+----------+\n",
      "|    keyboard|         1|       6|2020-08-01|\n",
      "|    keyboard|         1|       7|2020-08-01|\n",
      "|      screen|         3|       3|2020-08-29|\n",
      "|       mouse|         2|       8|2020-08-03|\n",
      "+------------+----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('product_id').orderBy(col('order_date').desc())\n",
    "\n",
    "orders_df.withColumn('rnk',rank().over(window_spec)).filter(col('rnk') == 1)\\\n",
    ".join(products_df,orders_df.product_id == products_df.productid,'inner' ) \\\n",
    ".select('product_name','product_id','order_id','order_date').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "c38bbf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+--------+----------+\n",
      "|product_name|product_id|order_id|order_date|\n",
      "+------------+----------+--------+----------+\n",
      "|    keyboard|         1|       6|2020-08-01|\n",
      "|    keyboard|         1|       7|2020-08-01|\n",
      "|      screen|         3|       3|2020-08-29|\n",
      "|       mouse|         2|       8|2020-08-03|\n",
      "+------------+----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.createOrReplaceTempView('ordersdf')\n",
    "products_df.createOrReplaceTempView('productsdf')\n",
    "\n",
    "spark.sql('''\n",
    "          select product_name,product_id,order_id,order_date from \n",
    "          (select product_name,product_id,order_id,order_date,\n",
    "               rank() OVER( PARTITION BY product_id ORDER BY order_date DESC ) as rnk from ordersdf\n",
    "               join productsdf on productid = product_id) where rnk = 1\n",
    "         ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "d19f47ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fbb91c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39af21a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738e32ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
